{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you need these\n",
    "# will add to requirements.txt in upcoming PR\n",
    "# !pip install dask_geopandas\n",
    "# !pip install pyogrio\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import sys\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "import dask_geopandas\n",
    "import re\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "def list_geospatial_files(path):\n",
    "    \"\"\" Build a list of shapefile URIs contained in S3 folder \"\"\"\n",
    "    # initiate empty list for s3 URIs\n",
    "    all_shapefiles = []\n",
    "    bucket_name = 'ca-climate-index' \n",
    "    # initiate s3 session\n",
    "    session = boto3.Session()\n",
    "    # use the session to get the resource\n",
    "    s3 = session.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    # iterate through directory\n",
    "    for obj in my_bucket.objects.filter(\n",
    "        Prefix=path):\n",
    "        # build list of shapefile URIs\n",
    "        if obj.key.endswith('.zip'):\n",
    "            # preceding the URI with 'zip' lets you read in the file without downloading, unzipping, etc\n",
    "            s3_uri = f\"zip+s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "        elif obj.key.endswith('.shp'):\n",
    "            s3_uri = \"s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "    return all_shapefiles\n",
    "\n",
    "# @append_metadata\n",
    "def reproject_large_shapefile(shp_fname, ca_boundaries, varname='', additional_comments='N/A'):\n",
    "    \"\"\"Given S3 URI which corresponds to a data shapefile and a shapefile\n",
    "    with California Census Tracts, (1) reproject the data shapefile to the CRS of the California Census Tracts, \n",
    "    (2) clip to California Census Tracts, and (3) send it off to S3. This code differs from the \n",
    "    reproject_shapefile() function by utilizing dask-geopandas to manipulate large datasets.\"\"\"    \n",
    "\n",
    "    # read in shapefile of interest from S3 \n",
    "    print(f\"Reading in file: {shp_fname}.\")\n",
    "    gdf = dask_geopandas.read_file(shp_fname, npartitions=10)\n",
    "    print(f\"Original CRS of data for {varname}: {gdf.crs}\")\n",
    "    # check the current coordinate system of the census tracts data\n",
    "    print(f\"CRS of Census Tracts Shapefile: {ca_boundaries.crs}\")\n",
    "\n",
    "    # reproject the data to the census tract CRS and clip to California\n",
    "    gdf_reprojected = gdf.to_crs(ca_boundaries.crs)\n",
    "    print(f\"{varname} reprojected from {gdf.crs} to {gdf_reprojected.crs} with geopandas to_crs() function.\")\n",
    "\n",
    "    # shuffle the geodataframe into spatially coherent partitions\n",
    "    # ddf = dask_geopandas.from_geopandas(gdf_reprojected, npartitions=8)\n",
    "    ddf = gdf_reprojected.spatial_shuffle()\n",
    "    print(f\"{varname} geodataframe has been partitioned into spatially coherent chunks via dask-geopandas spatial_shuffle().\")\n",
    "\n",
    "    ca_ddf = dask_geopandas.from_geopandas(ca_boundaries, npartitions=10)\n",
    "    ca_ddf = ca_ddf.spatial_shuffle()\n",
    "    print(f\"California Census Tracts geodataframe has been partitioned into spatially coherent chunks via dask-geopandas spatial_shuffle().\")\n",
    "\n",
    "    clipped_gdf = ddf.sjoin(ca_ddf, how='inner', predicate='intersects').compute()\n",
    "    print(f\"{varname} clipped to California Census Tract boundaries via dask-geopandas sjoin() using the 'intersection' method.\")\n",
    "    # try to minimize the data by dropping unnecessary columns\n",
    "    clipped_gdf = clipped_gdf.reset_index()\n",
    "    to_drop=['USCB_STATEFP', 'USCB_COUNTYFP', 'USCB_TRACTCE', \n",
    "       'USCB_NAMELSAD', 'USCB_MTFCC', 'USCB_FUNCSTAT',\n",
    "       'USCB_ALAND', 'USCB_AWATER', 'USCB_INTPTLAT', 'USCB_INTPTLON',\n",
    "        'hilbert_distance']\n",
    "\n",
    "    clipped_gdf = clipped_gdf.drop(columns=to_drop)\n",
    "    \n",
    "    print(f\"Additional comments: {additional_comments}.\") # eg, code rerun, bug fix, etc\n",
    "    \n",
    "    # upload it to S3\n",
    "    s3_client = boto3.client('s3')  \n",
    "    bucket_name = 'ca-climate-index' \n",
    "\n",
    "    if shp_fname.endswith('.zip'):\n",
    "        shp_fname = shp_fname.replace(\n",
    "            'zip+',\n",
    "            '')\n",
    "    ddf_part = dask_geopandas.from_geopandas(clipped_gdf, npartitions=5)\n",
    "    \n",
    "    if varname==\"climate_iowa_mesonet_flash_flood_warnings\":\n",
    "        print(\"The resulting database is too large to save as a single file and will be partitioned into 5 chunks.\")\n",
    "        for i in range(len(list(ddf_part.partitions))):\n",
    "            df = ddf_part.partitions[i].compute()\n",
    "            dest_f = shp_fname.replace(shp_fname.split('/')[-1],f\"{varname}_{i}.parquet.gzip\")\n",
    "            print(f\"Dataframe partition {i} saved to: {dest_f}\")\n",
    "            df.to_parquet(dest_f, compression='gzip')\n",
    "    else:\n",
    "        dest_f = shp_fname.replace(shp_fname.split('/')[-1],f\"{varname}.parquet.gzip\")\n",
    "        dest_f = re.sub(r'1_pull_data|2a_subset', '2b_reproject', dest_f)\n",
    "        \n",
    "        ddf_part.to_parquet(dest_f, compression='gzip')\n",
    "        print(f\"Database saved to: {dest_f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the CSV with the data details\n",
    "ref_file = sys.path[-1]+'/metadata/Full Data Pipeline Notes - 1_ Pull.csv'\n",
    "df = pd.read_csv(ref_file)\n",
    "\n",
    "# subset for shapefiles\n",
    "ref_df = df.fillna('N/A')\n",
    "# comment out for now as 'Pulled Format' column not updated\n",
    "# ref_df = ref_df[ref_df[\"Pulled Format\"].str.contains(\"shp\")]\n",
    "\n",
    "### Define the path\n",
    "path1 = \"1_pull_data\"\n",
    "path2 = \"2a_subset\"\n",
    "#  build a list of shapefiles in the above s3 paths\n",
    "my_list = list_geospatial_files(path1) \n",
    "my_list += list_geospatial_files(path2)\n",
    "\n",
    "# read in CA census tiger file\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "# need to rename columns so we don't have any duplicates in the final geodatabase\n",
    "column_names = ca_boundaries.columns\n",
    "new_column_names = [\"USCB_\"+column for column in column_names if column != \"geometry\"]\n",
    "ca_boundaries = ca_boundaries.rename(columns=dict(zip(column_names, new_column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_files = [\n",
    "    'climate_iowa_mesonet_flash_flood_warnings', \n",
    "    'climate_koordinates_floodplain', \n",
    "    'climate_iowa_mesonet_wildfire_warnings',\n",
    "    'governance_usda_watershed_risk',\n",
    "    'governance_usda_fuel_reduction'\n",
    "]\n",
    "\n",
    "# get reference dataframe for the above large files\n",
    "large_df = ref_df[ref_df.Variable.isin(large_files)]\n",
    "# build list of paths corresponding to the large files\n",
    "large_list = [f for f in my_list if f.split('/')[-1] in large_df[\"File Name\"].values]\n",
    "# dictionary to map file names to variable names\n",
    "fname_dict = dict(zip(large_df[\"File Name\"].values, large_df[\"Variable\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fpath in large_list:\n",
    "    # get the file name by itself (no subdirectories)\n",
    "    fname = fpath.split('/')[-1]\n",
    "    varname = fname_dict[fname]\n",
    "    if (varname=='governance_usda_fuel_reduction'): # skip for now - error with datetime formatting\n",
    "        continue\n",
    "    print(varname)\n",
    "    reproject_large_shapefile(fpath, ca_boundaries, varname=varname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
