{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-05T20:58:04.106627Z",
     "iopub.status.busy": "2024-03-05T20:58:04.106120Z",
     "iopub.status.idle": "2024-03-05T20:58:05.248664Z",
     "shell.execute_reply": "2024-03-05T20:58:05.247996Z",
     "shell.execute_reply.started": "2024-03-05T20:58:04.106608Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import sys\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "import dask_geopandas\n",
    "import re\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-05T20:58:05.250141Z",
     "iopub.status.busy": "2024-03-05T20:58:05.249951Z",
     "iopub.status.idle": "2024-03-05T20:58:05.262992Z",
     "shell.execute_reply": "2024-03-05T20:58:05.262335Z",
     "shell.execute_reply.started": "2024-03-05T20:58:05.250127Z"
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "def list_geospatial_files(path):\n",
    "    \"\"\" Build a list of shapefile URIs contained in S3 folder \"\"\"\n",
    "    # initiate empty list for s3 URIs\n",
    "    all_shapefiles = []\n",
    "    bucket_name = 'ca-climate-index' \n",
    "    # initiate s3 session\n",
    "    session = boto3.Session()\n",
    "    # use the session to get the resource\n",
    "    s3 = session.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    # iterate through directory\n",
    "    for obj in my_bucket.objects.filter(\n",
    "        Prefix=path):\n",
    "        # build list of shapefile URIs\n",
    "        if obj.key.endswith('.zip'):\n",
    "            # preceding the URI with 'zip' lets you read in the file without downloading, unzipping, etc\n",
    "            s3_uri = f\"zip+s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "        elif obj.key.endswith('.shp'):\n",
    "            s3_uri = \"s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "    return all_shapefiles\n",
    "\n",
    "# @append_metadata\n",
    "def reproject_large_shapefile(shp_fname, ca_boundaries, varname='', additional_comments='N/A'):\n",
    "    \"\"\"Given S3 URI which corresponds to a data shapefile and a shapefile\n",
    "    with California Census Tracts, (1) reproject the data shapefile to the CRS of the California Census Tracts, \n",
    "    (2) clip to California Census Tracts, and (3) send it off to S3. This code differs from the \n",
    "    reproject_shapefile() function by utilizing dask-geopandas to manipulate large datasets.\"\"\"    \n",
    "\n",
    "    # read in shapefile of interest from S3 \n",
    "    print(f\"Reading in file: {shp_fname}.\")\n",
    "    gdf = dask_geopandas.read_file(shp_fname, npartitions=10)\n",
    "    orig_crs = gdf.crs\n",
    "    print(f\"Original CRS of data for {varname}: {orig_crs}\")\n",
    "    # check the current coordinate system of the census tracts data\n",
    "    print(f\"CRS of Census Tracts Shapefile: {ca_boundaries.crs}\")\n",
    "\n",
    "    if (orig_crs==ca_boundaries.crs):   \n",
    "        print(f\"Do not need to reproject {varname} since it is already in the same projection as the Census Tracts Shapefile.\")\n",
    "    else:       \n",
    "        gdf = gdf.to_crs(ca_boundaries.crs)\n",
    "        print(f\"{varname} reprojected from {orig_crs} to {gdf.crs} with geopandas to_crs() function.\")\n",
    "\n",
    "    # one file has missing geometries so we drop them here\n",
    "    if varname == 'governance_usda_fuel_reduction': \n",
    "        print(\"Dropping rows with empty geometries (invalid data).\")\n",
    "        # drop a problematic and unnecessary column first\n",
    "        gdf = gdf.drop(columns=['REV_DATE'])\n",
    "        #orig_len = len(gdf.index) #597115\n",
    "        orig_len = 597115\n",
    "        gdf_isna = gdf[\"geometry\"].isna()\n",
    "        gdf = gdf[~gdf_isna]\n",
    "        # valid_len = len(gdf.index) #525832\n",
    "        valid_len = 525832\n",
    "        pct_dropped = ((orig_len - valid_len)/orig_len)*100\n",
    "        print(f\"Dropped {pct_dropped:.2f}% of data entries due to empty geometries.\")\n",
    "        # fix up columns with time stamps\n",
    "        for column in gdf.columns:\n",
    "            if 'DATE' in column:\n",
    "                gdf[column]  = pd.to_datetime(gdf[column], infer_datetime_format=True)\n",
    "           \n",
    "    # shuffle the geodataframe into spatially coherent partitions\n",
    "    ddf = gdf.spatial_shuffle()\n",
    "    print(f\"{varname} geodataframe has been partitioned into spatially coherent chunks via dask-geopandas spatial_shuffle().\")\n",
    "\n",
    "    ca_ddf = dask_geopandas.from_geopandas(ca_boundaries, npartitions=10)\n",
    "    ca_ddf = ca_ddf.spatial_shuffle()\n",
    "    print(f\"California Census Tracts geodataframe has been partitioned into spatially coherent chunks via dask-geopandas spatial_shuffle().\")\n",
    "\n",
    "    clipped_gdf = ddf.sjoin(ca_ddf, how='inner', predicate='intersects').compute()\n",
    "    print(f\"{varname} clipped to California Census Tract boundaries via dask-geopandas sjoin() using the 'intersection' method.\")\n",
    "    # try to minimize the data by dropping unnecessary columns\n",
    "    clipped_gdf = clipped_gdf.reset_index()\n",
    "    to_drop=['USCB_STATEFP', 'USCB_COUNTYFP', 'USCB_TRACTCE', \n",
    "       'USCB_NAMELSAD', 'USCB_MTFCC', 'USCB_FUNCSTAT',\n",
    "       'USCB_ALAND', 'USCB_AWATER', 'USCB_INTPTLAT', 'USCB_INTPTLON',\n",
    "        'hilbert_distance']\n",
    "    clipped_gdf = clipped_gdf.drop(columns=to_drop)\n",
    "    \n",
    "    print(f\"Additional comments: {additional_comments}.\") # eg, code rerun, bug fix, etc\n",
    "    \n",
    "    # upload it to S3\n",
    "    s3_client = boto3.client('s3')  \n",
    "    bucket_name = 'ca-climate-index' \n",
    "\n",
    "    if shp_fname.endswith('.zip'):\n",
    "        shp_fname = shp_fname.replace(\n",
    "            'zip+',\n",
    "            '')\n",
    "    ddf_part = dask_geopandas.from_geopandas(clipped_gdf, npartitions=5)\n",
    "    if varname==\"climate_iowa_mesonet_flash_flood_warnings\":\n",
    "        print(\"The resulting database is too large to save as a single file and will be partitioned into 5 files.\")\n",
    "        for i in range(len(list(ddf_part.partitions))):\n",
    "            df = ddf_part.partitions[i].compute()\n",
    "            dest_f = shp_fname.replace(shp_fname.split('/')[-1],f\"{varname}_{i}.parquet.gzip\")\n",
    "            print(f\"Dataframe partition {i} saved to: {dest_f}\")\n",
    "            df.to_parquet(dest_f, compression='gzip')\n",
    "    else:\n",
    "        dest_f = shp_fname.replace(shp_fname.split('/')[-1],f\"{varname}.parquet.gzip\")\n",
    "        dest_f = re.sub(r'1_pull_data|2a_subset', '2b_reproject', dest_f)\n",
    "        \n",
    "        ddf_part.to_parquet(dest_f, compression='gzip')\n",
    "        print(f\"Database saved to: {dest_f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-05T20:58:17.253609Z",
     "iopub.status.busy": "2024-03-05T20:58:17.253335Z",
     "iopub.status.idle": "2024-03-05T20:58:34.239573Z",
     "shell.execute_reply": "2024-03-05T20:58:34.239005Z",
     "shell.execute_reply.started": "2024-03-05T20:58:17.253594Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the CSV with the data details\n",
    "ref_file = sys.path[-1]+'/metadata/Full Data Pipeline Notes - 1_ Pull.csv'\n",
    "df = pd.read_csv(ref_file)\n",
    "\n",
    "# subset for shapefiles\n",
    "ref_df = df.fillna('N/A')\n",
    "# comment out for now as 'Pulled Format' column not updated\n",
    "ref_df = ref_df[\n",
    "(ref_df[\"Pulled Format\"].str.contains(\"shp\")) \n",
    "| (ref_df[\"Pulled Format\"].str.contains(\"gdb\"))\n",
    "]\n",
    "\n",
    "### Define the path\n",
    "path1 = \"1_pull_data\"\n",
    "path2 = \"2a_subset\"\n",
    "#  build a list of shapefiles in the above s3 paths\n",
    "my_list = list_geospatial_files(path1) \n",
    "my_list += list_geospatial_files(path2)\n",
    "\n",
    "# read in CA census tiger file\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "# need to rename columns so we don't have any duplicates in the final geodatabase\n",
    "column_names = ca_boundaries.columns\n",
    "new_column_names = [\"USCB_\"+column for column in column_names if column != \"geometry\"]\n",
    "ca_boundaries = ca_boundaries.rename(columns=dict(zip(column_names, new_column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T18:17:52.203970Z",
     "iopub.status.busy": "2024-02-28T18:17:52.203776Z",
     "iopub.status.idle": "2024-02-28T18:17:52.210959Z",
     "shell.execute_reply": "2024-02-28T18:17:52.210344Z",
     "shell.execute_reply.started": "2024-02-28T18:17:52.203957Z"
    }
   },
   "outputs": [],
   "source": [
    "large_files = [\n",
    "    'climate_iowa_mesonet_flash_flood_warnings', \n",
    "    'climate_koordinates_floodplain', \n",
    "    'climate_iowa_mesonet_wildfire_warnings',\n",
    "    'governance_usda_watershed_risk',\n",
    "    'governance_usda_fuel_reduction'\n",
    "]\n",
    "\n",
    "# get reference dataframe for the above large files\n",
    "large_df = ref_df[ref_df.Variable.isin(large_files)]\n",
    "# build list of paths corresponding to the large files\n",
    "large_list = [f for f in my_list if f.split('/')[-1] in large_df[\"File Name\"].values]\n",
    "# dictionary to map file names to variable names\n",
    "fname_dict = dict(zip(large_df[\"File Name\"].values, large_df[\"Variable\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fpath in large_list:\n",
    "    # get the file name by itself (no subdirectories)\n",
    "    fname = fpath.split('/')[-1]\n",
    "    varname = fname_dict[fname]\n",
    "    # skip problem file for now\n",
    "    if (varname!='governance_usda_fuel_reduction'):\n",
    "        continue\n",
    "    print(varname)\n",
    "    reproject_large_shapefile(fpath, ca_boundaries, varname=varname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
