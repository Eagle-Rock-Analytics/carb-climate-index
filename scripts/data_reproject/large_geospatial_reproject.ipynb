{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cal-CRAI Reprojection -- LARGE geospatial file inputs\n",
    "This notebook processes geospatial data files, of large complex size, for reprojection where necessary and includes the necessary metadata pieces for clarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import sys\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "import dask_geopandas\n",
    "import re\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "def list_geospatial_files(path):\n",
    "    \"\"\" Build a list of shapefile URIs contained in S3 folder \"\"\"\n",
    "    # initiate empty list for s3 URIs\n",
    "    all_shapefiles = []\n",
    "    bucket_name = 'ca-climate-index' \n",
    "    # initiate s3 session\n",
    "    session = boto3.Session()\n",
    "    # use the session to get the resource\n",
    "    s3 = session.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    # iterate through directory\n",
    "    for obj in my_bucket.objects.filter(\n",
    "        Prefix=path):\n",
    "        # build list of shapefile URIs\n",
    "        if obj.key.endswith('.zip'):\n",
    "            # preceding the URI with 'zip' lets you read in the file without downloading, unzipping, etc\n",
    "            s3_uri = f\"zip+s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "        elif obj.key.endswith('.shp'):\n",
    "            s3_uri = \"s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "    return all_shapefiles\n",
    "\n",
    "@append_metadata\n",
    "def reproject_large_shapefile(shp_fname, ca_boundaries, run_code=True, varname=''):\n",
    "    \"\"\"\n",
    "    Reprojects large shapefiles to California Census Tract Coordinate Reference System, then clips to these \n",
    "    CA tracts, and uploads to AWS S3. This code differs from the \n",
    "    reproject_shapefile() function by utilizing dask-geopandas to manipulate large datasets. \n",
    "\n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in\n",
    "    ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "     \n",
    "    Methods\n",
    "    -------\n",
    "    Use dask-geopandas to work with the large datasets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    shp_fname: string\n",
    "        filename of the large geodataframe shapefile\n",
    "    ca_boundaries: \n",
    "        read-in gpd file of California Census Tracts\n",
    "    run_code: bool\n",
    "        if True, code will run. If false, just metadata file will be updated\n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    large_geospatial_reproject.ipynb    \n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')  \n",
    "    bucket_name = 'ca-climate-index' \n",
    "\n",
    "    if shp_fname.endswith('.zip'):\n",
    "        shp_fname_intermediate = shp_fname.replace(\n",
    "            'zip+',\n",
    "            '')\n",
    "    else:\n",
    "        shp_fname_intermediate = shp_fname\n",
    "                \n",
    "    print('Data transformation: Reproject to standard coordinate reference system: 4269.')\n",
    "    if varname == 'governance_usda_fuel_reduction':\n",
    "        print(\"Data transformation: Subsetted from countrywide to California.\")\n",
    "    \n",
    "    print('Data transformation: sjoin large geodata with CA census tract boundaries data.')\n",
    "    \n",
    "    if varname==\"climate_iowa_mesonet_flash_flood_warnings\":\n",
    "        print(\n",
    "                \"Data transformation: Saved as 10 spatially coherent parquet files because\"\n",
    "                +\" the resulting dataset is too large to be saved as one file.\"\n",
    "        )\n",
    "        print(f\"Database saved to: s3://ca-climate-index/2b_reproject/climate_risk/flood/exposure/isu_environmental_mesonet/\")\n",
    "    else:\n",
    "        dest_f = shp_fname_intermediate.replace(\n",
    "            shp_fname_intermediate.split('/')[-1],f\"{varname}.parquet.gzip\")\n",
    "        dest_f = re.sub(r'1_pull_data|2a_subset', '2b_reproject', dest_f)\n",
    "        print('Data transformation: Saved as a parquet file')\n",
    "        print(f\"Database saved to: {dest_f}\")\n",
    "        \n",
    "    if run_code==True:\n",
    "        # read in shapefile of interest from S3 \n",
    "        gdf = dask_geopandas.read_file(shp_fname, npartitions=10)\n",
    "        orig_crs = gdf.crs\n",
    "\n",
    "        if (orig_crs==ca_boundaries.crs):   \n",
    "            print(f\"Do not need to reproject {varname} since it is already in the same projection as the Census Tracts Shapefile.\")\n",
    "        else:       \n",
    "            gdf = gdf.to_crs(ca_boundaries.crs)\n",
    "\n",
    "        # one file needs to be subsetted to California first\n",
    "        if varname == 'governance_usda_fuel_reduction':         \n",
    "            df_list = []\n",
    "            gdf = gdf.drop(columns=['REV_DATE'])\n",
    "            for i in range(len(list(gdf.partitions))):\n",
    "                edf = gdf.partitions[i].compute()\n",
    "                cdf = edf.loc[edf['STATE_ABBR']=='CA']\n",
    "            df_list.append(cdf) \n",
    "            all_ca_df = pd.concat(df_list)\n",
    "            clipped_gdf = all_ca_df.sjoin(ca_boundaries, how='inner', predicate='intersects')            \n",
    "        else:       \n",
    "            # shuffle the geodataframes into spatially coherent partitions\n",
    "            ddf = gdf.spatial_shuffle()       \n",
    "            ca_ddf = dask_geopandas.from_geopandas(ca_boundaries, npartitions=10)\n",
    "            ca_ddf = ca_ddf.spatial_shuffle()      \n",
    "            clipped_gdf = ddf.sjoin(ca_ddf, how='inner', predicate='intersects').compute()\n",
    "            # try to minimize the data by dropping unnecessary columns\n",
    "        \n",
    "        clipped_gdf = clipped_gdf.reset_index()\n",
    "        to_drop=['USCB_STATEFP', 'USCB_COUNTYFP', 'USCB_TRACTCE', \n",
    "        'USCB_NAMELSAD', 'USCB_MTFCC', 'USCB_FUNCSTAT',\n",
    "        'USCB_ALAND', 'USCB_AWATER', 'USCB_INTPTLAT', 'USCB_INTPTLON']\n",
    "        clipped_gdf = clipped_gdf.drop(columns=to_drop)        \n",
    "        ddf_part = dask_geopandas.from_geopandas(clipped_gdf, npartitions=10)\n",
    "        \n",
    "        if varname==\"climate_iowa_mesonet_flash_flood_warnings\":\n",
    "            for i in range(len(list(ddf_part.partitions))):\n",
    "                df = ddf_part.partitions[i].compute()\n",
    "                # upload it to s3\n",
    "                dest_f = shp_fname_intermediate.replace(\n",
    "                    shp_fname_intermediate.split('/')[-1],f\"{varname}_{i}.parquet.gzip\")\n",
    "                df.to_parquet(dest_f, compression='gzip')\n",
    "        else:\n",
    "            # upload it to s3\n",
    "            ddf_part.to_parquet(dest_f, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in the CSV with the data details\n",
    "ref_file = sys.path[-1]+'/metadata/Full Data Pipeline Notes - 1_ Pull.csv'\n",
    "df = pd.read_csv(ref_file)\n",
    "\n",
    "# subset for shapefiles\n",
    "ref_df = df.fillna('N/A')\n",
    "# comment out for now as 'Pulled Format' column not updated\n",
    "ref_df = ref_df[\n",
    "(ref_df[\"Pulled Format\"].str.contains(\"shp\")) \n",
    "| (ref_df[\"Pulled Format\"].str.contains(\"gdb\"))\n",
    "]\n",
    "\n",
    "### Define the path\n",
    "path1 = \"1_pull_data\"\n",
    "path2 = \"2a_subset\"\n",
    "#  build a list of shapefiles in the above s3 paths\n",
    "my_list = list_geospatial_files(path1) \n",
    "my_list += list_geospatial_files(path2)\n",
    "\n",
    "# read in CA census tiger file\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "# need to rename columns so we don't have any duplicates in the final geodatabase\n",
    "column_names = ca_boundaries.columns\n",
    "new_column_names = [\"USCB_\"+column for column in column_names if column != \"geometry\"]\n",
    "ca_boundaries = ca_boundaries.rename(columns=dict(zip(column_names, new_column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_files = [\n",
    "    'climate_iowa_mesonet_flash_flood_warnings', \n",
    "    'climate_koordinates_floodplain', \n",
    "    'climate_iowa_mesonet_wildfire_warnings',\n",
    "    'governance_usda_watershed_risk',\n",
    "    'governance_usda_fuel_reduction'\n",
    "]\n",
    "\n",
    "# get reference dataframe for the above large files\n",
    "large_df = ref_df[ref_df.Variable.isin(large_files)]\n",
    "# build list of paths corresponding to the large files\n",
    "large_list = [f for f in my_list if f.split('/')[-1] in large_df[\"File Name\"].values]\n",
    "# dictionary to map file names to variable names\n",
    "fname_dict = dict(zip(large_df[\"File Name\"].values, large_df[\"Variable\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fpath in large_list:\n",
    "    # get the file name by itself (no subdirectories)\n",
    "    fname = fpath.split('/')[-1]\n",
    "    varname = fname_dict[fname]\n",
    "    print(varname)\n",
    "    reproject_large_shapefile(fpath, ca_boundaries, run_code=False, varname=varname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
