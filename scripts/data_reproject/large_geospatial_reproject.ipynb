{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T23:17:29.008596Z",
     "iopub.status.busy": "2024-04-22T23:17:29.008325Z",
     "iopub.status.idle": "2024-04-22T23:17:29.920835Z",
     "shell.execute_reply": "2024-04-22T23:17:29.920131Z",
     "shell.execute_reply.started": "2024-04-22T23:17:29.008574Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import sys\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "import dask_geopandas\n",
    "import re\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T23:17:29.924255Z",
     "iopub.status.busy": "2024-04-22T23:17:29.923803Z",
     "iopub.status.idle": "2024-04-22T23:17:29.937986Z",
     "shell.execute_reply": "2024-04-22T23:17:29.937341Z",
     "shell.execute_reply.started": "2024-04-22T23:17:29.924235Z"
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "def list_geospatial_files(path):\n",
    "    \"\"\" Build a list of shapefile URIs contained in S3 folder \"\"\"\n",
    "    # initiate empty list for s3 URIs\n",
    "    all_shapefiles = []\n",
    "    bucket_name = 'ca-climate-index' \n",
    "    # initiate s3 session\n",
    "    session = boto3.Session()\n",
    "    # use the session to get the resource\n",
    "    s3 = session.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    # iterate through directory\n",
    "    for obj in my_bucket.objects.filter(\n",
    "        Prefix=path):\n",
    "        # build list of shapefile URIs\n",
    "        if obj.key.endswith('.zip'):\n",
    "            # preceding the URI with 'zip' lets you read in the file without downloading, unzipping, etc\n",
    "            s3_uri = f\"zip+s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "        elif obj.key.endswith('.shp'):\n",
    "            s3_uri = \"s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "    return all_shapefiles\n",
    "\n",
    "@append_metadata\n",
    "def reproject_large_shapefile(shp_fname, ca_boundaries, run_code=True, varname=''):\n",
    "    \"\"\"\n",
    "    Reprojects large shapefiles to California Census Tract Coordinate Reference System, then clips to these \n",
    "    CA tracts, and uploads to AWS S3. This code differs from the \n",
    "    reproject_shapefile() function by utilizing dask-geopandas to manipulate large datasets. \n",
    "     \n",
    "    Methods\n",
    "    -------\n",
    "    Use dask-geopandas to work with the large datasets\n",
    "    \n",
    "    Script\n",
    "    ------\n",
    "    large_geospatial_reproject.ipynb\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    shp_fname: string\n",
    "        filename of the large geodataframe shapefile\n",
    "    ca_boundaries: \n",
    "        read-in gpd file of California Census Tracts\n",
    "    run_code: bool\n",
    "        if True, code will run. If false, just metadata file will be updated\n",
    "    \n",
    "    \"\"\"\n",
    "    print('Data transformation: Reproject to CRSIs standard coordinate reference system: 4269')\n",
    "    print('Data transformation: sjoin large geodata with CA boundaries data')\n",
    "    print('Data transformation: Clip to California boundaries')\n",
    "    print('Data transformation: Save as a parquet file')\n",
    "    \n",
    "    if run_code==True:\n",
    "        # read in shapefile of interest from S3 \n",
    "        #print(f\"Reading in file: {shp_fname}.\")\n",
    "        gdf = dask_geopandas.read_file(shp_fname, npartitions=10)\n",
    "        orig_crs = gdf.crs\n",
    "        #print(f\"Original CRS of data for {varname}: {orig_crs}\")\n",
    "        # check the current coordinate system of the census tracts data\n",
    "        #print(f\"CRS of Census Tracts Shapefile: {ca_boundaries.crs}\")\n",
    "\n",
    "        if (orig_crs==ca_boundaries.crs):   \n",
    "            print(f\"Do not need to reproject {varname} since it is already in the same projection as the Census Tracts Shapefile.\")\n",
    "        else:       \n",
    "            gdf = gdf.to_crs(ca_boundaries.crs)\n",
    "            #print(f\"{varname} reprojected from {orig_crs} to {gdf.crs} with geopandas to_crs() function.\")\n",
    "\n",
    "        # one file needs to be subsetted to California first\n",
    "        if varname == 'governance_usda_fuel_reduction':         \n",
    "            df_list = []\n",
    "            gdf = gdf.drop(columns=['REV_DATE'])\n",
    "            for i in range(len(list(gdf.partitions))):\n",
    "                edf = gdf.partitions[i].compute()\n",
    "                cdf = edf.loc[edf['STATE_ABBR']=='CA']\n",
    "            df_list.append(cdf) \n",
    "            all_ca_df = pd.concat(df_list)\n",
    "            #print(f\"{varname} has been subsetted from countrywide to California.\")\n",
    "            clipped_gdf = all_ca_df.sjoin(ca_boundaries, how='inner', predicate='intersects')\n",
    "            #print(f\"{varname} clipped to California Census Tract boundaries via geopandas sjoin() using the 'intersection' method.\")\n",
    "            \n",
    "        else:       \n",
    "            # shuffle the geodataframe into spatially coherent partitions\n",
    "            ddf = gdf.spatial_shuffle()\n",
    "            #print(f\"{varname} geodataframe has been partitioned into spatially coherent chunks via dask-geopandas spatial_shuffle().\")\n",
    "        \n",
    "            ca_ddf = dask_geopandas.from_geopandas(ca_boundaries, npartitions=10)\n",
    "            ca_ddf = ca_ddf.spatial_shuffle()\n",
    "            #print(f\"California Census Tracts geodataframe has been partitioned into spatially coherent chunks via dask-geopandas spatial_shuffle().\")\n",
    "        \n",
    "            clipped_gdf = ddf.sjoin(ca_ddf, how='inner', predicate='intersects').compute()\n",
    "            #print(f\"{varname} clipped to California Census Tract boundaries via dask-geopandas sjoin() using the 'intersection' method.\")\n",
    "            # try to minimize the data by dropping unnecessary columns\n",
    "        clipped_gdf = clipped_gdf.reset_index()\n",
    "        to_drop=['USCB_STATEFP', 'USCB_COUNTYFP', 'USCB_TRACTCE', \n",
    "        'USCB_NAMELSAD', 'USCB_MTFCC', 'USCB_FUNCSTAT',\n",
    "        'USCB_ALAND', 'USCB_AWATER', 'USCB_INTPTLAT', 'USCB_INTPTLON']\n",
    "        clipped_gdf = clipped_gdf.drop(columns=to_drop)\n",
    "        \n",
    "        #print(f\"Additional comments: {additional_comments}.\") # eg, code rerun, bug fix, etc\n",
    "        \n",
    "        # upload it to S3\n",
    "        s3_client = boto3.client('s3')  \n",
    "        bucket_name = 'ca-climate-index' \n",
    "\n",
    "        if shp_fname.endswith('.zip'):\n",
    "            shp_fname = shp_fname.replace(\n",
    "                'zip+',\n",
    "                '')\n",
    "        ddf_part = dask_geopandas.from_geopandas(clipped_gdf, npartitions=10)\n",
    "        if varname==\"climate_iowa_mesonet_flash_flood_warnings\":\n",
    "            #print(\"The resulting database is too large to save as a single file and will be partitioned into 10 files.\")\n",
    "            for i in range(len(list(ddf_part.partitions))):\n",
    "                df = ddf_part.partitions[i].compute()\n",
    "                dest_f = shp_fname.replace(shp_fname.split('/')[-1],f\"{varname}_{i}.parquet.gzip\")\n",
    "                print(f\"Dataframe partition {i} saved to: {dest_f}\")\n",
    "                df.to_parquet(dest_f, compression='gzip')\n",
    "        else:\n",
    "            dest_f = shp_fname.replace(shp_fname.split('/')[-1],f\"{varname}.parquet.gzip\")\n",
    "            dest_f = re.sub(r'1_pull_data|2a_subset', '2b_reproject', dest_f)\n",
    "            \n",
    "            ddf_part.to_parquet(dest_f, compression='gzip')\n",
    "            #print(f\"Database saved to: {dest_f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T23:17:30.367466Z",
     "iopub.status.busy": "2024-04-22T23:17:30.366968Z",
     "iopub.status.idle": "2024-04-22T23:17:50.364195Z",
     "shell.execute_reply": "2024-04-22T23:17:50.363267Z",
     "shell.execute_reply.started": "2024-04-22T23:17:30.367446Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the CSV with the data details\n",
    "ref_file = sys.path[-1]+'/metadata/Full Data Pipeline Notes - 1_ Pull.csv'\n",
    "df = pd.read_csv(ref_file)\n",
    "\n",
    "# subset for shapefiles\n",
    "ref_df = df.fillna('N/A')\n",
    "# comment out for now as 'Pulled Format' column not updated\n",
    "ref_df = ref_df[\n",
    "(ref_df[\"Pulled Format\"].str.contains(\"shp\")) \n",
    "| (ref_df[\"Pulled Format\"].str.contains(\"gdb\"))\n",
    "]\n",
    "\n",
    "### Define the path\n",
    "path1 = \"1_pull_data\"\n",
    "path2 = \"2a_subset\"\n",
    "#  build a list of shapefiles in the above s3 paths\n",
    "my_list = list_geospatial_files(path1) \n",
    "my_list += list_geospatial_files(path2)\n",
    "\n",
    "# read in CA census tiger file\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "# need to rename columns so we don't have any duplicates in the final geodatabase\n",
    "column_names = ca_boundaries.columns\n",
    "new_column_names = [\"USCB_\"+column for column in column_names if column != \"geometry\"]\n",
    "ca_boundaries = ca_boundaries.rename(columns=dict(zip(column_names, new_column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T23:17:50.365453Z",
     "iopub.status.busy": "2024-04-22T23:17:50.365241Z",
     "iopub.status.idle": "2024-04-22T23:17:50.371595Z",
     "shell.execute_reply": "2024-04-22T23:17:50.370780Z",
     "shell.execute_reply.started": "2024-04-22T23:17:50.365427Z"
    }
   },
   "outputs": [],
   "source": [
    "large_files = [\n",
    "    'climate_iowa_mesonet_flash_flood_warnings', \n",
    "    'climate_koordinates_floodplain', \n",
    "    'climate_iowa_mesonet_wildfire_warnings',\n",
    "    'governance_usda_watershed_risk',\n",
    "    'governance_usda_fuel_reduction'\n",
    "]\n",
    "\n",
    "# get reference dataframe for the above large files\n",
    "large_df = ref_df[ref_df.Variable.isin(large_files)]\n",
    "# build list of paths corresponding to the large files\n",
    "large_list = [f for f in my_list if f.split('/')[-1] in large_df[\"File Name\"].values]\n",
    "# dictionary to map file names to variable names\n",
    "fname_dict = dict(zip(large_df[\"File Name\"].values, large_df[\"Variable\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T23:17:50.372794Z",
     "iopub.status.busy": "2024-04-22T23:17:50.372370Z",
     "iopub.status.idle": "2024-04-22T23:38:31.774147Z",
     "shell.execute_reply": "2024-04-22T23:38:31.773260Z",
     "shell.execute_reply.started": "2024-04-22T23:17:50.372767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate_koordinates_floodplain\n",
      "climate_iowa_mesonet_wildfire_warnings\n",
      "governance_usda_fuel_reduction\n",
      "governance_usda_watershed_risk\n",
      "climate_iowa_mesonet_flash_flood_warnings\n"
     ]
    }
   ],
   "source": [
    "for fpath in large_list:\n",
    "    # get the file name by itself (no subdirectories)\n",
    "    fname = fpath.split('/')[-1]\n",
    "    varname = fname_dict[fname]\n",
    "    print(varname)\n",
    "    reproject_large_shapefile(fpath, ca_boundaries, run_code=False, varname=varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
