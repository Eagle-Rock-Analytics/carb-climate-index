{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T22:24:54.458321Z",
     "iopub.status.busy": "2024-08-02T22:24:54.458196Z",
     "iopub.status.idle": "2024-08-02T22:24:55.443404Z",
     "shell.execute_reply": "2024-08-02T22:24:55.442681Z",
     "shell.execute_reply.started": "2024-08-02T22:24:54.458307Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xarray as xr\n",
    "import boto3\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# suppress pandas purely educational warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import pull_csv_from_directory, upload_csv_aws, filter_counties\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T22:24:55.445443Z",
     "iopub.status.busy": "2024-08-02T22:24:55.444783Z",
     "iopub.status.idle": "2024-08-02T22:24:55.451588Z",
     "shell.execute_reply": "2024-08-02T22:24:55.450904Z",
     "shell.execute_reply.started": "2024-08-02T22:24:55.445424Z"
    }
   },
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def reproject_nlcd_impervious_lands(ds, ca_boundaries, run_code=True, varname=''):\n",
    "    \"\"\"\n",
    "    Reprojects the CA-wide USGS impervious lands zarr to California Census Tract Coordinate Reference System, \n",
    "    then clips to these CA tracts, and uploads to AWS S3. This code differs from the \n",
    "    reproject_shapefile() function by utilizing dask-geopandas to manipulate large datasets and saving the result\n",
    "    as 13 parquet files. \n",
    "\n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in\n",
    "    ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "     \n",
    "    Methods\n",
    "    -------\n",
    "    Use dask-geopandas to work with the large datasets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    zarr_fname: string\n",
    "        filename of the USGS impervious lands zarr\n",
    "    ca_boundaries: \n",
    "        read-in gpd file of California Census Tracts\n",
    "    run_code: bool\n",
    "        if True, code will run. If false, just metadata file will be updated\n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    large_geospatial_reproject.ipynb    \n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')  \n",
    "    bucket_name = 'ca-climate-index' \n",
    "    var = 'natural_usgs_impervious'\n",
    "    dest_f = in_fname.replace(\n",
    "        in_fname.split('/')[-1],f\"{var}.parquet.gzip\")\n",
    "    dest_f = re.sub(r'1_pull_data', '2b_reproject', dest_f)\n",
    "                \n",
    "    print('Data transformation: Reproject to standard coordinate reference system: 4269.')    \n",
    "    print('Data transformation: sjoin large geodata with CA census tract boundaries data.')    \n",
    "    print(\n",
    "            \"Data transformation: Saved as multiple parquet files because\"\n",
    "            +\" the resulting dataset is too large to be saved as one file.\"\n",
    "    )\n",
    "    print(f\"Parquets saved to: s3://ca-climate-index/2b_reproject/natural_systems/ecosystem_condition/usgs/\")\n",
    "        \n",
    "    if run_code==True:\n",
    "        orig_crs = ds.spatial_ref.attrs[\"crs_wkt\"]\n",
    "        cb_crs = ca_boundaries.crs\n",
    "        \n",
    "        geometry = [Point(xy) for xy in zip(ds['x'], ds['y'])]\n",
    "        df = ds.to_dask_dataframe()\n",
    "        geom = pd.Series(geometry)\n",
    "        df[\"geometry\"] = geom\n",
    "        df = df[[\"impervious_surface\",\"geometry\"]]\n",
    "        print('made dask df')\n",
    "\n",
    "        for i in range(len(list(df.partitions))):\n",
    "            part_df = df.partitions[i].compute()\n",
    "            print(f\"partition {i} read.\")\n",
    "            gdf = gpd.GeoDataFrame(part_df, geometry=part_df.geometry)\n",
    "            gdf = gdf.set_crs(orig_crs)\n",
    "            gdf = gdf.to_crs(cb_crs)  \n",
    "            gdf = gdf.sjoin(ca_boundaries, how='inner', predicate='intersects')\n",
    "            gdf = gdf.replace(127.0,np.nan) # replace no-data values\n",
    "            \n",
    "            dest_f = dest_f.replace(\n",
    "                dest_f.split('/')[-1],f\"ca_clipped_{var}_{i}.parquet.gzip\")\n",
    "            gdf.to_parquet(dest_f, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T22:24:55.454217Z",
     "iopub.status.busy": "2024-08-02T22:24:55.454027Z",
     "iopub.status.idle": "2024-08-02T22:40:34.144736Z",
     "shell.execute_reply": "2024-08-02T22:40:34.144072Z",
     "shell.execute_reply.started": "2024-08-02T22:24:55.454203Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4926/1152666761.py:10: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  rdf = reproject_nlcd_impervious_lands(ds, ca_boundaries, run_code=True, varname=varname)\n",
      "/tmp/ipykernel_4926/1152666761.py:10: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  rdf = reproject_nlcd_impervious_lands(ds, ca_boundaries, run_code=True, varname=varname)\n"
     ]
    }
   ],
   "source": [
    "# open NLCD zarr from our S3 bucket\n",
    "in_fname = 's3://ca-climate-index/1_pull_data/natural_systems/ecosystem_condition/usgs/ncld_ca_developed_impervious.zarr'\n",
    "ds = xr.open_zarr(in_fname)\n",
    "\n",
    "# read in CA census tiger file\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "\n",
    "varname = 'test'\n",
    "rdf = reproject_nlcd_impervious_lands(ds, ca_boundaries, run_code=True, varname=varname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
