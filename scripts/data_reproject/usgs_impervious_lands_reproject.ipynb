{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cal-CRAI Reprojection -- Impervious Surface Data\n",
    "This notebook processes impervious surface data sourced from the USGS: \\\n",
    "https://www.arcgis.com/home/item.html?id=1fdbb561c58b45c58f8f966c00c78ae6\n",
    "\n",
    "Data reprojecting includes:\n",
    "* Clipping data to California boundaries\n",
    "* Reprojecting data to standardized coordinate reference system (EPSG:4269)\n",
    "\n",
    "Output is uploaded to 2b_reproject directory within AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xarray as xr\n",
    "import boto3\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# suppress pandas purely educational warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import pull_csv_from_directory, upload_csv_aws, filter_counties\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def reproject_nlcd_impervious_lands(ds, ca_boundaries, run_code=True, varname=''):\n",
    "    \"\"\"\n",
    "    Reprojects the CA-wide USGS impervious lands zarr to California Census Tract Coordinate Reference System, \n",
    "    then clips to these CA tracts, and uploads to AWS S3. This code differs from the \n",
    "    reproject_shapefile() function by utilizing dask-geopandas to manipulate this very large dataset\n",
    "    and saving the result as 45 parquet files. \n",
    "\n",
    "    This was run in a script in a computing cluster to leverage additional memory. \n",
    "\n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in\n",
    "    ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "     \n",
    "    Methods\n",
    "    -------\n",
    "    Use dask-geopandas to work with the large datasets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    zarr_fname: string\n",
    "        filename of the USGS impervious lands zarr\n",
    "    ca_boundaries: \n",
    "        read-in gpd file of California Census Tracts\n",
    "    run_code: bool\n",
    "        if True, code will run. If false, just metadata file will be updated\n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    usgs_impervious_lands_reproject.py    \n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')  \n",
    "    bucket_name = 'ca-climate-index' \n",
    "    var = 'natural_usgs_impervious'\n",
    "    dest_f = in_fname.replace(\n",
    "        in_fname.split('/')[-1],f\"{var}.parquet.gzip\")\n",
    "    dest_f = re.sub(r'1_pull_data', '2b_reproject', dest_f)\n",
    "                \n",
    "    print('Data transformation: Reproject to standard coordinate reference system: 4269.')    \n",
    "    print('Data transformation: sjoin large geodata with CA census tract boundaries data.')    \n",
    "    print(\n",
    "            \"Data transformation: Saved as multiple parquet files because\"\n",
    "            +\" the resulting dataset is too large to be saved as one file.\"\n",
    "    )\n",
    "    print(f\"Parquets saved to: s3://ca-climate-index/2b_reproject/natural_systems/ecosystem_condition/usgs/\")\n",
    "        \n",
    "    if run_code==True:\n",
    "        orig_crs = ds.spatial_ref.attrs[\"crs_wkt\"]\n",
    "        cb_crs = ca_boundaries.crs\n",
    "        ca_boundaries = ca_boundaries[[\"GEOID\",\"geometry\"]]\n",
    "\n",
    "        da = ds.impervious_surface\n",
    "        df = da.to_dask_dataframe()\n",
    "        df = df[[\"impervious_surface\",\"x\",\"y\"]]\n",
    "\n",
    "        for i in range(len(list(df.partitions))):\n",
    "            part_df = df.partitions[i].compute()\n",
    "            part_df = part_df[part_df[\"impervious_surface\"]!=127.0]\n",
    "            gdf = gpd.GeoDataFrame(\n",
    "                part_df, geometry=gpd.points_from_xy(part_df.x,part_df.y, crs=orig_crs)\n",
    "            )\n",
    "            gdf = gdf.to_crs(cb_crs)\n",
    "            gdf = gdf.sjoin(ca_boundaries, how='inner', predicate='intersects')\n",
    "            gdf = gdf.drop(columns=[\"index_right\",\"x\",\"y\"])\n",
    "            print(gdf)\n",
    "            dest_f = dest_f.replace(\n",
    "                dest_f.split('/')[-1],f\"ca_clipped_{var}_{i}.parquet.gzip\")\n",
    "            gdf.to_parquet(dest_f, compression='gzip')\n",
    "            \n",
    "# open NLCD zarr from our S3 bucket\n",
    "in_fname = 's3://ca-climate-index/1_pull_data/natural_systems/ecosystem_condition/usgs/nlcd_ca_developed_impervious.zarr'\n",
    "ds = xr.open_zarr(in_fname)\n",
    "# read in CA census tiger file\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "varname = 'natural_usgs_impervious'\n",
    "\n",
    "rdf = reproject_nlcd_impervious_lands(ds, ca_boundaries, run_code=False, varname=varname)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
