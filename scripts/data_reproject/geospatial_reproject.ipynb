{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cal-CRAI Reprojection -- geospatial file inputs\n",
    "This notebook processes geospatial data files, of manageable size, for reprojection where necessary and includes the necessary metadata pieces for clarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from geopandas.tools import overlay\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import zipfile\n",
    "import sys\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import re\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "def list_geospatial_files(path):\n",
    "    \"\"\" Build a list of shapefile URIs contained in S3 folder \"\"\"\n",
    "    # initiate empty list for s3 URIs\n",
    "    all_shapefiles = []\n",
    "    bucket_name = 'ca-climate-index' \n",
    "    # initiate s3 session\n",
    "    session = boto3.Session()\n",
    "    # use the session to get the resource\n",
    "    s3 = session.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    # iterate through directory\n",
    "    for obj in my_bucket.objects.filter(\n",
    "        Prefix=path):\n",
    "        # build list of shapefile URIs\n",
    "        if obj.key.endswith('.zip'):\n",
    "            # preceding the URI with 'zip' lets you read in the file without downloading, unzipping, etc\n",
    "            s3_uri = f\"zip+s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "        elif obj.key.endswith('.shp'):\n",
    "            s3_uri = \"s3://ca-climate-index/\"+obj.key\n",
    "            all_shapefiles.append(s3_uri)\n",
    "    return all_shapefiles\n",
    "\n",
    "@append_metadata\n",
    "def reproject_shapefile(shp_fname, ca_boundaries, varname='', export=False, additional_comments='N/A'):\n",
    "    \"\"\" \n",
    "    Given S3 URI which corresponds to a data shapefile and a shapefile with California Census Tract: \n",
    "    (1) reproject the data shapefile to the CRS of the California Census Tracts, \n",
    "    (2) clip to California, and \n",
    "    (3) send it off to S3.\n",
    "\n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in\n",
    "    ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    shp_fname: string\n",
    "        Local main folder where the pulled files will be saved.\n",
    "    ca_boundaries: string\n",
    "        Name of the CA census tract shape file.\n",
    "    export: bool\n",
    "        If True, exports resulting file to S3.\n",
    "        If False, will return the metadata.\n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    geospatial_reproject.ipynb\n",
    "    \"\"\"\n",
    "    if export == False:\n",
    "        return\n",
    "    else:\n",
    "        s3_client = boto3.client('s3')  \n",
    "        bucket_name = 'ca-climate-index' \n",
    "        # read in shapefile of interest from S3 and take a look at it\n",
    "        gdf = gpd.read_file(shp_fname)\n",
    "        print(f\"Original CRS of {varname}: {gdf.crs}\")\n",
    "        fig, ax = plt.subplots()\n",
    "        gdf.plot(ax=ax, markersize=1)\n",
    "        plt.title(f\"{varname} on original projection\")\n",
    "        plt.show()\n",
    "\n",
    "        # reproject the data to the census tract CRS and clip to California\n",
    "        gdf_reprojected = gdf.to_crs(ca_boundaries.crs)\n",
    "        print(f\"{varname} reprojected from {gdf.crs} to {gdf_reprojected.crs} with geopandas to_crs() function.\")\n",
    "\n",
    "        clipped_gdf = overlay(gdf_reprojected, ca_boundaries, how='intersection')\n",
    "        print(f\"{varname} clipped to California boundaries via geopandas overlay using the 'intersection' method.\")\n",
    "\n",
    "        # visualize results\n",
    "        fig, ax = plt.subplots()\n",
    "        ca_boundaries.plot(ax=ax, color='white', edgecolor='black')\n",
    "        clipped_gdf.plot(ax=ax, marker='o', color='red', markersize=1)\n",
    "        plt.title(f\"{varname} on new projection\")\n",
    "        plt.show()\n",
    "\n",
    "        # write the reprojected file to disk - still looking for a way around this\n",
    "        # if not os.path.exists(f\"{varname}.gpkg\"):\n",
    "        clipped_gdf.to_file(f\"{varname}.gpkg\", driver=\"GPKG\")\n",
    "        print(f\"{varname}.gpkg has been made\")\n",
    "\n",
    "        if shp_fname.endswith('.zip'):\n",
    "            shp_fname = shp_fname.replace(\n",
    "                'zip+',\n",
    "                '')\n",
    "            \n",
    "        dest_path = shp_fname.replace(\n",
    "            's3://ca-climate-index/',\n",
    "            '')\n",
    "        dest_path = re.sub(r'1_pull_data|2a_subset', '2b_reproject', dest_path)\n",
    "        dest_path = dest_path.replace(dest_path.split('/')[-1],f\"{varname}.gpkg\")\n",
    "        print(f\"Reprojected data called {varname}.gpkg sent to S3 bucket: {dest_path}\")\n",
    "\n",
    "        if export == True:\n",
    "            # upload it to S3\n",
    "            s3_client.upload_file(f\"{varname}.gpkg\", f'{bucket_name}', f'{dest_path}'\n",
    "            )\n",
    "            \n",
    "        os.remove(f\"{varname}.gpkg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the CSV with the data details\n",
    "# ref_file = sys.path[-1]+'/metadata/Full Data Pipeline Notes - 1_ Pull.csv'\n",
    "ref_file = r'C:/Users/jespi/eagle/carb-climate-index-7/metadata/Full Data Pipeline Notes - 1_ Pull.csv'\n",
    "df = pd.read_csv(ref_file)\n",
    "\n",
    "# subset for shapefiles\n",
    "ref_df = df.fillna('N/A')\n",
    "# comment out for now as 'Pulled Format' column not updated\n",
    "ref_df = ref_df[\n",
    "(ref_df[\"Pulled Format\"].str.contains(\"shp\")) \n",
    "| (ref_df[\"Pulled Format\"].str.contains(\"gdb\"))\n",
    "]\n",
    "\n",
    "### Define the path\n",
    "path1 = \"1_pull_data\"\n",
    "path2 = \"2a_subset\"\n",
    "#  build a list of shapefiles in the above s3 paths\n",
    "my_list = list_geospatial_files(path1) \n",
    "my_list += list_geospatial_files(path2)\n",
    "\n",
    "# read in CA census tiger file\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "# need to rename columns so we don't have any duplicates in the final geodatabase\n",
    "column_names = ca_boundaries.columns\n",
    "new_column_names = [\"USCB_\"+column for column in column_names if column != \"geometry\"]\n",
    "ca_boundaries = ca_boundaries.rename(columns=dict(zip(column_names, new_column_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the reprojection code over the desired files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "additional_comments = \"N/A\"\n",
    "# build list of file names\n",
    "file_names = [name for name in ref_df['File Name'].values if name != 'N/A']\n",
    "# define large files, which are reprojected elsewhere\n",
    "large_files = [\n",
    "    'climate_iowa_mesonet_flash_flood_warnings', \n",
    "    'climate_koordinates_floodplain', \n",
    "    'climate_iowa_mesonet_wildfire_warnings',\n",
    "    'governance_usda_watershed_risk',\n",
    "    'governance_usda_fuel_reduction'\n",
    "]\n",
    "# skip problematic files\n",
    "problem_vars = [\n",
    "    \"natural_calfire_vegetation_types\", \n",
    "    \"natural_cnra_protected_areas\"\n",
    "]\n",
    "# excluded files\n",
    "excluded_files = large_files+problem_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the list \n",
    "for fpath in my_list:\n",
    "    # get the file name by itself (no subdirectories)\n",
    "    fname = fpath.split('/')[-1]\n",
    "    if fname in file_names:\n",
    "         # want the subsetted TV contours data in 2a_subset folder\n",
    "        if fname==\"TV_Broadcast_Contours.zip\":\n",
    "            continue\n",
    "        if fname =='sta_tv_contours.zip':\n",
    "            varname = \"built_hifld_tv_contour\"\n",
    "        else:\n",
    "            # match up file name to variable name\n",
    "            varname = ref_df.loc[ref_df[\"File Name\"] == fname][\"Variable\"].values[0]\n",
    "            # exclude some files\n",
    "            if varname in excluded_files:\n",
    "                continue\n",
    "            else:\n",
    "                reproject_shapefile(fpath, ca_boundaries, export=False, additional_comments=additional_comments, varname=varname)\n",
    "    else:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
