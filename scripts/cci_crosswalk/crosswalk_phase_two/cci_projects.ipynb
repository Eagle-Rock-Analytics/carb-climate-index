{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T20:35:26.100281Z",
     "iopub.status.busy": "2024-02-27T20:35:26.099019Z",
     "iopub.status.idle": "2024-02-27T20:35:26.610862Z",
     "shell.execute_reply": "2024-02-27T20:35:26.610359Z",
     "shell.execute_reply.started": "2024-02-27T20:35:26.100227Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T20:35:27.714477Z",
     "iopub.status.busy": "2024-02-27T20:35:27.713563Z",
     "iopub.status.idle": "2024-02-27T20:35:35.348017Z",
     "shell.execute_reply": "2024-02-27T20:35:35.346286Z",
     "shell.execute_reply.started": "2024-02-27T20:35:27.714441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling file\n"
     ]
    }
   ],
   "source": [
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Bucket name and file paths\n",
    "bucket_name = 'ca-climate-index'\n",
    "directory = '0_map_data/crosswalk_data/CCI_Projects_Project_Category_Update_02142024.xlsm'\n",
    "\n",
    "print('Pulling file')\n",
    "s3_client.download_file(bucket_name, directory, 'CCI_Projects_Project_Category_Update_02142024.xlsm')\n",
    "print('File pulled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T20:35:36.344140Z",
     "iopub.status.busy": "2024-02-27T20:35:36.343285Z",
     "iopub.status.idle": "2024-02-27T20:37:17.536687Z",
     "shell.execute_reply": "2024-02-27T20:37:17.536179Z",
     "shell.execute_reply.started": "2024-02-27T20:35:36.344090Z"
    }
   },
   "outputs": [],
   "source": [
    "crosswalk_data = pd.read_excel('CCI_Projects_Project_Category_Update_02142024.xlsm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many rows within the original dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T20:39:41.319266Z",
     "iopub.status.busy": "2024-02-27T20:39:41.317837Z",
     "iopub.status.idle": "2024-02-27T20:39:41.591527Z",
     "shell.execute_reply": "2024-02-27T20:39:41.591120Z",
     "shell.execute_reply.started": "2024-02-27T20:39:41.319198Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print('Number of rows within dataset:', len(crosswalk_data))\n",
    "display(crosswalk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T20:39:50.913448Z",
     "iopub.status.busy": "2024-02-27T20:39:50.912759Z",
     "iopub.status.idle": "2024-02-27T20:39:50.926646Z",
     "shell.execute_reply": "2024-02-27T20:39:50.925840Z",
     "shell.execute_reply.started": "2024-02-27T20:39:50.913406Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of columns:', len(crosswalk_data.columns.tolist()))\n",
    "display(crosswalk_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting columns relevant to initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T20:39:56.829905Z",
     "iopub.status.busy": "2024-02-27T20:39:56.829160Z",
     "iopub.status.idle": "2024-02-27T20:39:56.837054Z",
     "shell.execute_reply": "2024-02-27T20:39:56.835396Z",
     "shell.execute_reply.started": "2024-02-27T20:39:56.829870Z"
    }
   },
   "outputs": [],
   "source": [
    "relevant_columns = [\n",
    "    'Program Name',\n",
    "    'Program Description',\n",
    "    'Sub Program Name',\n",
    "    'Project Type',\n",
    "    'Project Description',\n",
    "    'SECTOR',\n",
    "    'CATEGORY',\n",
    "    'ACTION',\n",
    "    'Census Tract',\n",
    "    'Total Project GHGReductions',\n",
    "    'Project Count'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T20:39:57.660228Z",
     "iopub.status.busy": "2024-02-27T20:39:57.659508Z",
     "iopub.status.idle": "2024-02-27T20:39:57.686696Z",
     "shell.execute_reply": "2024-02-27T20:39:57.686055Z",
     "shell.execute_reply.started": "2024-02-27T20:39:57.660193Z"
    }
   },
   "outputs": [],
   "source": [
    "data_of_interest = crosswalk_data[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T20:39:59.134495Z",
     "iopub.status.busy": "2024-02-27T20:39:59.133805Z",
     "iopub.status.idle": "2024-02-27T20:39:59.162696Z",
     "shell.execute_reply": "2024-02-27T20:39:59.161836Z",
     "shell.execute_reply.started": "2024-02-27T20:39:59.134462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set display options to show all columns and rows\n",
    "# pd.set_option('display.max_columns', None)  # To display all columns\n",
    "# pd.set_option('display.max_rows', None)     # To display all rows\n",
    "\n",
    "# Now display data_of_interest\n",
    "display(data_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a climate risk dictionary to scan through data based on dictionary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T21:50:21.445388Z",
     "iopub.status.busy": "2024-02-27T21:50:21.444496Z",
     "iopub.status.idle": "2024-02-27T21:50:21.457083Z",
     "shell.execute_reply": "2024-02-27T21:50:21.455910Z",
     "shell.execute_reply.started": "2024-02-27T21:50:21.445334Z"
    }
   },
   "outputs": [],
   "source": [
    "climate_risk_dict = {\n",
    "    'wildfire mitigation': ['wildfire', 'prescribed fire', 'fire prevention', 'burn', 'controlled burn', 'controlled_burning', \n",
    "                            'prescribed burn', 'prescribed burning' 'firefighting', 'reforest', 'reforestation', 'vegetation management', \n",
    "                            'roadside brushing', 'fuel break', 'fuel reduction', 'ignition', 'crown', 'fuel load', 'Fire and Forest Management'],\n",
    "    \n",
    "    'sea level rise mitigation': ['sea level rise', 'slr', 'seawall', 'seawalls', 'shoreline', 'wetland', 'mangrove', 'coastal'],\n",
    "    \n",
    "    'extreme heat mitigation': ['extreme heat', 'shade', 'shading', 'cooling center', 'cooling centers', 'heat-resistant', \n",
    "                                'heat resistant', 'heat reducing', 'heat-reducing'],\n",
    "    \n",
    "    'drought mitigation': ['drought', 'irrigation', 'soil moisture', 'rainwater harvest', 'rainwater harvesting', 'water storage', \n",
    "                           'water allocation', 'water management', 'soil health', 'soil management', 'organic matter', 'water efficiency'],\n",
    "    \n",
    "    'inland flooding mitigation': ['flooding', 'runoff', 'inland flood', 'inland flooding', 'floodplain', 'flood proof', 'floodproofing', \n",
    "                                   'elevated flood', 'flood barrier', 'flood barriers', 'drainage', 'riparian', 'stormwater'],\n",
    "    \n",
    "    'greenhouse gas mitigation': ['ghg', 'GHG', 'greenhouse gas', 'emission', 'emissions', 'carbon sequestration', 'electrification', \n",
    "                                  'carbon capture', 'solar power', 'renewable energy', 'wind energy', 'hydroelectricity', 'geothermal energy', \n",
    "                                  'biomass energy', 'Energy-efficiency']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many total and unique entries for each column? Will help decide which column to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T21:50:22.726128Z",
     "iopub.status.busy": "2024-02-27T21:50:22.725385Z",
     "iopub.status.idle": "2024-02-27T21:50:22.826313Z",
     "shell.execute_reply": "2024-02-27T21:50:22.825934Z",
     "shell.execute_reply.started": "2024-02-27T21:50:22.726092Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_entries(dataframe):\n",
    "    total_entries = dataframe.count()\n",
    "    unique_entries = dataframe.nunique()\n",
    "    return total_entries, unique_entries\n",
    "\n",
    "# Example usage:\n",
    "total_entries, unique_entries = count_entries(data_of_interest)\n",
    "print(\"Total entries per column:\")\n",
    "print(total_entries)\n",
    "print(\"\\nUnique entries per column:\")\n",
    "print(unique_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding to loop through 'Project Description' first as it has a large number of variation to capture many datasets, and makes most practical scense for filtering climate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T21:50:24.368467Z",
     "iopub.status.busy": "2024-02-27T21:50:24.367561Z",
     "iopub.status.idle": "2024-02-27T21:50:24.390493Z",
     "shell.execute_reply": "2024-02-27T21:50:24.389748Z",
     "shell.execute_reply.started": "2024-02-27T21:50:24.368422Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_climate_risk_column(df, keyword_dict, output_csv=None):\n",
    "    # Initialize new columns to store climate risk mitigation keywords, detected values, repeat counts, and total unique descriptions\n",
    "    df['Climate_Risk_Mitigation'] = ''\n",
    "    df['Detected_Climate_Risk_Mitigation_Keyword'] = ''\n",
    "    df['Repeat_Project_Description_Count'] = 0\n",
    "\n",
    "    # Initialize a counter for each keyword\n",
    "    keyword_counter = {keyword: 0 for keyword in keyword_dict}\n",
    "\n",
    "    # Create a dictionary to store the repeat count for each unique project description\n",
    "    description_counts = {}\n",
    "\n",
    "    # Create a dictionary to store the unique count for each keyword\n",
    "    unique_keyword_counts = {keyword: set() for keyword in keyword_dict}\n",
    "\n",
    "    # Iterate through each unique description\n",
    "    unique_descriptions = df['Project Description'].unique()\n",
    "    total_unique_descriptions = len(unique_descriptions)\n",
    "\n",
    "    for description in unique_descriptions:\n",
    "        # Find all rows with this description\n",
    "        description_rows = df[df['Project Description'] == description]\n",
    "        repeat_count = len(description_rows)\n",
    "        # Update the repeat count for this description\n",
    "        description_counts[description] = repeat_count\n",
    "\n",
    "        # Iterate through each row with this description\n",
    "        for index, row in description_rows.iterrows():\n",
    "            keywords_found = set()  # To store unique keywords found in each row\n",
    "            detected_values = []    # To store the detected values for each row\n",
    "            # Iterate through each keyword in the dictionary\n",
    "            for keyword, values in keyword_dict.items():\n",
    "                # Check if any value of the keyword is present in the description (case-insensitive)\n",
    "                detected = [val for val in values if val.lower() in description.lower()]\n",
    "                if detected:\n",
    "                    keywords_found.add(keyword)\n",
    "                    keyword_counter[keyword] += 1\n",
    "                    detected_values.extend(detected)\n",
    "                    # Add the description to unique count for this keyword\n",
    "                    unique_keyword_counts[keyword].add(description)\n",
    "\n",
    "            # Update the 'Climate_Risk_Mitigation' column with unique keywords found\n",
    "            df.at[index, 'Climate_Risk_Mitigation'] = ', '.join(keywords_found)\n",
    "            # Update the 'Detected_Values' column with detected values\n",
    "            df.at[index, 'Detected_Climate_Risk_Mitigation_Keyword'] = ', '.join(detected_values)\n",
    "            # Update the 'Repeat_Project_Description_Count' column with the repeat count for this description\n",
    "            df.at[index, 'Repeat_Project_Description_Count'] = repeat_count\n",
    "\n",
    "    # Print keyword counts\n",
    "    print(\"Keyword Counts:\")\n",
    "    for keyword, count in keyword_counter.items():\n",
    "        print(f\"{keyword}: {count}\")\n",
    "    print('')\n",
    "    # Print total unique descriptions count\n",
    "    print(f\"Total Unique Project Descriptions: {total_unique_descriptions}\")\n",
    "    print('')\n",
    "\n",
    "    # Print unique counts from each keyword\n",
    "    print(\"Unique Counts from Each Keyword:\")\n",
    "    for keyword, unique_count in unique_keyword_counts.items():\n",
    "        print(f\"{keyword}: {len(unique_count)}\")\n",
    "        print('')\n",
    "    # Save DataFrame as CSV if output_csv is provided\n",
    "    if output_csv:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"DataFrame saved as {output_csv}\")\n",
    "        print('')\n",
    "        # Initialize the S3 client\n",
    "        s3_client = boto3.client('s3')\n",
    "\n",
    "        # Bucket name and file paths\n",
    "        bucket_name = 'ca-climate-index'\n",
    "        directory = f'0_map_data/crosswalk_data/{output_csv}'\n",
    "        # Upload the CSV file to S3\n",
    "        print(f'Uploading {output_csv} to AWS')\n",
    "        with open(output_csv, 'rb') as file:\n",
    "            s3_client.upload_fileobj(file, bucket_name, directory)\n",
    "            print(f'Upload complete! File is in {directory}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function on whole dataset, the function will:\n",
    "- loop through each 'Project Description' and look for words/phrases in our climate risk dictionary and append to keyword counter\n",
    "- total keywords are counted\n",
    "- number of unique 'Project Description' entries are counted, so are climate risk keys associate with those unique entries\n",
    "- makes two new columns: 'Repeat Project Description Count' and 'Detected Climate Risk Mitigation Keyword' to add more context and improve dictionary keywords\n",
    "\n",
    "## The cell below runs the function but also adds a few things:\n",
    "- makes a data preview, just selecting relevant columns that were made and help interpret Project Description screening results\n",
    "- orders the data in decending order from the Repeat Project Description Count to show Project Descriptions with multiple entries first (make sure dictionary is properly assigning large entries with correct climate risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T21:50:25.720470Z",
     "iopub.status.busy": "2024-02-27T21:50:25.719515Z",
     "iopub.status.idle": "2024-02-27T21:51:45.513157Z",
     "shell.execute_reply": "2024-02-27T21:51:45.512783Z",
     "shell.execute_reply.started": "2024-02-27T21:50:25.720398Z"
    }
   },
   "outputs": [],
   "source": [
    "add_climate_risk_column(crosswalk_data, climate_risk_dict) #, 'climate_risk_attributed_crosswalk_data.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "data_preview = crosswalk_data[['Project Description', 'Repeat_Project_Description_Count', 'Detected_Climate_Risk_Mitigation_Keyword', 'Climate_Risk_Mitigation']]\n",
    "\n",
    "# Filter the DataFrame to show only rows with entries in the 'Climate_Risk_Mitigation' column\n",
    "data_preview_filtered = data_preview[data_preview['Climate_Risk_Mitigation'] != '']\n",
    "\n",
    "# Filter the DataFrame to show only rows with entries in the 'Climate_Risk_Mitigation' column\n",
    "data_preview_filtered = data_preview[data_preview['Climate_Risk_Mitigation'] != '']\n",
    "\n",
    "# Sort the DataFrame based on 'Repeat_Project_Description_Count' in descending order\n",
    "data_preview_filtered_sorted = data_preview_filtered.sort_values(by='Repeat_Project_Description_Count', ascending=False)\n",
    "\n",
    "# Drop duplicates based on both 'Repeat_Project_Description_Count' and 'Project Description' to keep only one row per unique combination\n",
    "data_preview_filtered_unique = data_preview_filtered_sorted.drop_duplicates(subset=['Repeat_Project_Description_Count', 'Project Description'])\n",
    "\n",
    "#display(data_preview_filtered_unique)\n",
    "display(data_preview_filtered_unique[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T21:51:45.514259Z",
     "iopub.status.busy": "2024-02-27T21:51:45.514099Z",
     "iopub.status.idle": "2024-02-27T21:51:45.518515Z",
     "shell.execute_reply": "2024-02-27T21:51:45.518116Z",
     "shell.execute_reply.started": "2024-02-27T21:51:45.514246Z"
    }
   },
   "outputs": [],
   "source": [
    "crosswalk_data\n",
    "crosswalk_data_sample = crosswalk_data.sample(n=100)\n",
    "# crosswalk_data_sample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Adding in tests to understand where more than one risk is assigned\n",
    "- If 2 are provided, but one is GHG --> assign the category to the associated climate risk (i.e., \"greenhouse gas mitigation, sea level rise mitigation\" should end up as \"sea level rise mitigation\")\n",
    "   - 654 instances\n",
    "- If 2+ climate risks are assigned, need manual intervention to identify climate risk to be final assigned\n",
    "   - Strip out all instances of \"greenhouse gas mitigation\" to reduce # of manual intervention\n",
    "   - Identify the \"main\" or \"priority\" risk denoted in the project description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T22:00:10.870861Z",
     "iopub.status.busy": "2024-02-27T22:00:10.870109Z",
     "iopub.status.idle": "2024-02-27T22:00:10.977919Z",
     "shell.execute_reply": "2024-02-27T22:00:10.977538Z",
     "shell.execute_reply.started": "2024-02-27T22:00:10.870822Z"
    }
   },
   "outputs": [],
   "source": [
    "# multi_risk = crosswalk_data.loc[crosswalk_data['Climate_Risk_Mitigation'].str.contains('greenhouse gas mitigation, wildfire mitigation')]\n",
    "multi_risk = crosswalk_data.loc[(crosswalk_data['Climate_Risk_Mitigation'].str.count(',') == 1) & (crosswalk_data['Climate_Risk_Mitigation'].str.contains('greenhouse gas mitigation'))]\n",
    "# clean-up view for easier access\n",
    "data_preview = multi_risk[['Project Description', 'Repeat_Project_Description_Count', 'Detected_Climate_Risk_Mitigation_Keyword', 'Climate_Risk_Mitigation']]\n",
    "\n",
    "print(len(data_preview))\n",
    "display(data_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating 'greenhouse gas mitigation' entries when other climate risks present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame to avoid modifying the original data\n",
    "crosswalk_data_copy = crosswalk_data.copy()\n",
    "\n",
    "# Filter rows containing 'greenhouse gas mitigation'\n",
    "multi_risk = crosswalk_data_copy.loc[(crosswalk_data_copy['Climate_Risk_Mitigation'].str.count(',') == 1) & \n",
    "                                (crosswalk_data_copy['Climate_Risk_Mitigation'].str.contains('greenhouse gas mitigation'))]\n",
    "\n",
    "# Replace 'greenhouse gas mitigation' with an empty string\n",
    "multi_risk['Climate_Risk_Mitigation'] = multi_risk['Climate_Risk_Mitigation'].str.replace('greenhouse gas mitigation', '')\n",
    "\n",
    "# Remove any remaining commas\n",
    "multi_risk['Climate_Risk_Mitigation'] = multi_risk['Climate_Risk_Mitigation'].str.replace(',', '')\n",
    "\n",
    "# Clean-up view for easier access\n",
    "data_preview = multi_risk[['Project Description', 'Repeat_Project_Description_Count', 'Detected_Climate_Risk_Mitigation_Keyword', 'Climate_Risk_Mitigation']]\n",
    "\n",
    "# Use the index of multi_risk to update the corresponding rows in crosswalk_data\n",
    "crosswalk_data_copy.update(multi_risk)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "#print(crosswalk_data)\n",
    "\n",
    "print(len(data_preview))\n",
    "display(data_preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame to avoid modifying the original data\n",
    "test = crosswalk_data_copy.copy()\n",
    "\n",
    "# Filter rows with three or more commas in 'Climate_Risk_Mitigation' column\n",
    "multi_risk = test.loc[(test['Climate_Risk_Mitigation'].str.count(',') >= 1)]\n",
    "\n",
    "# Iterate over the climate risk dictionary to filter rows and update the DataFrame\n",
    "for mitigation_type, keywords in climate_risk_dict.items():\n",
    "    # Create a boolean mask to filter rows containing any of the keywords\n",
    "    mask = test['CATEGORY'].str.contains('|'.join(keywords), case=False) | \\\n",
    "           test['SECTOR'].str.contains('|'.join(keywords), case=False)\n",
    "    \n",
    "    # Filter rows based on the mask\n",
    "    filtered_rows = multi_risk[mask]\n",
    "    \n",
    "    # Update the 'Climate_Risk_Mitigation' column for the filtered rows\n",
    "    filtered_rows['Climate_Risk_Mitigation'] = mitigation_type\n",
    "    \n",
    "    # Use the index of filtered rows to update the corresponding rows in crosswalk_data_copy\n",
    "    test.update(filtered_rows)\n",
    "\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_crosswalk_data = crosswalk_data_copy.copy()\n",
    "\n",
    "# Define the rows to update based on the specified criteria\n",
    "sea_level_rise_rows = [15693, 62317, 74956, 89774]\n",
    "inland_flooding_rows = [60114, 60160, 74973, 75775, 89750, 89847, 89918, 91016, 128903, 60253, 60265, 60292, 89679]\n",
    "drought_rows = [41034, 75775, 89918, 113037, 119459, 75188, 75107, 75814, 89828, 89846, 109976, 110600]\n",
    "wildfire_rows = [89455, 90049, 110288, 110291, 110294, 110297, 110298, 110303, 110305, 110333, 110337, 110339, 110347, 110361, 110368, 110372, 110447, 110466, 111867, 111874, 116515, 116520, 116541, 116543, 116548, 116582, 116583, 119503, 119516, 119518, 119554, 124581, 124582, 127999, 128030, 128063, 128083, 128144, 128262, 128280, 110373, 119509, 62321, 75163, 75165, 75166, 75167, 75168, 75169, 75170, 75171, 75172, 75173, 75174]\n",
    "extreme_heat_rows = [75775]\n",
    "greenhouse_gas_rows = [60117, 89961, 113029, 113036, 110373, 113030, 113026]\n",
    "unclassified = [60109, 112994, 116130, 116131, 116132, 116133, 116685, 116843, 119568]\n",
    "\n",
    "# Create a dictionary mapping mitigation types to their corresponding rows\n",
    "mitigation_mapping = {\n",
    "    'sea level rise mitigation': sea_level_rise_rows,\n",
    "    'inland flooding mitigation': inland_flooding_rows,\n",
    "    'drought mitigation': drought_rows,\n",
    "    'wildfire mitigation': wildfire_rows,\n",
    "    'extreme heat mitigation': extreme_heat_rows,\n",
    "    'greenhouse gas mitigation': greenhouse_gas_rows,\n",
    "    'unclassified': unclassified\n",
    "}\n",
    "\n",
    "# Iterate through the mitigation types and their corresponding rows\n",
    "for mitigation_type, rows_to_update in mitigation_mapping.items():\n",
    "    # Update the 'Climate_Risk_Mitigation' column for each row\n",
    "    for row_index in rows_to_update:\n",
    "        cleaned_crosswalk_data.loc[row_index, 'Climate_Risk_Mitigation'] = mitigation_type\n",
    "\n",
    "multi_risk = cleaned_crosswalk_data.loc[(cleaned_crosswalk_data['Climate_Risk_Mitigation'].str.count(',') >= 1)]\n",
    "print(len(multi_risk))\n",
    "multi_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify how many have 3+ risks assigned\n",
    "multi_risk = test.loc[(test['Climate_Risk_Mitigation'].str.count(',') == 1)]\n",
    "data_preview = multi_risk[['Project Description', 'CATEGORY', 'SECTOR', 'Repeat_Project_Description_Count', 'Detected_Climate_Risk_Mitigation_Keyword', 'Climate_Risk_Mitigation']]\n",
    "print(len(data_preview))\n",
    "pd.set_option('display.max_rows', None)  \n",
    "#data_preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify how many have 3+ risks assigned\n",
    "multi_risk = crosswalk_data_copy.loc[(crosswalk_data_copy['Climate_Risk_Mitigation'].str.count(',') >= 3)]\n",
    "data_preview = multi_risk[['Project Description', 'CATEGORY', 'SECTOR', 'Repeat_Project_Description_Count', 'Detected_Climate_Risk_Mitigation_Keyword', 'Climate_Risk_Mitigation']]\n",
    "print(len(data_preview))\n",
    "data_preview\n",
    "\n",
    "## 60 instances where there are 3 risks assigned -- most have ghg included and can be reduced to 2 to isolate between, but not all\n",
    "## 3 instances where there are 4 risks assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_row = test.iloc[60230]\n",
    "display(specific_row[['SECTOR']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
