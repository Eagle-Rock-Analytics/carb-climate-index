{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import boto3\n",
    "import zipfile\n",
    "import sys\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @append_metadata\n",
    "def merge_flood(file_names, aws_dir, aws_out_dir, \n",
    "                varname='climate_iowa_mesonet_flash_flood_warnings'):\n",
    "    \"\"\"\n",
    "    Iowa State University Mesonet data on flood warnings is spread over three different files. \n",
    "    This function merges all three into one file for smoother subsequent analysis.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')  \n",
    "    bucket_name = 'ca-climate-index'  \n",
    "\n",
    "    # Local directory to store the downloaded zip file and extracted contents\n",
    "    local_directory = 'temp'\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "\n",
    "    # List to store GeoPandas DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    for name in file_names:\n",
    "        print(f'Pulling data from filename: {name}')\n",
    "        # Specify the S3 key (path + file) to download\n",
    "        s3_key = f'{aws_dir}/{name}'\n",
    "        print('S3 Key = ', s3_key)\n",
    "\n",
    "        # Download the zip file\n",
    "        local_zip_file_path = os.path.join(local_directory, name)\n",
    "\n",
    "        if not os.path.exists(local_zip_file_path):\n",
    "            # File not found locally, download from S3\n",
    "            print('This can take a good bit, depending on file size.')\n",
    "            s3_client.download_file(bucket_name, s3_key, local_zip_file_path)\n",
    "            print(f'Download complete: {name}')\n",
    "        \n",
    "        # Read the file using GeoPandas\n",
    "        data = gpd.read_file(local_zip_file_path)\n",
    "        # Print number of rows for each individual dataset\n",
    "        print(f'Number of rows in {name}: {len(data)}')        \n",
    "        print('')\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(data)\n",
    "        \n",
    "        # Merge all DataFrames together\n",
    "    merged_data = pd.concat(dfs, ignore_index=True)\n",
    "    print('Number of merged rows before looking at duplicates:', len(merged_data))\n",
    "    \n",
    "    # Identify and print all rows of duplicates\n",
    "    duplicates_mask = merged_data.duplicated(subset=['geometry', 'ISSUED'], keep=False)\n",
    "    duplicates = merged_data[duplicates_mask]\n",
    "\n",
    "    print(f'Number of duplicated rows: {len(duplicates)}') \n",
    "    print(\"Rows of duplicates:\")\n",
    "    display(duplicates)\n",
    "    print(\"\")\n",
    "\n",
    "    # Drop duplicate rows, keeping only the first occurrence\n",
    "    merged_data.drop_duplicates(subset=['geometry', 'ISSUED'], keep='first', inplace=True)\n",
    "    print('Number of merged rows after removing duplicates:', len(merged_data))\n",
    "\n",
    "    # Create a directory to store the shapefile and its associated files\n",
    "    output_folder = os.path.join(local_directory, 'output_shapefile')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Save the filtered shapefile inside the output folder\n",
    "    output_shapefile_path = os.path.join(output_folder, 'merged_flood.shp')\n",
    "    merged_data.to_file(output_shapefile_path)\n",
    "    \n",
    "    # Zip the output folder\n",
    "    print('Zipping file...')\n",
    "    output_zip_file_path = os.path.join(local_directory, 'merged_flood.zip')\n",
    "    shutil.make_archive(output_zip_file_path[:-4], 'zip', output_folder)\n",
    "\n",
    "    print('Uploading to AWS...')\n",
    "    s3_client.upload_file(output_zip_file_path, bucket_name, os.path.join(aws_out_dir, 'merged_flood.zip'))\n",
    "    print(f'merged_flood.zip uploaded to {aws_out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_files = [\n",
    "            'fa_flood_warnings_1986_2024.zip', \n",
    "            'fl_flood_warnings_1986_2024.zip', \n",
    "            'flash_flood_warnings_1986_2024.zip'\n",
    "]\n",
    "aws_dir = \"1_pull_data/climate_risk/flood/exposure/isu_environmental_mesonet\"\n",
    "aws_out_dir = \"2a_subset/climate_risk/flood/exposure/isu_environmental_mesonet/\"\n",
    "merge_flood(flood_files, aws_dir, aws_out_dir, \n",
    "            varname='climate_iowa_mesonet_flash_flood_warnings')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
