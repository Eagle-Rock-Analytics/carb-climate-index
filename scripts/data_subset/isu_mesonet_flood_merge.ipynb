{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cal-CRAI Subsetting -- ISU Mesonet Flood Warning Data\n",
    "This notebook processes three different flood datasets sourced from the Iowa State University Mesonet:\n",
    "https://mesonet.agron.iastate.edu/request/gis/watchwarn.phtml\n",
    "\n",
    "Data subsetting includes:\n",
    "* merging datasets into one based on time and location\n",
    "* eliminates duplicate warnings across the datasets based on time and location\n",
    "\n",
    "Output is uploaded to 2a_subset directory within AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import boto3\n",
    "import zipfile\n",
    "import sys\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def merge_flood(file_names, aws_dir, aws_out_dir, export=False, varname=''):\n",
    "    \"\"\"\n",
    "Iowa State University Mesonet data on flood warnings is spread over three different files: flash floods and two separate entries of flood (fl and fa). This function merges all three into one file and checks for duplicate flood/flash flood warning entries based on the same time and location. The resulting merged and subsetted .zip file is then uploaded to AWS.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are \n",
    "    stored in ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_names: string\n",
    "        list of flood files  \n",
    "    aws_dir: string\n",
    "        AWS directory of the .zip flood files: \n",
    "        1_pull_data/governance/natural_resource_conservation/ca_state_water_resources_board\n",
    "    aws_out_dir: string\n",
    "        AWS directory for the output .zip file:\n",
    "        2a_subset/climate_risk/flood/exposure/isu_environmental_mesonet/\n",
    "    export: bool\n",
    "        if True, uploads final result to designated AWS bucket\n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    isu_mesonet_flood_merge.ipynb\n",
    "    \"\"\"\n",
    "    print('Data transformation: the three different flooding files are merged together')\n",
    "    print('Data transformation: duplicate entries were removed based on location and date')\n",
    "    print('Data transformation: the final file was saved as a shp file, zipped, and reuploaded to AWS')\n",
    "\n",
    "    s3_client = boto3.client('s3')  \n",
    "    bucket_name = 'ca-climate-index'  \n",
    "\n",
    "    # Local directory to store the downloaded zip file and extracted contents\n",
    "    local_directory = 'temp'\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "\n",
    "    # List to store GeoPandas DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    for name in file_names:\n",
    "        #print(f'Pulling data for filename: {name}')\n",
    "        # Specify the S3 key (path + file) to download\n",
    "        s3_key = f'{aws_dir}/{name}'\n",
    "        #print('S3 Key = ', s3_key)\n",
    "\n",
    "        # Download the zip file\n",
    "        local_zip_file_path = os.path.join(local_directory, name)\n",
    "\n",
    "        if not os.path.exists(local_zip_file_path):\n",
    "            # File not found locally, download from S3\n",
    "            #print('This can take a good bit, depending on file size.')\n",
    "            s3_client.download_file(bucket_name, s3_key, local_zip_file_path)\n",
    "            #print(f'Download complete: {name}')\n",
    "        \n",
    "        # Read the file using GeoPandas\n",
    "        data = gpd.read_file(local_zip_file_path)\n",
    "        # Print number of rows for each individual dataset\n",
    "        #print(f'Number of rows in {name}: {len(data)}')        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(data)\n",
    "        \n",
    "    # Merge all DataFrames together\n",
    "    #print('Merging the datasets together')\n",
    "    merged_data = pd.concat(dfs, ignore_index=True)\n",
    "    #print('Merge complete')\n",
    "    #print('Number of merged rows before looking at duplicates:', len(merged_data))\n",
    "    \n",
    "    # Identify and print all rows of duplicates\n",
    "    duplicates_mask = merged_data.duplicated(subset=['geometry', 'ISSUED'], keep=False)\n",
    "    duplicates = merged_data[duplicates_mask]\n",
    "\n",
    "    #print(f'Number of duplicated rows: {len(duplicates)}') \n",
    "    #print(\"Rows of duplicates:\")\n",
    "    display(duplicates)\n",
    "\n",
    "    # Drop duplicate rows, keeping only the first occurrence\n",
    "    merged_data.drop_duplicates(subset=['geometry', 'ISSUED'], keep='first', inplace=True)\n",
    "    #print('Number of merged rows after removing duplicates:', len(merged_data))\n",
    "\n",
    "    # Create a directory to store the shapefile and its associated files\n",
    "    output_folder = os.path.join(local_directory, 'output_shapefile')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Save the filtered shapefile inside the output folder\n",
    "    #print('Now saving merged and subsetted flood data to: merged_flood.shp')\n",
    "    output_shapefile_path = os.path.join(output_folder, 'merged_flood.shp')\n",
    "    merged_data.to_file(output_shapefile_path)\n",
    "    \n",
    "    # Zip the output folder\n",
    "    #print('Zipping file...')\n",
    "    output_zip_file_path = os.path.join(local_directory, 'merged_flood.zip')\n",
    "    shutil.make_archive(output_zip_file_path[:-4], 'zip', output_folder)\n",
    "    #print('Zip complete')\n",
    "    \n",
    "    if export == True:\n",
    "        print('Uploading to AWS...')\n",
    "        s3_client.upload_file(output_zip_file_path, bucket_name, os.path.join(aws_out_dir, 'merged_flood.zip'))\n",
    "        print(f'merged_flood.zip uploaded to {aws_out_dir}')\n",
    "\n",
    "    if export == False:\n",
    "        print(f'merged_flood.zip uploaded to AWS.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_files = [\n",
    "            'fa_flood_warnings_1986_2024.zip', \n",
    "            'fl_flood_warnings_1986_2024.zip', \n",
    "            'flash_flood_warnings_1986_2024.zip'\n",
    "]\n",
    "aws_dir = \"1_pull_data/climate_risk/flood/exposure/isu_environmental_mesonet\"\n",
    "aws_out_dir = \"2a_subset/climate_risk/flood/exposure/isu_environmental_mesonet/\"\n",
    "merge_flood(flood_files, aws_dir, aws_out_dir, export=False, varname='climate_iowa_mesonet_flash_flood_warnings')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
