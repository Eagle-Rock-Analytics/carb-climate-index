{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cal-CRAI Subsetting -- TV Contour Data\n",
    "This notebook processes TV Broadcast Contours data sourced from Homeland Infrastructure Foundation-Level Data\n",
    "\n",
    "Data subsetting includes:\n",
    "* Isolating for 'STA' status TV broadcast contour stations\n",
    "\n",
    "Output is uploaded to 2a_subset directory within AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def subset_tv_contours(export=False, varname=''):\n",
    "    \"\"\"\n",
    "    The TV Broadcast Contours shapefile is large and includes data irrelevant to our Index. \n",
    "    With little source data documentation, we internally decided to subset the data to 'STA' \n",
    "    status TV broadcast contours as it included much of the data while reducing overlap. \n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    tv_contour_subset.ipynb\n",
    "    \"\"\"\n",
    "    print(\"Data transformation: Subset to 'STA' status stations\")\n",
    "\n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Bucket name and file paths\n",
    "    bucket_name = 'ca-climate-index'\n",
    "    directory = '1_pull_data/built_environment/communication_infrastructure/homeland_infrastructure_foundation_level_data/TV_Broadcast_Contours.zip'\n",
    "    out_directory = '2a_subset/built_environment/communication_infrastructure/homeland_infrastructure_foundation_level_data/'\n",
    "    \n",
    "    # Local directory to store the downloaded zip file and extracted contents\n",
    "    local_directory = 'temp'\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "    \n",
    "    # Download the zip file\n",
    "    #print(f'Pulling TV broadcast contour data from S3 bucket: {directory}')\n",
    "    local_zip_file_path = os.path.join(local_directory, os.path.basename(directory))\n",
    "    s3_client.download_file(bucket_name, directory, local_zip_file_path)\n",
    "    \n",
    "    # Extract the contents of the zip file\n",
    "    with ZipFile(local_zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(local_directory)\n",
    "    \n",
    "    # Manipulate the shapefile within the extracted contents\n",
    "    shapefile_path = None\n",
    "    \n",
    "    if export == True:\n",
    "        for filename in os.listdir(local_directory):\n",
    "            if filename.endswith('.shp'):\n",
    "                shapefile_path = os.path.join(local_directory, filename)\n",
    "                # Perform your manipulation with the shapefile using geopandas or other libraries\n",
    "                #print(\"Isolating shapefile to 'STA' status stations.\")\n",
    "                gdf = gpd.read_file(shapefile_path)\n",
    "                sta_gdf = gdf[gdf['STATUS'] == 'STA']\n",
    "                #print('Isolation complete, now making a folder to hold new datafile')\n",
    "                \n",
    "                # Create a directory to store the shapefile and its associated files\n",
    "                output_folder = os.path.join(local_directory, 'output_shapefile')\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                \n",
    "                # Save the filtered shapefile inside the output folder\n",
    "                output_shapefile_path = os.path.join(output_folder, 'sta_tv_contours.shp')\n",
    "                sta_gdf.to_file(output_shapefile_path)\n",
    "                \n",
    "                # Zip the output folder\n",
    "                #print('Zipping file')\n",
    "                output_zip_file_path = os.path.join(local_directory, 'sta_tv_contours.zip')\n",
    "                shutil.make_archive(output_zip_file_path[:-4], 'zip', output_folder)\n",
    "                \n",
    "                #print('Zipping complete, now uploading to AWS')\n",
    "                # Upload the zipped folder to AWS S3\n",
    "                s3_client.upload_file(output_zip_file_path, bucket_name, os.path.join(out_directory, 'sta_tv_contours.zip'))\n",
    "                #print(f'Zipped file uploaded to S3 bucket: {out_directory}')\n",
    "\n",
    "    if export == False:\n",
    "        print(f'sta_tv_contours.zip uploaded to AWS.')        \n",
    "    # Clean up temporary files\n",
    "    os.remove(local_zip_file_path)  # Remove the downloaded zip file\n",
    "    shutil.rmtree(local_directory)  # Remove the extracted files except the new shapefile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_tv_contours(export=False, varname='built_hifld_tv_contour')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
