{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script uploads manually downloaded data to AWS bucket for the California Climate Risk Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set AWS credentials\n",
    "s3 = boto3.resource('s3')\n",
    "s3_cl = boto3.client('s3') # for lower-level processes\n",
    "bucket_name = 'ca-climate-index'\n",
    "raw_path = '1_pull_data/' # path to raw datafiles in AWS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_datasource_dirs(domain, datasource):\n",
    "    \"\"\"Creates a dir in the respective domain dir, if not already available\"\"\"\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    # path to folder in aws\n",
    "    datasource_dir = '{0}{1}/{2}/'.format(raw_path, domain, datasource)\n",
    "\n",
    "    # # check if folder already exists\n",
    "    dirs = []\n",
    "    for item in bucket.objects.filter(Prefix=raw_path+domain+'/'):\n",
    "        d = str(item.key)\n",
    "        dirs += [d]\n",
    "\n",
    "    if datasource_dir not in dirs:\n",
    "        print('Creating folder for {}'.format(datasource_dir))\n",
    "    #     bucket.put_object(Key=datasource_dir)\n",
    "\n",
    "    return datasource_dir\n",
    "\n",
    "@append_metadata\n",
    "def manual_to_aws(domain, datasource, loc, export=False, varname=''):\n",
    "    \"\"\"\n",
    "    Uploads data that was manually downloaded to AWS bucket.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    domain: string\n",
    "        built_environment, governance, natural_systems, society_economy, climate_risk\n",
    "    datasource: string\n",
    "        Organization of datasource\n",
    "    loc: string\n",
    "        Local path to filename to upload\n",
    "    export: bool\n",
    "        If True, exports file to specified AWS bucket\n",
    "    \n",
    "    Script\n",
    "    ------\n",
    "    manual_pull_upload.ipynb\n",
    "    '''\n",
    "    \"\"\"\n",
    "    path_to_save = aws_datasource_dirs(domain, datasource)\n",
    "    print('{0} saved to {1}'.format(fname, path_to_save))\n",
    "    if export ==True:\n",
    "        # first check that folder is not already available\n",
    "        path_to_save = aws_datasource_dirs(domain, datasource)\n",
    "\n",
    "        # extract the filename from path\n",
    "        loc = loc.replace('\\\\', '/')\n",
    "        fname = loc.split('/')[-1]\n",
    "\n",
    "        # point to location of file(s) locally and upload to aws\n",
    "        try:\n",
    "            s3_cl.upload_file(\n",
    "                loc,\n",
    "                bucket_name,\n",
    "                aws_datasource_dirs(domain, datasource)+fname\n",
    "            )\n",
    "            print('{0} saved to {1}'.format(fname, path_to_save))\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling data pipeline file to obtain all variable names for metadata generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_file = sys.path[-1]+'/metadata/Full Data Pipeline Notes - 1_ Pull.csv'\n",
    "df = pd.read_csv(ref_file)\n",
    "# drop empty columns\n",
    "df = df.loc[:, df.columns.notna()]\n",
    "\n",
    "df = df.drop(columns=['Link','Unnamed: 15'])\n",
    "ref_df = df.fillna('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "#ref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter in metrics that were scraped and adjusted before uploading to AWS, as they have their own metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of file names excluding 'N/A'\n",
    "variable_names = [name for name in df['Variable'].values if name != 'N/A']\n",
    "\n",
    "# Define problematic files which we are still investigating\n",
    "skip_vars = ['natural_epa_air_quality',\n",
    "             'governance_edd_responder_firefighter',\n",
    "             'governance_edd_responder_nurse',\n",
    "             'governance_edd_responder_parametics',\n",
    "             'governance_edd_responder_police',\n",
    "             'climate_noaa_flood_fatalities',\n",
    "             'climate_usda_heat_crop_loss',\n",
    "             'climate_usda_heat_crop_cost']\n",
    "\n",
    "# Exclude files from the list\n",
    "included_vars = [name for name in variable_names if name not in skip_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating metadata for all metrics that were manually downloaded and uploaded to AWS\n",
    "* loop through each variable name not excluded and generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in included_vars:\n",
    "    # get the file name by itself (no subdirectories)\n",
    "    varname = \"test\"\n",
    "    print(varname)\n",
    "    # this is just for metadata creation, so export is set to false and the first\n",
    "    # three variables can be anything\n",
    "    manual_to_aws('all', 'all', 'any', export=False, varname=varname)\n",
    "    raise Exception"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
