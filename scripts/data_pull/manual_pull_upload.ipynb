{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script uploads manually downloaded data to AWS bucket for the California Climate Risk Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T22:03:42.520296Z",
     "iopub.status.busy": "2024-06-20T22:03:42.519105Z",
     "iopub.status.idle": "2024-06-20T22:03:43.116562Z",
     "shell.execute_reply": "2024-06-20T22:03:43.116094Z",
     "shell.execute_reply.started": "2024-06-20T22:03:42.520214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T22:03:43.117700Z",
     "iopub.status.busy": "2024-06-20T22:03:43.117525Z",
     "iopub.status.idle": "2024-06-20T22:03:43.138669Z",
     "shell.execute_reply": "2024-06-20T22:03:43.138359Z",
     "shell.execute_reply.started": "2024-06-20T22:03:43.117684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set AWS credentials\n",
    "s3 = boto3.resource('s3')\n",
    "s3_cl = boto3.client('s3') # for lower-level processes\n",
    "bucket_name = 'ca-climate-index'\n",
    "raw_path = '1_pull_data/' # path to raw datafiles in AWS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T22:05:44.465570Z",
     "iopub.status.busy": "2024-06-20T22:05:44.464367Z",
     "iopub.status.idle": "2024-06-20T22:05:44.482916Z",
     "shell.execute_reply": "2024-06-20T22:05:44.481924Z",
     "shell.execute_reply.started": "2024-06-20T22:05:44.465503Z"
    }
   },
   "outputs": [],
   "source": [
    "def aws_datasource_dirs(domain, datasource):\n",
    "    \"\"\"Creates a dir in the respective domain dir, if not already available\"\"\"\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    # path to folder in aws\n",
    "    datasource_dir = '{0}{1}/{2}/'.format(raw_path, domain, datasource)\n",
    "\n",
    "    # # check if folder already exists\n",
    "    dirs = []\n",
    "    for item in bucket.objects.filter(Prefix=raw_path+domain+'/'):\n",
    "        d = str(item.key)\n",
    "        dirs += [d]\n",
    "\n",
    "    if datasource_dir not in dirs:\n",
    "        print('Creating folder for {}'.format(datasource_dir))\n",
    "    #     bucket.put_object(Key=datasource_dir)\n",
    "\n",
    "    return datasource_dir\n",
    "\n",
    "@append_metadata\n",
    "def manual_to_aws(domain, datasource, loc, export=False, varname=''):\n",
    "    \"\"\"\n",
    "    Uploads data that was manually downloaded to AWS bucket.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    domain: string\n",
    "        built_environment, governance, natural_systems, society_economy, climate_risk\n",
    "    datasource: string\n",
    "        Organization of datasource\n",
    "    loc: string\n",
    "        Local path to filename to upload\n",
    "    export: bool\n",
    "        If True, exports file to specified AWS bucket\n",
    "    \n",
    "    Script\n",
    "    ------\n",
    "    manual_pull_upload.ipynb\n",
    "    '''\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract the filename from path\n",
    "    loc = loc.replace('\\\\', '/')\n",
    "    fname = loc.split('/')[-1]\n",
    "    path_to_save = aws_datasource_dirs(domain, datasource)\n",
    "    \n",
    "    if export == True:\n",
    "        # point to location of file(s) locally and upload to aws\n",
    "        try:\n",
    "            s3_cl.upload_file(\n",
    "                loc,\n",
    "                bucket_name,\n",
    "                aws_datasource_dirs(domain, datasource)+fname\n",
    "            )\n",
    "            print('{0} saved to {1}'.format(fname, path_to_save))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    if export == False:\n",
    "        # Metadata update optionality\n",
    "        print('{0} saved to {1}'.format(fname, path_to_save))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling data pipeline file to obtain all variable names for metadata generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T22:05:48.086304Z",
     "iopub.status.busy": "2024-06-20T22:05:48.085565Z",
     "iopub.status.idle": "2024-06-20T22:05:48.111941Z",
     "shell.execute_reply": "2024-06-20T22:05:48.111301Z",
     "shell.execute_reply.started": "2024-06-20T22:05:48.086228Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_file = sys.path[-1]+'/metadata/Full Data Pipeline Notes - 1_ Pull.csv'\n",
    "df = pd.read_csv(ref_file)\n",
    "# drop empty columns\n",
    "df = df.loc[:, df.columns.notna()]\n",
    "\n",
    "df = df.drop(columns=['Link','Unnamed: 15'])\n",
    "ref_df = df.fillna('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T22:05:48.537027Z",
     "iopub.status.busy": "2024-06-20T22:05:48.536318Z",
     "iopub.status.idle": "2024-06-20T22:05:48.544115Z",
     "shell.execute_reply": "2024-06-20T22:05:48.542786Z",
     "shell.execute_reply.started": "2024-06-20T22:05:48.536992Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "#ref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter in metrics that were scraped and adjusted before uploading to AWS, as they have their own metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T22:05:55.444982Z",
     "iopub.status.busy": "2024-06-20T22:05:55.444293Z",
     "iopub.status.idle": "2024-06-20T22:05:55.453666Z",
     "shell.execute_reply": "2024-06-20T22:05:55.451486Z",
     "shell.execute_reply.started": "2024-06-20T22:05:55.444941Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build list of file names excluding 'N/A'\n",
    "variable_names = [name for name in df['Variable'].values if name != 'N/A']\n",
    "\n",
    "# Define problematic files which we are still investigating\n",
    "skip_vars = ['natural_epa_air_quality',\n",
    "             'governance_edd_responder_firefighter',\n",
    "             'governance_edd_responder_nurse',\n",
    "             'governance_edd_responder_parametics',\n",
    "             'governance_edd_responder_police',\n",
    "             'climate_noaa_flood_fatalities',\n",
    "             'climate_usda_heat_crop_loss',\n",
    "             'climate_usda_heat_crop_cost']\n",
    "\n",
    "# Exclude files from the list\n",
    "included_vars = [name for name in variable_names if name not in skip_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating metadata for all metrics that were manually downloaded and uploaded to AWS\n",
    "* loop through each variable name not excluded and generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T22:07:10.237950Z",
     "iopub.status.busy": "2024-06-20T22:07:10.236740Z",
     "iopub.status.idle": "2024-06-20T22:07:10.247196Z",
     "shell.execute_reply": "2024-06-20T22:07:10.246196Z",
     "shell.execute_reply.started": "2024-06-20T22:07:10.237867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'built_cpuc_internet'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_vars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T22:06:02.980936Z",
     "iopub.status.busy": "2024-06-20T22:06:02.980121Z",
     "iopub.status.idle": "2024-06-20T22:06:03.807247Z",
     "shell.execute_reply": "2024-06-20T22:06:03.806664Z",
     "shell.execute_reply.started": "2024-06-20T22:06:02.980872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# this is just for metadata creation, so export is set to false and the first\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# three variables can be anything\u001b[39;00m\n\u001b[1;32m      7\u001b[0m manual_to_aws(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m, export\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, varname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for var in included_vars:\n",
    "    # get the file name by itself (no subdirectories)\n",
    "    varname = \"test\"\n",
    "    manual_to_aws(domain='built', datasource='all', loc='any', export=False, varname=varname)\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
