{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import upload_csv_aws\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d1997_c20220425.csv/StormEvents_details-ftp_v1.0_d1997_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d1997_c20220425.csv\\StormEvents_details-ftp_v1.0_d1997_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d1998_c20220425.csv/StormEvents_details-ftp_v1.0_d1998_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d1998_c20220425.csv\\StormEvents_details-ftp_v1.0_d1998_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d1999_c20220425.csv/StormEvents_details-ftp_v1.0_d1999_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d1999_c20220425.csv\\StormEvents_details-ftp_v1.0_d1999_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2000_c20220425.csv/StormEvents_details-ftp_v1.0_d2000_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2000_c20220425.csv\\StormEvents_details-ftp_v1.0_d2000_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2001_c20220425.csv/StormEvents_details-ftp_v1.0_d2001_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2001_c20220425.csv\\StormEvents_details-ftp_v1.0_d2001_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2002_c20220425.csv/StormEvents_details-ftp_v1.0_d2002_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2002_c20220425.csv\\StormEvents_details-ftp_v1.0_d2002_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2003_c20220425.csv/StormEvents_details-ftp_v1.0_d2003_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2003_c20220425.csv\\StormEvents_details-ftp_v1.0_d2003_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2004_c20220425.csv/StormEvents_details-ftp_v1.0_d2004_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2004_c20220425.csv\\StormEvents_details-ftp_v1.0_d2004_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2005_c20220425.csv/StormEvents_details-ftp_v1.0_d2005_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2005_c20220425.csv\\StormEvents_details-ftp_v1.0_d2005_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2006_c20220425.csv/StormEvents_details-ftp_v1.0_d2006_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2006_c20220425.csv\\StormEvents_details-ftp_v1.0_d2006_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2007_c20220425.csv/StormEvents_details-ftp_v1.0_d2007_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2007_c20220425.csv\\StormEvents_details-ftp_v1.0_d2007_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2008_c20220425.csv/StormEvents_details-ftp_v1.0_d2008_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2008_c20220425.csv\\StormEvents_details-ftp_v1.0_d2008_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2009_c20231116.csv/StormEvents_details-ftp_v1.0_d2009_c20231116.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2009_c20231116.csv\\StormEvents_details-ftp_v1.0_d2009_c20231116.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2010_c20220425.csv/StormEvents_details-ftp_v1.0_d2010_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2010_c20220425.csv\\StormEvents_details-ftp_v1.0_d2010_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2011_c20230417.csv/StormEvents_details-ftp_v1.0_d2011_c20230417.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2011_c20230417.csv\\StormEvents_details-ftp_v1.0_d2011_c20230417.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2012_c20221216.csv/StormEvents_details-ftp_v1.0_d2012_c20221216.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2012_c20221216.csv\\StormEvents_details-ftp_v1.0_d2012_c20221216.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2013_c20230118.csv/StormEvents_details-ftp_v1.0_d2013_c20230118.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2013_c20230118.csv\\StormEvents_details-ftp_v1.0_d2013_c20230118.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2014_c20231116.csv/StormEvents_details-ftp_v1.0_d2014_c20231116.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2014_c20231116.csv\\StormEvents_details-ftp_v1.0_d2014_c20231116.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2015_c20220425.csv/StormEvents_details-ftp_v1.0_d2015_c20220425.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2015_c20220425.csv\\StormEvents_details-ftp_v1.0_d2015_c20220425.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2016_c20220719.csv/StormEvents_details-ftp_v1.0_d2016_c20220719.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2016_c20220719.csv\\StormEvents_details-ftp_v1.0_d2016_c20220719.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2017_c20230317.csv/StormEvents_details-ftp_v1.0_d2017_c20230317.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2017_c20230317.csv\\StormEvents_details-ftp_v1.0_d2017_c20230317.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2018_c20230616.csv/StormEvents_details-ftp_v1.0_d2018_c20230616.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2018_c20230616.csv\\StormEvents_details-ftp_v1.0_d2018_c20230616.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2019_c20231017.csv/StormEvents_details-ftp_v1.0_d2019_c20231017.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2019_c20231017.csv\\StormEvents_details-ftp_v1.0_d2019_c20231017.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/StormEvents_details-ftp_v1.0_d2020_c20231217.csv/StormEvents_details-ftp_v1.0_d2020_c20231217.csv' to 'noaa_storm_event_files\\StormEvents_details-ftp_v1.0_d2020_c20231217.csv\\StormEvents_details-ftp_v1.0_d2020_c20231217.csv'\n",
      "Saved '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files/all_noaa_storm_events_ca.csv' to 'noaa_storm_event_files\\all_noaa_storm_events_ca.csv'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "def pull_csv_or_xls_from_directory(bucket_name, directory, main_folder, search_zipped=True):\n",
    "    \"\"\"\n",
    "    Pulls CSV or XLS files from a specified directory in an S3 bucket and downloads them locally\n",
    "    into a main folder.\n",
    "\n",
    "    Parameters:\n",
    "    - bucket_name (str): The name of the S3 bucket.\n",
    "    - directory (str): The directory within the bucket to search for CSV or XLS files.\n",
    "    - main_folder (str): The local main folder where files will be saved.\n",
    "    - search_zipped (bool): If True, search for CSV or XLS files within zip files. If False, search for CSV or XLS files directly.\n",
    "    \"\"\"\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # List objects in the specified directory\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=directory)\n",
    "\n",
    "    # Ensure main folder exists\n",
    "    if not os.path.exists(main_folder):\n",
    "        os.makedirs(main_folder)\n",
    "\n",
    "    # Check if objects were found\n",
    "    if 'Contents' in response:\n",
    "        # Iterate through each object found\n",
    "        for obj in response['Contents']:\n",
    "            # Get the key (filename) of the object\n",
    "            key = obj['Key']\n",
    "\n",
    "            # Determine the local file path\n",
    "            relative_path = os.path.relpath(key, directory)  # Get relative path\n",
    "            local_file_path = os.path.join(main_folder, relative_path)\n",
    "            local_dir_path = os.path.dirname(local_file_path)\n",
    "\n",
    "            # Ensure local directory exists\n",
    "            os.makedirs(local_dir_path, exist_ok=True)\n",
    "\n",
    "            # Check if the object is a .zip file\n",
    "            if search_zipped and key.endswith('.zip'):\n",
    "                # Download the zip file into memory\n",
    "                zip_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                zip_data = io.BytesIO(zip_object['Body'].read())\n",
    "\n",
    "                # Open the zip file\n",
    "                with zipfile.ZipFile(zip_data, 'r') as zip_ref:\n",
    "                    # Iterate through each file in the zip\n",
    "                    for file_name in zip_ref.namelist():\n",
    "                        # Check if the file is a .csv or .xls file\n",
    "                        if file_name.endswith('.csv') or file_name.endswith('.xls'):\n",
    "                            # Extract and save the file\n",
    "                            zip_ref.extract(file_name, local_dir_path)\n",
    "                            print(f\"Saved '{file_name}' to '{local_dir_path}'\")\n",
    "            elif not search_zipped and (key.endswith('.csv') or key.endswith('.xls')):\n",
    "                # Directly download the CSV or XLS file\n",
    "                file_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                file_data = io.BytesIO(file_object['Body'].read())\n",
    "\n",
    "                # Save the file locally\n",
    "                with open(local_file_path, 'wb') as f:\n",
    "                    f.write(file_data.getbuffer())\n",
    "                print(f\"Saved '{key}' to '{local_file_path}'\")\n",
    "    else:\n",
    "        print(\"No objects found in the specified directory.\")\n",
    "\n",
    "## Set-up for AWS\n",
    "s3_client = boto3.client('s3')  \n",
    "bucket_name = 'ca-climate-index'  \n",
    "directory = '1_pull_data/climate_risk/flood/loss/noaa/downloaded_files'\n",
    "\n",
    "pull_csv_or_xls_from_directory(bucket_name, directory, 'noaa_storm_event_files', search_zipped=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the main directory containing subfolders and CSV files\n",
    "main_folder = 'noaa_storm_event_files'\n",
    "\n",
    "# Output file to store the filtered and merged data\n",
    "output_file = 'all_noaa_storm_events_ca.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def merge_and_filter(data_folder, output_file, export=False, varname=''):\n",
    "    '''\n",
    "    Iterates through a folder with NOAA's storm event data and filters to the state of California, compiles to a single .csv file, and uploads to AWS bucket.\n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path: string\n",
    "        The folder containing all NOAA storm event CSV files from NOAA's bulk storm event download page: https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/\n",
    "    output_file: string\n",
    "        Final output as a .csv file.\n",
    "    '''\n",
    "\n",
    "    headers_written = False  # Flag to track if headers have been written to the output file\n",
    "\n",
    "    # Create an empty list to hold rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through the directory structure\n",
    "    for root, dirs, files in os.walk(data_folder):\n",
    "        for file in files:\n",
    "            # Check if the file is a CSV file\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Open and read each CSV file\n",
    "                with open(file_path, 'r', newline='') as infile:\n",
    "                    reader = csv.reader(infile)\n",
    "                    # Read the header row from the first file and write it to the output file\n",
    "                    if not headers_written:\n",
    "                        headers = next(reader)\n",
    "                        rows.append(headers)\n",
    "                        headers_written = True\n",
    "\n",
    "                        # Find the index of the column related to 'State'\n",
    "                        state_index = headers.index('STATE')\n",
    "\n",
    "                    # Append rows for California only using the index of the 'State' column\n",
    "                    for row in reader:\n",
    "                        if headers_written and row[state_index] == 'CALIFORNIA':  # Filter for California data\n",
    "                            rows.append(row)\n",
    "    \n",
    "    # Write the data to a CSV file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    if export == True:\n",
    "        # Save the file to AWS S3 using the client\n",
    "        with open(output_file, 'rb') as data:\n",
    "            s3_client.upload_fileobj(data, bucket_name, f\"{directory}/{output_file}\")\n",
    "        print(f\"Merged and sorted files written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the main folder path and output file\n",
    "merge_and_filter(main_folder, output_file, export=False, varname='climate_noaa_flood_fatalities')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
