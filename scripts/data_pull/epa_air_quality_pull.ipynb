{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load some library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import zipfile\n",
    "import io\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# suppress pandas purely educational warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import upload_csv_aws\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def scrape_website(base_url, starting_string, gathered_links, download_dir, final_csv_file_name, export=False, varname=''):\n",
    "    '''\n",
    "Web scrapes EPA's annual summarized air quality index (AQI) data. The data cleaned and eventually merged into a single .csv file containing California county air quality data from 1980-2023. Data was sourced from the Environmental Protection Agency (EPA).\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    Data was scraped by looking for a link keyword shared by all counties and data years.\n",
    "    Data was subsetted to California and separate files were merged into a single resulting csv file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base_url: string\n",
    "              Use the url to EPA's annual summary AQI data: \n",
    "              https://aqs.epa.gov/aqsweb/airdata/download_files.html#Annual\n",
    "    starting_string: string\n",
    "              A shared string that all links to the data within the url share: annual_aqi_by_county\n",
    "    gathered_links: string\n",
    "              Name a place to store all of the url references before data is converted to a folder with all the\n",
    "              data\n",
    "    download_dir: string\n",
    "              Name of the folder which will hold data csv files\n",
    "    final_csv_file_name: string\n",
    "              Name of the final single csv file containing all CA county air quality data 1980-2023\n",
    "    export: bool\n",
    "              If True, exports final csv to AWS bucket\n",
    "    \n",
    "    Script\n",
    "    ------\n",
    "    epa_air_quality_pull.ipynb\n",
    "    '''\n",
    "    print('Data transformation: Data subsetted for California.')\n",
    "    print('Data transformation: Data files merged into a single csv file.')\n",
    "    \n",
    "    if export == False:\n",
    "        return\n",
    "    else:\n",
    "        scraped_links = []\n",
    "        try:\n",
    "            # Send an HTTP GET request to the specified base_url with SSL certificate verification disabled\n",
    "            response = requests.get(base_url, verify=True)\n",
    "\n",
    "            # Check if the request was successful (status code 200)\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content of the page\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                links = soup.find_all('a')\n",
    "                for link in links:\n",
    "                    link_text = link.text\n",
    "                    link_href = link.get('href')\n",
    "                    if link_href and link_href.startswith(starting_string):\n",
    "                        scraped_links.append((link_text, link_href))\n",
    "\n",
    "                # Write the scraped links to a CSV file\n",
    "                with open('gathered_links.csv', 'w', newline='') as csv_file:\n",
    "\n",
    "                    csv_writer = csv.writer(csv_file)\n",
    "                    csv_writer.writerow(['Link Text', 'Link Href'])  # Write header row\n",
    "                    for link in scraped_links:\n",
    "                        csv_writer.writerow(link)\n",
    "\n",
    "                # Download and extract data from ZIP files\n",
    "                for link_text, link_href in scraped_links:\n",
    "                    zip_url = urljoin(base_url, link_href)  # Correctly construct the URL\n",
    "                    response = requests.get(zip_url)\n",
    "                    if response.status_code == 200:\n",
    "                        zip_data = response.content\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(io.BytesIO(zip_data), 'r') as zip_ref:\n",
    "                                zip_ref.extractall(download_dir)\n",
    "                        except zipfile.BadZipFile:\n",
    "                            print(f\"Failed to extract: {link_text} ({link_href}) is not a valid ZIP file.\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "        # Directory where the CSV files are located\n",
    "        input_folder = download_dir\n",
    "\n",
    "        # Output CSV file to store the filtered data\n",
    "        output_csv_file = \"all_ca_air_quality_data.csv\"\n",
    "\n",
    "        # Initialize an empty list to store the filtered data\n",
    "        filtered_data = []\n",
    "\n",
    "        # Define the filter condition (filtering files for California)\n",
    "        filter_condition = \"California\"\n",
    "\n",
    "        # Iterate through CSV files in the input folder\n",
    "        for root, _, files in os.walk(input_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"): \n",
    "                    file_path = os.path.join(root, file)\n",
    "                    with open(file_path, 'r') as csv_file:\n",
    "                        csv_reader = csv.reader(csv_file)\n",
    "                        header = next(csv_reader)  # Read the header row\n",
    "                        state_index = header.index('State')  # Find the index of 'State' column\n",
    "                        for row in csv_reader:\n",
    "                            if row[state_index] == filter_condition:\n",
    "                                filtered_data.append(row)\n",
    "\n",
    "        # Write the filtered data to a single CSV file\n",
    "        with open(output_csv_file, 'w', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(header)  # Write the header row\n",
    "            for row in filtered_data:\n",
    "                csv_writer.writerow(row)\n",
    "                \n",
    "        # Read in new CSV file with all CA county data for all years\n",
    "        ca_air_quality = pd.read_csv('all_ca_air_quality_data.csv')\n",
    "        # Drop duplicate years per county\n",
    "        ca_air_quality = ca_air_quality.drop_duplicates(subset=['Year', 'County'])\n",
    "        ca_air_quality.to_csv(final_csv_file_name) #saving CA air quality data that has been cleaned of repeats\n",
    "\n",
    "        if export == True:\n",
    "            bucket_name = 'ca-climate-index'\n",
    "            directory = '1_pull_data/natural_systems/ecosystem_condition/epa'\n",
    "            export_filename = [final_csv_file_name]\n",
    "            upload_csv_aws(export_filename, bucket_name, directory)\n",
    "\n",
    "        if os.path.exists:\n",
    "            os.remove('gathered_links.csv')\n",
    "            os.remove('all_ca_air_quality_data.csv')\n",
    "\n",
    "        # Define the folders you want to remove\n",
    "        folders_to_remove = ['air_quality_csv_files']\n",
    "\n",
    "        # Remove each folder\n",
    "        for folder in folders_to_remove:\n",
    "            if os.path.exists(folder):\n",
    "                shutil.rmtree(folder)\n",
    "            else:\n",
    "                print(f\"Folder not found: {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Run the function to extract EPA's county level annual air quality data      \n",
    "scrape_website('https://aqs.epa.gov/aqsweb/airdata/download_files.html#Annual', 'annual_aqi_by_county', 'gathered_links', 'air_quality_csv_files', 'natural_epa_air_quality.csv', export=False, varname='natural_epa_air_quality')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
