{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cal-CRAI Data Pull -- Crop Loss Data\n",
    "This notebook processes crop loss data sourced from USDA Risk Management: \\\n",
    "https://legacy.rma.usda.gov/data/cause.html\n",
    "\n",
    "Data pulling includes:\n",
    "* Isolates data to a list of desired columns\n",
    "* Merges all data to single .csv file\n",
    "\n",
    "Output is uploaded to 1_pull_data directory within AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import upload_csv_aws\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def pull_process_crop_loss(main_folder, output_folder, export=False, search_zipped=True, varname=''):\n",
    "    '''\n",
    "Pulls manually downloaded crop loss files sourced from USDA: https://legacy.rma.usda.gov/data/cause.html\n",
    "Pulled TXT files are cleaned and converted to CSV's, then merged into one single CSV file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    main_folder: string\n",
    "        Local main folder where the pulled files will be saved.\n",
    "    output_folder: string\n",
    "        Local folder where the merged output CSV will be saved.\n",
    "    gathered_links: string\n",
    "        Name a place to store all of the url references before data is converted to a folder with all the data\n",
    "    export: bool\n",
    "        If True, upload results to AWS bucket.\n",
    "    searched_zipped: bool\n",
    "        If True, search for csv, xls, or txt files within zip files from AWS. If false, look for these file types \n",
    "        directly\n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    usda_crop_loss.ipynb\n",
    "    '''\n",
    "    print('Data Transformation: data is converted from .txt to .csv files and merged into one final .csv')\n",
    "    # Set-up for AWS\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket_name = 'ca-climate-index'\n",
    "    directory = '1_pull_data/climate_risk/extreme_heat/loss/usda/usda_crop_loss_heat_files/'\n",
    "\n",
    "    file_contents = []  # Initialize file_contents list\n",
    "    # Function to extract year from line\n",
    "    def get_year_from_line(line):\n",
    "        elements = line.split('|')\n",
    "        if len(elements) > 1:\n",
    "            potential_year = elements[0].strip()\n",
    "            if len(potential_year) == 4 and potential_year.isdigit():\n",
    "                return potential_year\n",
    "        return ''\n",
    "\n",
    "    # Function to sort files based on the first column\n",
    "    def sort_files(file_contents):\n",
    "        sorted_files = []\n",
    "        for file, lines in file_contents:\n",
    "            lines.sort(key=lambda x: get_year_from_line(x))\n",
    "            sorted_files.append((file, lines))\n",
    "        return sorted_files\n",
    "\n",
    "    # Function to merge files\n",
    "    def merge_files(sorted_files, output_file):\n",
    "        headers = ['year', 'state_code', 'state_abbreviation', 'county_code',\n",
    "                   'county_name', 'commodity_code', 'commodity_name',\n",
    "                   'insurance_plan_code', 'insurance_plan_abbreviation',\n",
    "                   'stage_code', 'damage_cause_code', 'damage_description',\n",
    "                   'determined_acres', 'indemnity_amount']\n",
    "\n",
    "        # Create an empty list to hold rows\n",
    "        rows = []\n",
    "\n",
    "        for file, lines in sorted_files:\n",
    "            for line in lines:\n",
    "                row_data = [element.strip() for element in line.split('|')]\n",
    "                if len(row_data) > 2 and row_data[2] == 'CA':\n",
    "                    rows.append(row_data)\n",
    "\n",
    "        # Create DataFrame from the list of rows\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "        # Save DataFrame to local output folder\n",
    "        output_file_path = os.path.join(output_folder, output_file)\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "        #print(f\"Merged and sorted files written to {output_file_path}\")\n",
    "\n",
    "        # Save DataFrame to AWS using the client\n",
    "        new_buffer = io.StringIO()\n",
    "        df.to_csv(new_buffer, index=False)\n",
    "        content = new_buffer.getvalue()\n",
    "        s3_client.put_object(Bucket=bucket_name, Body=content, Key=directory + output_file)\n",
    "\n",
    "        # Optionally, return the DataFrame\n",
    "        return df\n",
    "\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # List objects in the specified directory\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=directory)\n",
    "\n",
    "    # Ensure main folder exists\n",
    "    if not os.path.exists(main_folder):\n",
    "        os.makedirs(main_folder)\n",
    "        #print(f\"Created main folder: {main_folder}\")\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        #print(f\"Created output folder: {output_folder}\")\n",
    "\n",
    "    # Check if objects were found\n",
    "    if 'Contents' in response:\n",
    "        # Iterate through each object found\n",
    "        for obj in response['Contents']:\n",
    "            # Get the key (filename) of the object\n",
    "            key = obj['Key']\n",
    "            #print(f\"Processing file: {key}\")\n",
    "\n",
    "            # Determine the local file path\n",
    "            relative_path = os.path.relpath(key, directory)  # Get relative path\n",
    "            local_file_path = os.path.join(main_folder, relative_path)\n",
    "            local_dir_path = os.path.dirname(local_file_path)\n",
    "\n",
    "            # Ensure local directory exists\n",
    "            os.makedirs(local_dir_path, exist_ok=True)\n",
    "            #print(f\"Ensured directory exists: {local_dir_path}\")\n",
    "\n",
    "            # Check if the object is a .zip file\n",
    "            if search_zipped and key.endswith('.zip'):\n",
    "                #print(f\"Downloading zip file: {key}\")\n",
    "                # Download the zip file into memory\n",
    "                zip_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                zip_data = io.BytesIO(zip_object['Body'].read())\n",
    "\n",
    "                # Open the zip file\n",
    "                with zipfile.ZipFile(zip_data, 'r') as zip_ref:\n",
    "                    # Iterate through each file in the zip\n",
    "                    for file_name in zip_ref.namelist():\n",
    "                        # Check if the file is a .csv, .xls, or .txt file\n",
    "                        if file_name.lower().endswith('.csv') or file_name.lower().endswith('.xls') or file_name.lower().endswith('.txt'):\n",
    "                            # Extract and save the file\n",
    "                            zip_ref.extract(file_name, local_dir_path)\n",
    "                            extracted_file_path = os.path.join(local_dir_path, file_name)\n",
    "                            #print(f\"Extracted and saved '{file_name}' to '{local_dir_path}'\")\n",
    "                            with open(extracted_file_path, 'r') as file:\n",
    "                                file_contents.append((file_name, file.readlines()))\n",
    "            elif not search_zipped and (key.endswith('.csv') or key.endswith('.xls') or key.endswith('.txt') or key.endswith('.TXT')):\n",
    "                #print(f\"Downloading file: {key}\")\n",
    "                # Directly download the CSV, XLS, or TXT file\n",
    "                file_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                file_data = io.BytesIO(file_object['Body'].read())\n",
    "\n",
    "                # Save the file locally\n",
    "                with open(local_file_path, 'wb') as f:\n",
    "                    f.write(file_data.getbuffer())\n",
    "                #print(f\"Saved '{key}' to '{local_file_path}'\")\n",
    "\n",
    "                # Read file contents\n",
    "                with open(local_file_path, 'r') as file:\n",
    "                    file_contents.append((key, file.readlines()))\n",
    "\n",
    "    sorted_files = sort_files(file_contents)\n",
    "    output_file = \"usda_crop_loss_merged.csv\"\n",
    "    merge_files(sorted_files, output_file)\n",
    "\n",
    "    if export == True:\n",
    "        directory = '1_pull_data/climate_risk/extreme_heat/loss/usda'\n",
    "        # Save the file to AWS S3 using the client\n",
    "        output_file_path = os.path.join(output_folder, output_file)\n",
    "        with open(output_file_path, 'rb') as data:\n",
    "            s3_client.upload_fileobj(data, bucket_name, f\"{directory}/{output_file}\")\n",
    "    if export == False:\n",
    "        print(f'{output_file} uploaded to AWS.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "pull_process_crop_loss('usda_crop_loss_heat_files', 'final_output_folder', export=False, search_zipped=True, varname='test')#'climate_usda_heat_crop_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_process_crop_loss('usda_crop_loss_heat_files', 'final_output_folder', export=False, search_zipped=True, varname='test')#'climate_usda_heat_crop_cost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
