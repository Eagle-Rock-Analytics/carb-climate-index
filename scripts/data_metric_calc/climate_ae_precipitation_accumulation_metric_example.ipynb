{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import climakitae as ck\n",
    "from climakitae.explore import warming_levels \n",
    "from climakitae.util.utils import add_dummy_time_to_wl\n",
    "from climakitae.core.data_interface import DataParameters\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import pyproj\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# import s3fs\n",
    "# import boto3\n",
    "# sys.path.append(os.path.expanduser('../../'))\n",
    "# from scripts.utils.file_helpers import upload_csv_aws\n",
    "\n",
    "# projection information\n",
    "import cartopy.crs as ccrs\n",
    "crs = ccrs.LambertConformal(\n",
    "    central_longitude=-70, \n",
    "    central_latitude=38, \n",
    "    false_easting=0.0, \n",
    "    false_northing=0.0,  \n",
    "    standard_parallels=[30, 60], \n",
    "    globe=None, \n",
    "    # cutoff=-30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sims_wl = [\n",
    "    'WRF_MPI-ESM1-2-HR_r3i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "    'WRF_MIROC6_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "    'WRF_EC-Earth3_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "    'WRF_TaiESM1_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "]\n",
    "sims_hist = [\n",
    "    'WRF_MPI-ESM1-2-HR_r3i1p1f1',\n",
    "    'WRF_MIROC6_r1i1p1f1', \n",
    "    'WRF_EC-Earth3_r1i1p1f1',\n",
    "    'WRF_TaiESM1_r1i1p1f1', \n",
    "]\n",
    "\n",
    "sim_name_dict = dict(zip(sims_wl,sims_hist)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reproject_to_tracts(ds_delta, ca_boundaries):\n",
    "    # this step takes about 12 minutes with 3km data (~1 min with 9km data)\n",
    "    df = ds_delta.to_dataframe().reset_index()\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.x,df.y))\n",
    "    gdf = gdf.set_crs(crs)\n",
    "    gdf = gdf.to_crs(ca_boundaries.crs)\n",
    "    \n",
    "    ca_boundaries = ca_boundaries.set_index(['GEOID'])    \n",
    "\n",
    "    clipped_gdf = gpd.sjoin_nearest(ca_boundaries, gdf, how='left')\n",
    "    clipped_gdf = clipped_gdf.drop(['index_right'], axis=1)\n",
    "    clipped_gdf = clipped_gdf.reset_index()[\n",
    "        [\"GEOID\",f\"{ds_delta.name}\",\"geometry\"]]\n",
    "    ### some coastal tracts do not contain any land grid cells ###\n",
    "    \n",
    "    # aggregate the gridded data to the tract level\n",
    "    clipped_gdf_diss = clipped_gdf.reset_index().dissolve(\n",
    "        by='GEOID', aggfunc='mean')\n",
    "    clipped_gdf_diss = clipped_gdf_diss.rename(\n",
    "        columns={f\"{ds_delta.name}_right\":\n",
    "                 ds_delta.name}\n",
    "    )\n",
    "    \n",
    "    # separate tracts with data from tracts without data\n",
    "    clipped_gdf_nan = clipped_gdf_diss[np.isnan(\n",
    "        clipped_gdf_diss[ds_delta.name]\n",
    "    )]\n",
    "    clipped_gdf_nan = clipped_gdf_nan[[\"geometry\",ds_delta.name]]\n",
    "    clipped_gdf_valid = clipped_gdf_diss[~np.isnan(\n",
    "        clipped_gdf_diss[ds_delta.name]\n",
    "    )]\n",
    "    clipped_gdf_valid = clipped_gdf_valid[[\"geometry\",ds_delta.name]]\n",
    "\n",
    "    # compute the centroid of each tract\n",
    "    clipped_gdf_nan[\"centroid\"] = clipped_gdf_nan.centroid\n",
    "    clipped_gdf_nan = clipped_gdf_nan.set_geometry(\"centroid\")\n",
    "    clipped_gdf_valid[\"centroid\"] = clipped_gdf_valid.centroid\n",
    "    clipped_gdf_valid = clipped_gdf_valid.set_geometry(\"centroid\")\n",
    "    \n",
    "    # fill in missing tracts with values from the closest tract\n",
    "    # in terms of distance between the tract centroids\n",
    "    clipped_gdf_filled = clipped_gdf_nan.sjoin_nearest(clipped_gdf_valid, how='left')\n",
    "    clipped_gdf_filled = clipped_gdf_filled[[\"geometry_left\",f\"{ds_delta.name}_right\"]]\n",
    "    clipped_gdf_filled = clipped_gdf_filled.rename(columns={\n",
    "        \"geometry_left\":\"geometry\", f\"{ds_delta.name}_right\":ds_delta.name\n",
    "    })\n",
    "    clipped_gdf_valid = clipped_gdf_valid.drop(columns=\"centroid\")\n",
    " \n",
    "    # concatenate filled-in tracts with the original tract which had data\n",
    "    gdf_all_tracts = pd.concat([clipped_gdf_valid,clipped_gdf_filled])\n",
    "\n",
    "    return gdf_all_tracts\n",
    "\n",
    "\n",
    "def min_max_standardize(df, col):\n",
    "    '''\n",
    "    Calculates min and max values for specified columns, then calculates\n",
    "    min-max standardized values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input dataframe   \n",
    "    cols_to_run_on: list\n",
    "        List of columns to calculate min, max, and standardize\n",
    "    '''\n",
    "    max_value = df[col].max()\n",
    "    min_value = df[col].min()\n",
    "\n",
    "    # Get min-max values, standardize, and add columns to df\n",
    "    prefix = col # Extracting the prefix from the column name\n",
    "    df[f'{prefix}_min'] = min_value\n",
    "    df[f'{prefix}_max'] = max_value\n",
    "    df[f'{prefix}_min_max_standardized'] = ((df[col] - min_value) / (max_value - min_value))\n",
    "\n",
    "    # note to add checker to make sure new min_max column values arent < 0 > 1\n",
    "    df[f'{prefix}_min_max_standardized'].loc[df[f'{prefix}_min_max_standardized'] < 0] = 0\n",
    "    df[f'{prefix}_min_max_standardized'].loc[df[f'{prefix}_min_max_standardized'] > 1] = 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve 2 deg C precipitation total data\n",
    "wl = warming_levels()\n",
    "wl.wl_params.timescale = \"daily\"\n",
    "wl.wl_params.downscaling_method = \"Dynamical\"\n",
    "wl.wl_params.variable = \"Precipitation (total)\"\n",
    "wl.wl_params.area_subset = \"CA counties\"\n",
    "wl.wl_params.cached_area = [\"Alameda County\"]\n",
    "wl.wl_params.warming_levels = [\"2.0\"]\n",
    "wl.wl_params.units = \"mm\"\n",
    "wl.wl_params.resolution = \"3 km\"\n",
    "wl.wl_params.anom = \"No\"\n",
    "wl.calculate()\n",
    "ds = wl.sliced_data[\"2.0\"] # grab 2.0 degC data\n",
    "ds = ds.sel(all_sims = list(sim_name_dict.keys()))\n",
    "total_precip = add_dummy_time_to_wl(ds)\n",
    "total_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve 2 deg C snowfall (snow and ice) data\n",
    "wl.wl_params.timescale = \"daily\"\n",
    "wl.wl_params.downscaling_method = \"Dynamical\"\n",
    "wl.wl_params.variable = \"Snowfall\"\n",
    "wl.wl_params.area_subset = \"CA counties\"\n",
    "wl.wl_params.cached_area = [\"Alameda County\"]\n",
    "wl.wl_params.warming_levels = [\"2.0\"]\n",
    "wl.wl_params.units = \"mm\"\n",
    "wl.wl_params.resolution = \"3 km\"\n",
    "wl.calculate()\n",
    "ds = wl.sliced_data[\"2.0\"] # grab 2.0 degC data\n",
    "ds = ds.sel(all_sims = list(sim_name_dict.keys()))\n",
    "total_snowfall = add_dummy_time_to_wl(ds)\n",
    "total_snowfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step 1b: Retrieve historical baseline data (1981-2010)\n",
    "# precip\n",
    "selections = DataParameters()\n",
    "selections.area_average = 'No'\n",
    "selections.timescale = 'daily'\n",
    "selections.variable = 'Precipitation (total)'\n",
    "selections.area_subset = 'CA counties'\n",
    "selections.cached_area = ['Alameda County']\n",
    "selections.scenario_historical = ['Historical Climate']\n",
    "selections.time_slice = (1981, 2010)\n",
    "selections.resolution = '3 km'\n",
    "selections.units = 'mm'\n",
    "hist_precip_ds = selections.retrieve()\n",
    "hist_precip_ds = hist_precip_ds.sel(simulation=sims_hist)\n",
    "hist_precip_ds\n",
    "\n",
    "# Snowfall (snow and ice)\n",
    "selections.variable = 'Snowfall'\n",
    "hist_snow_ds = selections.retrieve()\n",
    "hist_snow_ds = hist_snow_ds.sel(simulation=sims_hist)\n",
    "hist_snow_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate delta signal\n",
    "* calculate metric first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove snow from precip\n",
    "rain_wl = total_precip - total_snowfall\n",
    "rain_wl = rain_wl.clip(min=0.1)\n",
    "rain_hist = hist_precip_ds - hist_snow_ds\n",
    "rain_hist = rain_hist.clip(min=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove leap days from historical data\n",
    "rain_hist = rain_hist.sel(time=~((rain_hist.time.dt.month == 2) & (rain_hist.time.dt.day == 29)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rain_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rain_wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pool the data first\n",
    "hist_pool = rain_hist.stack(index=['simulation', 'time']).squeeze()\n",
    "wl_pool = rain_wl.stack(index=['all_sims', 'time']).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_wrf_pool_perc = hist_pool.chunk(\n",
    "    dict(index=-1)).quantile([.99],\n",
    "    dim='index').compute().squeeze()\n",
    "\n",
    "wl_wrf_pool_perc = wl_pool.chunk(\n",
    "    dict(index=-1)).quantile([.99],\n",
    "    dim='index').compute().squeeze()\n",
    "\n",
    "delta_wrf_pool_perc = (wl_wrf_pool_perc - hist_wrf_pool_perc)\n",
    "# absolute change in 99th percentile, data pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_wrf_pool_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_wrf_pool_perc.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename metric to be friendly for our remaining process\n",
    "delta_wrf_pool_perc.name = \"precip_99percentile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Reproject data to census tract projection\n",
    "# load in census tract shapefile\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\" \n",
    "# Beth needs to load locally for some reason:\n",
    "# census_shp_dir = \"tl_2021_06_tract.shp\"\n",
    "\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "\n",
    "# convert to area-preserving CRS\n",
    "ca_boundaries = ca_boundaries.to_crs(crs=3310)\n",
    "rain_df = reproject_to_tracts(delta_wrf_pool_perc, ca_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Min-max standardization\n",
    "# Using Cal-CRAI min-max standardization function, available in `utils.calculate_index.py`\n",
    "rain_std = min_max_standardize(rain_df, col=delta_wrf_pool_perc.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Export data as csv\n",
    "# data will be exported via pcluster run\n",
    "\n",
    "# clean up dataframes prior to export\n",
    "rain_std = rain_std.drop(columns=['geometry'])\n",
    "\n",
    "# export\n",
    "bucket_name = 'ca-climate-index'\n",
    "directory = '3_fair_data/index_data'\n",
    "\n",
    "precip_name = 'climate_metric.csv'\n",
    "rain_std.to_csv(precip_name)\n",
    "upload_csv_aws([precip_name], bucket_name, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
