{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cal-CRAI Metric Calculation\n",
    "Domain: Climate Risks \\\n",
    "Indicator: Flood Exposure\n",
    "\n",
    "This notebook calculates one metric, sourced from Iowa State University - Environmental Mesonet:\n",
    "* Metric 1: Median number of flood warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import dask_geopandas\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import pull_csv_from_directory, upload_csv_aws\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The biggest dataset: ISU Mesonet's flood warning database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of paths, since this is actually 10 files\n",
    "def build_isu_mesonet_file_list(\n",
    "    path='2b_reproject/climate_risk/flood/exposure/isu_environmental_mesonet'\n",
    "):\n",
    "    \"\"\" Build a list of shapefile URIs contained in S3 folder \"\"\"\n",
    "    # initiate empty list for s3 URIs\n",
    "    all_shapefiles = []\n",
    "    bucket_name = 'ca-climate-index' \n",
    "    # initiate s3 session\n",
    "    session = boto3.Session()\n",
    "    # use the session to get the resource\n",
    "    s3 = session.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    # iterate through directory\n",
    "    for obj in my_bucket.objects.filter(\n",
    "        Prefix=path):\n",
    "        all_shapefiles.append(obj.key)\n",
    "    return all_shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqt_list = build_isu_mesonet_file_list()\n",
    "path = '2b_reproject/climate_risk/flood/exposure/isu_environmental_mesonet' \n",
    "to_drop = ['hilbert_distance', 'WFO', 'EXPIRED', 'INIT_ISS', 'INIT_EXP',\n",
    "       'PHENOM', 'GTYPE', 'SIG', 'ETN', 'STATUS', 'NWS_UGC',\n",
    "       'UPDATED', 'HV_NWSLI', 'HV_SEV', 'HV_CAUSE', 'HV_REC', \n",
    "       'POLY_BEG', 'POLY_END', 'WINDTAG', 'HAILTAG', 'TORNTAG', \n",
    "        'DAMAGTAG', 'index_right', 'USCB_NAME','AREA_KM2','EMERGENC',\n",
    "          'geometry']\n",
    "bucket = 'ca-climate-index'\n",
    "\n",
    "df_list = []\n",
    "for f in pqt_list:\n",
    "    bucket_uri = f's3://{bucket}/{f}'\n",
    "    # read in as dask geopandas dataframe\n",
    "    df = dask_geopandas.read_parquet(bucket_uri)\n",
    "    # reduce memory use by dropping unneeded columns\n",
    "    df = df.drop(columns=to_drop)\n",
    "    # reduce by counting the # of events per tract:\n",
    "    # shave off time issued so we only have days    \n",
    "    df['ISSUED_day'] = df['ISSUED'].str.slice(0,8)\n",
    "    df = df.drop_duplicates(subset=['ISSUED_day', 'USCB_GEOID'], keep='first')\n",
    "    df['ISSUED_year'] = df['ISSUED'].str.slice(0,4)\n",
    "    df_out = df.groupby(\n",
    "        ['USCB_GEOID','ISSUED_day']\n",
    "    )['ISSUED'].count().compute().reset_index(\n",
    "    ).rename(columns={'ISSUED':'number_warnings'})\n",
    "    # append df_out to the list of dfs\n",
    "    df_list.append(df_out)\n",
    "    # clear memory\n",
    "    df_out = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat(df_list)\n",
    "# remove more duplicates which are exposed after merging\n",
    "df_merged = df_merged.drop_duplicates(subset=['ISSUED_day', 'USCB_GEOID'], keep='first')\n",
    "df_merged['ISSUED_year'] = df_merged['ISSUED_day'].str.slice(0,4)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df_merged.groupby(\n",
    "        ['USCB_GEOID','ISSUED_year']).count()\n",
    "dfg = dfg.drop(columns='ISSUED_day').unstack()\n",
    "dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = pd.DataFrame(dfg.median(axis=1)).reset_index()\n",
    "df_agg = df_agg.rename(columns={0:\"median_warning_days\",'USCB_GEOID':'GEOID'}) \n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in CA census tiger file\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "flood_warning_df = pd.merge(df_agg,ca_boundaries,on=\"GEOID\")\n",
    "flood_gdf = gpd.GeoDataFrame(\n",
    "    flood_warning_df, geometry=flood_warning_df[\"geometry\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing island tract, so we merge with 2021 census data which will add the island tract and give it NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "island_tract = df_agg[df_agg['GEOID'] == '06075980401']\n",
    "island_tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_boundaries = ca_boundaries['GEOID']\n",
    "ca_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_flood_data = pd.merge(df_agg, ca_boundaries, on='GEOID', how='right')\n",
    "merged_flood_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "island_tract = merged_flood_data[merged_flood_data['GEOID'] == '06075980401']\n",
    "island_tract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [2,4,6,8,10,12,14,16,18,20]\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "flood_gdf.plot(\n",
    "    column=\"median_warning_days\",\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    "    scheme='user_defined',\n",
    "    classification_kwds={'bins': bins})\n",
    "ax.set_title(\"Median annual flood warning days\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_flood_data = merged_flood_data.rename(columns={'median_warning_days':'median_flood_warning_days'})\n",
    "merged_flood_data.to_csv('climate_flood_warning_metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def isu_flood_warning_upload(input_csv, export=False, varname=''):\n",
    "    '''\n",
    "    Uploads the calculated flood warning metric to S3 bucket. The metric is:\n",
    "    Median number of flood (including coastal and flash) warning days\n",
    "    \n",
    "    Data for this metric was sourced from ISU's Environmental Mesonet at:\n",
    "    https://mesonet.agron.iastate.edu/request/gis/watchwarn.phtml\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    Data parquet files were read in and merged together.\n",
    "    Flood warning date, location, and count columns were retained.\n",
    "    Duplicate entries for a given location and date were dropped.\n",
    "    Data was grouped by location and flood warning year.\n",
    "    Number of flood warnings per year were summed per census tract.\n",
    "    The median number of flood warnings were calculated for each census tract.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_csv: string\n",
    "        csv flood warning data \n",
    "    export: True/False boolean\n",
    "        False = will not upload resulting df containing CAL CRAI flood warning metric to AWS\n",
    "        True = will upload resulting df containing CAL CRAI flood warning metric to AWS\n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    geoparquet-open.ipynb\n",
    "\n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "    '''\n",
    "    print('Data transformation: previously reprojected data parquets were read in and merged together.')\n",
    "    print('Data transformation: relevant metric columns were isolated.')\n",
    "    print('Data transformation: duplicate entries per location and date were dropped.')\n",
    "    print('Data transformation: GEOID 06075980401 (Farallon Islands, San Francisco County) filled with nan.') \n",
    " \n",
    "    if export == True:\n",
    "        bucket_name = 'ca-climate-index'\n",
    "        directory = '3_fair_data/index_data'\n",
    "        export_filename = [input_csv]\n",
    "        upload_csv_aws(export_filename, bucket_name, directory)\n",
    "\n",
    "    if export == False:\n",
    "        print(f'{input_csv} uploaded to AWS.')\n",
    " \n",
    "    if os.path.exists(input_csv):\n",
    "        os.remove(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = 'climate_flood_warning_metric.csv'\n",
    "varname = 'climate_iowa_mesonet_flash_flood_warnings'\n",
    "\n",
    "isu_flood_warning_upload(input_csv, export=True, varname='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
