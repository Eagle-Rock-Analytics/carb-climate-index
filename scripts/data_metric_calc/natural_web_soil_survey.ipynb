{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cal-CRAI Metric Calculation for: Natural Systems / Soil Health Metrics\n",
    "This notebook calculates 3 metrics, all sourced from the United States Department of Agriculture web soil survey.\n",
    "* % of soil cover rated fragile\n",
    "* % of soil rated moderately or severely drought vulnerable\n",
    "* % of soil moderately or severely susceptible to fire damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "\n",
    "# suppress pandas purely educational warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import pull_csv_from_directory, upload_csv_aws, filter_counties\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull csv from aws\n",
    "bucket_name = 'ca-climate-index'\n",
    "aws_dir = '1_pull_data/natural_systems/ecosystem_condition/usda/'\n",
    "\n",
    "pull_csv_from_directory(bucket_name, aws_dir, search_zipped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "drought_vulnerable_data = pd.read_csv('usda_web_soil_survey_drought_vulnerable.csv')\n",
    "fire_susceptibility_data = pd.read_csv('usda_web_soil_survey_fire_damage_susceptibility.csv')\n",
    "fragile_soils_data = pd.read_csv('usda_web_soil_survey_fragile_soils.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at all columns between the three datasets\n",
    "print(drought_vulnerable_data.columns)\n",
    "print(fire_susceptibility_data.columns)\n",
    "print(fragile_soils_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_vulnerable_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_susceptibility_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragile_soils_data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to clean all three datasets and calculate soil metrics\n",
    "* removes % sign and convert to numeric so calculations can be performed\n",
    "* while all three datasets share columns, their entries within have some variance, \n",
    "so we group all desired entries with the 'Ratings' column to be isolated for each dataset\n",
    "* the percentage column is summed within counties that have applicable ratings.\n",
    "* some counties have multiple entries (usually indicating a split within the county, east and west for example),\n",
    "so percentage sums are then averaged to estimate soil vulnerability between the sub-county split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets\n",
    "all_data = [drought_vulnerable_data, fire_susceptibility_data, fragile_soils_data]\n",
    "\n",
    "# Remove '%' symbol and convert to numeric for each DataFrame in all_data\n",
    "for i in range(len(all_data)):\n",
    "    all_data[i]['Percent of AOI'] = all_data[i]['Percent of AOI'].str.rstrip('%').astype(float)\n",
    "\n",
    "# List of ratings to filter\n",
    "ratings = [\n",
    "    'Fragile', 'Highly fragile', 'Extremely fragile', 'Moderately fragile',\n",
    "    'Moderately susceptible', 'Highly susceptible',\n",
    "    'Moderately drought vulnerable', 'Severely drought vulnerable'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "result_list = []\n",
    "\n",
    "# Function to clean and average county data\n",
    "def clean_and_average_counties(df):\n",
    "    df['Main County'] = df['County'].str.split(',').str[0]\n",
    "    df_cleaned = df.groupby('Main County')['Percent of AOI'].mean().reset_index()\n",
    "    df_cleaned.rename(columns={'Main County': 'county', 'Percent of AOI':'percent_vulnerable'}, inplace=True)\n",
    "    return df_cleaned\n",
    "\n",
    "# Loop through each dataset\n",
    "for data in all_data:\n",
    "    # Filter the dataset based on the 'Rating' values\n",
    "    filtered_data = data[data['Rating'].isin(ratings)]\n",
    "    \n",
    "    # Group by 'County' and sum the 'percent_vulnerable'\n",
    "    grouped_data = filtered_data.groupby('County')['Percent of AOI'].sum().reset_index()\n",
    "    \n",
    "    # Clean and average counties\n",
    "    cleaned_data = clean_and_average_counties(grouped_data)\n",
    "    \n",
    "    # Lower case all counties\n",
    "    cleaned_data = cleaned_data.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "    # Append the result to the list\n",
    "    result_list.append(cleaned_data)\n",
    "\n",
    "# Each element in result_list is a DataFrame with cleaned and averaged 'percent_vulnerable' per county for each dataset\n",
    "drought_vulnerable_result = result_list[0]\n",
    "fire_susceptibility_result = result_list[1]\n",
    "fragile_soils_result = result_list[2]\n",
    "\n",
    "# Display the results\n",
    "print(\"Drought Vulnerable Data Summed and Cleaned:\")\n",
    "print(drought_vulnerable_result)\n",
    "\n",
    "print(\"\\nFire Susceptibility Data Summed and Cleaned:\")\n",
    "print(fire_susceptibility_result)\n",
    "\n",
    "print(\"\\nFragile Soils Data Summed and Cleaned:\")\n",
    "print(fragile_soils_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there arent non-applicable county entries with our filter_counties function\n",
    "# Ran on all three resulting dfs, no non-applicable entries\n",
    "filtered, omitted = filter_counties(fragile_soils_result, 'county')\n",
    "omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in CA census tiger file\n",
    "ca_tract_county = \"s3://ca-climate-index/0_map_data/ca_tracts_county.csv\"\n",
    "ca_tract_county = gpd.read_file(ca_tract_county)\n",
    "ca_tract_county = ca_tract_county.drop(columns={'field_1', 'geometry', 'COUNTYFP'})\n",
    "ca_tract_county.columns = ca_tract_county.columns.str.lower()\n",
    "ca_tract_county = ca_tract_county.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "ca_tract_county"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each resulting df:\n",
    "* rename the percent column to indicate the metric\n",
    "* merge df with CA tract data based on shared county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_vulnerable_result = drought_vulnerable_result.rename(columns={'percent_vulnerable':'percent_vulnerable_drought'})\n",
    "drought_metric = pd.merge(ca_tract_county, drought_vulnerable_result, on='county', how='left')\n",
    "print(len(drought_metric))\n",
    "print(drought_metric.head())\n",
    "\n",
    "fire_susceptibility_result = fire_susceptibility_result.rename(columns={'percent_vulnerable':'percent_vulnerable_fire'})\n",
    "fire_soil_metric = pd.merge(ca_tract_county, fire_susceptibility_result, on='county', how='left')\n",
    "print(len(fire_soil_metric))\n",
    "print(fire_soil_metric.head())\n",
    "\n",
    "fragile_soils_result = fragile_soils_result.rename(columns={'percent_vulnerable':'percent_vulnerable_soils'})\n",
    "fragile_soil_metric = pd.merge(ca_tract_county, fragile_soils_result, on='county', how='left')\n",
    "print(len(fragile_soil_metric))\n",
    "print(fragile_soil_metric.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save each resulting df as a CSV to upload to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_metric.to_csv('natural_soils_vulnerable_drought_metric.csv')\n",
    "fire_soil_metric.to_csv('natural_soils_vulnerable_fire_metric.csv')\n",
    "fragile_soil_metric.to_csv('natural_fragile_soils_metric.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def web_soil_survey_metric_upload(input_csv, export=False, varname=''):    \n",
    "    '''\n",
    "    Uploads three csv files that contain metric calculations for soil health within Cal-CRAI's Natural Systems Domain.\n",
    "    Data was sourced from the USDA from: https://websoilsurvey.sc.egov.usda.gov/app/WebSoilSurvey.aspx\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    Each of the three datasets had the same columns, including the soil 'Rating'.\n",
    "    Desired entries within the 'Rating' column for each dataset were listed and isolated for each dataset.\n",
    "    Flagged ratings include: Fragile, Highly fragile, Extremely fragile, Moderately fragile, Moderately susceptible, Highly susceptible, Moderately drought vulnerable, Severely drought vulnerable.\n",
    "    Counties were grouped up, with the percentage column being summed to estimate total percentage vulnerability for each dataset.\n",
    "    Some counties were separated into sub-categories like 'Southern Humboldt' and 'Central Humboldt'. For counties with these multiple entries, their summed vulnerable percentages were averaged, and a single county entry was maintained.\n",
    "    Data were then merged to California census tract data to attribute county level soil vulnerabilities to tracts residing within that county \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: string\n",
    "        the dataframe containing the initial soil data\n",
    "    export: True/False boolean\n",
    "        False = will not upload resulting df containing CAL CRAI soil metric to AWS\n",
    "        True = will upload resulting df containing CAL CRAI soil metric to AWS\n",
    "    import_csv: string\n",
    "        name of the csv file to be uploaded to AWS\n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    natural_web_soil_survey.ipynb\n",
    "\n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are\n",
    "    stored in ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "    '''\n",
    "    print('Data transformation: data filtered for severity ratings.')\n",
    "    print('Data transformation: average percentage values for multi-county entries.')\n",
    "    print('Data transformation: merge data to California tracts.')\n",
    "\n",
    "    bucket_name = 'ca-climate-index'\n",
    "    directory = '3_fair_data/index_data'\n",
    "    export_filename = [input_csv]\n",
    "\n",
    "    if export == True:\n",
    "        upload_csv_aws(export_filename, bucket_name, directory)\n",
    "\n",
    "    if export == False:\n",
    "        print(f'{export_filename} uploaded to AWS.')\n",
    "\n",
    "    #if os.path.exists(input_csv):\n",
    "    #   os.remove(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = [\n",
    "            'natural_fragile_soils_metric.csv',\n",
    "            'natural_soils_vulnerable_drought_metric.csv',\n",
    "            'natural_soils_vulnerable_fire_metric.csv',\n",
    "            ]\n",
    "\n",
    "varnames = [\n",
    "    'natural_usda_soil_condition_1',\n",
    "    'natural_usda_soil_condition_2',\n",
    "    'natural_usda_soil_condition_3'\n",
    "    ]\n",
    "\n",
    "for csv, var in zip(input_csv, varnames):\n",
    "    web_soil_survey_metric_upload(csv, export=True, varname='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
