{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Surface Runoff metric\n",
    "This notebook briefly walks through how to calculate the flooding exposure metric `absolute change in 99th percentile surface runoff` from Cal-Adapt: Analytics Engine data. This notebook may be expanded upon for inclusion in cae-notebooks in the future.\n",
    "\n",
    "Order of operations:\n",
    "1. Read data in\n",
    "2. Calculate base function (FFWI, SPEI, warm nights, etc.)\n",
    "3. Calculate chronic\n",
    "4. Calculate delta signal\n",
    "5. Reprojection to census tracts\n",
    "6. Min-max standardization\n",
    "7. Export data\n",
    "\n",
    "Runtime: This notebook takes approximately ~3 hours to run due to data size, warming levels, and reprojection steps.\n",
    "\n",
    "### Step 0: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:16:40.838164Z",
     "iopub.status.busy": "2024-09-12T17:16:40.837746Z",
     "iopub.status.idle": "2024-09-12T17:16:56.975885Z",
     "shell.execute_reply": "2024-09-12T17:16:56.975165Z",
     "shell.execute_reply.started": "2024-09-12T17:16:40.838097Z"
    }
   },
   "outputs": [],
   "source": [
    "import climakitae as ck\n",
    "from climakitae.core.data_interface import DataParameters\n",
    "from climakitae.explore import warming_levels \n",
    "from climakitae.util.utils import add_dummy_time_to_wl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import pyproj\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "\n",
    "# projection information\n",
    "import cartopy.crs as ccrs\n",
    "crs = ccrs.LambertConformal(\n",
    "    central_longitude=-70, \n",
    "    central_latitude=38, \n",
    "    false_easting=0.0, \n",
    "    false_northing=0.0,  \n",
    "    standard_parallels=[30, 60], \n",
    "    globe=None, \n",
    "    # cutoff=-30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:16:56.978611Z",
     "iopub.status.busy": "2024-09-12T17:16:56.977212Z",
     "iopub.status.idle": "2024-09-12T17:16:56.982389Z",
     "shell.execute_reply": "2024-09-12T17:16:56.981764Z",
     "shell.execute_reply.started": "2024-09-12T17:16:56.978580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sims_wl = [\n",
    "    'WRF_MPI-ESM1-2-HR_r3i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "    'WRF_MIROC6_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "    'WRF_EC-Earth3_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "    'WRF_TaiESM1_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "]\n",
    "sims_hist = [\n",
    "    'WRF_MPI-ESM1-2-HR_r3i1p1f1',\n",
    "    'WRF_MIROC6_r1i1p1f1', \n",
    "    'WRF_EC-Earth3_r1i1p1f1',\n",
    "    'WRF_TaiESM1_r1i1p1f1', \n",
    "]\n",
    "\n",
    "sim_name_dict = dict(zip(sims_wl,sims_hist)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:16:56.983594Z",
     "iopub.status.busy": "2024-09-12T17:16:56.983257Z",
     "iopub.status.idle": "2024-09-12T17:16:56.993950Z",
     "shell.execute_reply": "2024-09-12T17:16:56.993298Z",
     "shell.execute_reply.started": "2024-09-12T17:16:56.983574Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reproject_to_tracts(ds_delta, ca_boundaries):\n",
    "    # this step takes about 12 minutes with 3km data (~1 min with 9km data)\n",
    "    df = ds_delta.to_dataframe().reset_index()\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.x,df.y))\n",
    "    gdf = gdf.set_crs(crs)\n",
    "    gdf = gdf.to_crs(ca_boundaries.crs)\n",
    "    \n",
    "    ca_boundaries = ca_boundaries.set_index(['GEOID'])    \n",
    "\n",
    "    clipped_gdf = gpd.sjoin_nearest(ca_boundaries, gdf, how='left')\n",
    "    clipped_gdf = clipped_gdf.drop(['index_right'], axis=1)\n",
    "    clipped_gdf = clipped_gdf.reset_index()[\n",
    "        [\"GEOID\",f\"{ds_delta.name}\",\"geometry\"]]\n",
    "    ### some coastal tracts do not contain any land grid cells ###\n",
    "    \n",
    "    # aggregate the gridded data to the tract level\n",
    "    clipped_gdf_diss = clipped_gdf.reset_index().dissolve(\n",
    "        by='GEOID', aggfunc='mean')\n",
    "    clipped_gdf_diss = clipped_gdf_diss.rename(\n",
    "        columns={f\"{ds_delta.name}_right\":\n",
    "                 ds_delta.name}\n",
    "    )\n",
    "    \n",
    "    # separate tracts with data from tracts without data\n",
    "    clipped_gdf_nan = clipped_gdf_diss[np.isnan(\n",
    "        clipped_gdf_diss[ds_delta.name]\n",
    "    )]\n",
    "    clipped_gdf_nan = clipped_gdf_nan[[\"geometry\",ds_delta.name]]\n",
    "    clipped_gdf_valid = clipped_gdf_diss[~np.isnan(\n",
    "        clipped_gdf_diss[ds_delta.name]\n",
    "    )]\n",
    "    clipped_gdf_valid = clipped_gdf_valid[[\"geometry\",ds_delta.name]]\n",
    "\n",
    "    # compute the centroid of each tract\n",
    "    clipped_gdf_nan[\"centroid\"] = clipped_gdf_nan.centroid\n",
    "    clipped_gdf_nan = clipped_gdf_nan.set_geometry(\"centroid\")\n",
    "    clipped_gdf_valid[\"centroid\"] = clipped_gdf_valid.centroid\n",
    "    clipped_gdf_valid = clipped_gdf_valid.set_geometry(\"centroid\")\n",
    "    \n",
    "    # fill in missing tracts with values from the closest tract\n",
    "    # in terms of distance between the tract centroids\n",
    "    clipped_gdf_filled = clipped_gdf_nan.sjoin_nearest(clipped_gdf_valid, how='left')\n",
    "    clipped_gdf_filled = clipped_gdf_filled[[\"geometry_left\",f\"{ds_delta.name}_right\"]]\n",
    "    clipped_gdf_filled = clipped_gdf_filled.rename(columns={\n",
    "        \"geometry_left\":\"geometry\", f\"{ds_delta.name}_right\":ds_delta.name\n",
    "    })\n",
    "    clipped_gdf_valid = clipped_gdf_valid.drop(columns=\"centroid\")\n",
    " \n",
    "    # concatenate filled-in tracts with the original tract which had data\n",
    "    gdf_all_tracts = pd.concat([clipped_gdf_valid,clipped_gdf_filled])\n",
    "\n",
    "    return gdf_all_tracts\n",
    "\n",
    "\n",
    "def min_max_standardize(df, col):\n",
    "    '''\n",
    "    Calculates min and max values for specified columns, then calculates\n",
    "    min-max standardized values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input dataframe   \n",
    "    cols_to_run_on: list\n",
    "        List of columns to calculate min, max, and standardize\n",
    "    '''\n",
    "    max_value = df[col].max()\n",
    "    min_value = df[col].min()\n",
    "\n",
    "    # Get min-max values, standardize, and add columns to df\n",
    "    prefix = col # Extracting the prefix from the column name\n",
    "    df[f'{prefix}_min'] = min_value\n",
    "    df[f'{prefix}_max'] = max_value\n",
    "    df[f'{prefix}_min_max_standardized'] = ((df[col] - min_value) / (max_value - min_value))\n",
    "\n",
    "    # note to add checker to make sure new min_max column values arent < 0 > 1\n",
    "    df[f'{prefix}_min_max_standardized'].loc[df[f'{prefix}_min_max_standardized'] < 0] = 0\n",
    "    df[f'{prefix}_min_max_standardized'].loc[df[f'{prefix}_min_max_standardized'] > 1] = 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:16:56.995500Z",
     "iopub.status.busy": "2024-09-12T17:16:56.995239Z",
     "iopub.status.idle": "2024-09-12T17:19:37.586726Z",
     "shell.execute_reply": "2024-09-12T17:19:37.585923Z",
     "shell.execute_reply.started": "2024-09-12T17:16:56.995480Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve 2 deg C surface runoff data data\n",
    "wl = warming_levels()\n",
    "wl.wl_params.enable_hidden_vars = True # how to grab surface runoff data!! sfc runoff is a \"hidden var\"\n",
    "wl.wl_params.timescale = \"daily\"\n",
    "wl.wl_params.downscaling_method = \"Dynamical\"\n",
    "wl.wl_params.variable = \"Surface runoff\"\n",
    "wl.wl_params.area_subset = \"states\"\n",
    "wl.wl_params.cached_area = [\"CA\"]\n",
    "wl.wl_params.warming_levels = [\"2.0\"]\n",
    "wl.wl_params.resolution = \"9 km\"\n",
    "wl.wl_params.anom = \"No\"\n",
    "wl.calculate()\n",
    "ds = wl.sliced_data[\"2.0\"] # grab 2.0 degC data\n",
    "ds = ds.sel(all_sims = list(sim_name_dict.keys()))\n",
    "wl_ds = add_dummy_time_to_wl(ds)\n",
    "wl_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:19:37.587895Z",
     "iopub.status.busy": "2024-09-12T17:19:37.587650Z",
     "iopub.status.idle": "2024-09-12T17:19:47.057688Z",
     "shell.execute_reply": "2024-09-12T17:19:47.056994Z",
     "shell.execute_reply.started": "2024-09-12T17:19:37.587873Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step 1b: Retrieve historical baseline data (1981-2010)\n",
    "selections = DataParameters()\n",
    "selections.enable_hidden_vars = True # how to grab surface runoff data!! sfc runoff is a \"hidden var\"\n",
    "selections.area_average = 'No'\n",
    "selections.timescale = 'daily'\n",
    "selections.downscaling_method = 'Dynamical'\n",
    "selections.variable = 'Surface runoff'\n",
    "selections.area_subset = 'states'\n",
    "selections.cached_area = ['CA']\n",
    "selections.scenario_historical = ['Historical Climate']\n",
    "selections.time_slice = (1981, 2010)\n",
    "selections.resolution = '9 km'\n",
    "hist_ds = selections.retrieve()\n",
    "hist_ds = hist_ds.sel(simulation=sims_hist)\n",
    "hist_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate delta signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:20:02.654987Z",
     "iopub.status.busy": "2024-09-12T17:20:02.654605Z",
     "iopub.status.idle": "2024-09-12T17:20:37.668739Z",
     "shell.execute_reply": "2024-09-12T17:20:37.668103Z",
     "shell.execute_reply.started": "2024-09-12T17:20:02.654963Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_99p = hist_ds.chunk(\n",
    "    dict(time=-1)).quantile([.99],\n",
    "    dim='time').compute().squeeze()\n",
    "\n",
    "wl_99p = wl_ds.chunk(\n",
    "    dict(time=-1)).quantile([.99],\n",
    "    dim='time').compute().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:21:54.611704Z",
     "iopub.status.busy": "2024-09-12T17:21:54.611316Z",
     "iopub.status.idle": "2024-09-12T17:21:54.631601Z",
     "shell.execute_reply": "2024-09-12T17:21:54.630869Z",
     "shell.execute_reply.started": "2024-09-12T17:21:54.611678Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# average over sim dimension for both\n",
    "wl_99p = wl_99p.mean(dim='all_sims')\n",
    "hist_99p = hist_99p.mean(dim='simulation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:22:17.311797Z",
     "iopub.status.busy": "2024-09-12T17:22:17.311422Z",
     "iopub.status.idle": "2024-09-12T17:22:17.318260Z",
     "shell.execute_reply": "2024-09-12T17:22:17.317463Z",
     "shell.execute_reply.started": "2024-09-12T17:22:17.311770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_sfcrun = (wl_99p - hist_99p)\n",
    "delta_sfcrun.name = 'surface_runoff' # rename metric to be friendly for our remaining process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:22:18.127760Z",
     "iopub.status.busy": "2024-09-12T17:22:18.127373Z",
     "iopub.status.idle": "2024-09-12T17:22:18.541685Z",
     "shell.execute_reply": "2024-09-12T17:22:18.540892Z",
     "shell.execute_reply.started": "2024-09-12T17:22:18.127738Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_sfcrun.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Reproject data to census tract projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:22:26.349423Z",
     "iopub.status.busy": "2024-09-12T17:22:26.349030Z",
     "iopub.status.idle": "2024-09-12T17:23:38.343379Z",
     "shell.execute_reply": "2024-09-12T17:23:38.342738Z",
     "shell.execute_reply.started": "2024-09-12T17:22:26.349399Z"
    }
   },
   "outputs": [],
   "source": [
    "# load in census tract shapefile\n",
    "# census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\" # pcluster run\n",
    "census_shp_dir = \"2021_ca_tract/tl_2021_06_tract.shp\" # local run, requires having census tracts loaded in file tree\n",
    "\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "\n",
    "# convert to area-preserving CRS\n",
    "ca_boundaries = ca_boundaries.to_crs(crs=3310)\n",
    "sfcrun_df = reproject_to_tracts(delta_sfcrun, ca_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Min-max standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:23:38.345563Z",
     "iopub.status.busy": "2024-09-12T17:23:38.344946Z",
     "iopub.status.idle": "2024-09-12T17:23:38.351754Z",
     "shell.execute_reply": "2024-09-12T17:23:38.351200Z",
     "shell.execute_reply.started": "2024-09-12T17:23:38.345529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Cal-CRAI min-max standardization function, available in `utils.calculate_index.py`\n",
    "sfcrun_std = min_max_standardize(sfcrun_df, col=delta_sfcrun.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:23:38.353138Z",
     "iopub.status.busy": "2024-09-12T17:23:38.352554Z",
     "iopub.status.idle": "2024-09-12T17:23:38.401152Z",
     "shell.execute_reply": "2024-09-12T17:23:38.400531Z",
     "shell.execute_reply.started": "2024-09-12T17:23:38.353115Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean up dataframes prior to export\n",
    "sfcrun_std = sfcrun_std.drop(columns=['geometry'])\n",
    "\n",
    "# export\n",
    "sfcrun_std.to_csv('climate_flood_exposure_surface_runoff_metric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T17:23:38.402800Z",
     "iopub.status.busy": "2024-09-12T17:23:38.402516Z",
     "iopub.status.idle": "2024-09-12T17:23:38.421452Z",
     "shell.execute_reply": "2024-09-12T17:23:38.420832Z",
     "shell.execute_reply.started": "2024-09-12T17:23:38.402779Z"
    }
   },
   "outputs": [],
   "source": [
    "sfcrun_std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
