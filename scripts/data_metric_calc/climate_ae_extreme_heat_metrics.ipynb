{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da4e854-5d2f-48f7-8dec-a074b2019239",
   "metadata": {},
   "source": [
    "## Extreme Heat Day and Warm Night Likelihood\n",
    "\n",
    "This notebook briefly walks through how to calculate the extreme heat exposure metric `% of change in extreme heat day and warm night event likelihood` from Cal-Adapt: Analytics Engine data. This notebook may be expanded upon for inclusion in cae-notebooks in the future. \n",
    "\n",
    "**Order of operations**:\n",
    "1. Read data in\n",
    "2. Calculate base function (FFWI, SPEI, warm nights, etc.)\n",
    "3. Calculate chronic\n",
    "4. Calculate delta signal\n",
    "5. Reprojection to census tracts\n",
    "6. Min-max standardization\n",
    "7. Export data\n",
    "8. Generate metadata (via Cal-CRAI environment, not AE)\n",
    "\n",
    "**Runtime**: This notebook takes approximately ~3 hours to run due to data size, warming levels, and reprojection steps. \n",
    "\n",
    "### Step 0: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261b8be-6c78-4ae2-b24f-0bfa4d770e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import climakitae as ck\n",
    "from climakitae.explore import warming_levels \n",
    "from climakitae.util.utils import add_dummy_time_to_wl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from xclim.indices import warm_night_frequency, hot_spell_frequency # extreme heat day and warm night function\n",
    "import pyproj\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "# projection information\n",
    "import cartopy.crs as ccrs\n",
    "crs = ccrs.LambertConformal(\n",
    "    central_longitude=-70, \n",
    "    central_latitude=38, \n",
    "    false_easting=0.0, \n",
    "    false_northing=0.0,  \n",
    "    standard_parallels=[30, 60], \n",
    "    globe=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06fac9-053f-4821-8c4d-8c18277eaae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sims_wl = [\n",
    "    'WRF_MPI-ESM1-2-HR_r3i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "    'WRF_MIROC6_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "    'WRF_EC-Earth3_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "    'WRF_TaiESM1_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual',\n",
    "]\n",
    "sims_hist = [\n",
    "    'WRF_MPI-ESM1-2-HR_r3i1p1f1',\n",
    "    'WRF_MIROC6_r1i1p1f1', \n",
    "    'WRF_EC-Earth3_r1i1p1f1',\n",
    "    'WRF_TaiESM1_r1i1p1f1', \n",
    "] \n",
    "\n",
    "def count_delta_extreme_heat_events(ds_hist,ds_wl):    \n",
    "  \n",
    "    # define the months over which we are going to \n",
    "    # determine the 98th percentile temperature threshold\n",
    "    # to define a hot day or warm night\n",
    "    months_to_measure = [m for m in np.arange(4,11,1)]\n",
    "    \n",
    "    sim_coord_dict = dict(zip(sims_wl,sims_hist))\n",
    "    \n",
    "    ds_hist = ds_hist.squeeze()\n",
    "    ds_wl = ds_wl.squeeze()\n",
    "    ds_template = ds_hist.isel(time=0, simulation=0).squeeze()\n",
    "    # first set consistent coordinates\n",
    "    ds_hist = ds_hist.sortby(\"simulation\")\n",
    "    ds_wl = ds_wl.rename({\"all_sims\" : \"simulation\"})\n",
    "    ds_wl = ds_wl.sortby(\"simulation\")\n",
    "    ds_wl = ds_wl.assign_coords({'simulation': list(sim_coord_dict.values())})\n",
    "    ds_wl = ds_wl.transpose(\"simulation\",\"time\",\"y\",\"x\")\n",
    "\n",
    "    # compute 98th percentile historical temperature between April and October\n",
    "    thresh_ds = ds_hist.sel(\n",
    "        time=ds_hist.time.dt.month.isin(months_to_measure)).chunk(\n",
    "            dict(time=-1)).quantile(0.98, dim=\"time\")\n",
    "    # count total days > 98th percentile in historical data and take annual average\n",
    "    hist_count = xr.where(ds_hist > thresh_ds, x=1, y=0).groupby(\n",
    "        \"time.year\").sum().mean(dim=\"year\").mean(dim=\"simulation\")\n",
    "    # count total days > 98th percentile in warming levels data and take annual average\n",
    "    chronic_count = xr.where(ds_wl > thresh_ds, x=1, y=0).groupby(\n",
    "        \"time.year\").sum().mean(dim=\"year\").mean(dim=\"simulation\")\n",
    "    # get the delta signal\n",
    "    delta_count = chronic_count - hist_count\n",
    "    # nan out non-CA grid points\n",
    "    delta_count = xr.where(np.isnan(ds_template), x=np.nan, y=delta_count)\n",
    "    return delta_count\n",
    "\n",
    "def reproject_to_tracts(ds_delta, ca_boundaries, county):\n",
    "    # this step takes about 12 minutes with 3km data (~1 min with 9km data)\n",
    "    df = ds_delta.to_dataframe().reset_index()\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.x,df.y))\n",
    "    gdf = gdf.set_crs(crs)\n",
    "    gdf = gdf.to_crs(ca_boundaries.crs)\n",
    "    \n",
    "    ca_boundaries = ca_boundaries.set_index(['GEOID'])\n",
    "    \n",
    "    clipped_gdf = gpd.sjoin_nearest(ca_boundaries, gdf, how='left')\n",
    "    clipped_gdf = clipped_gdf.drop(['index_right'], axis=1)\n",
    "    clipped_gdf = clipped_gdf[clipped_gdf[\"NAME\"]==county[0]]\n",
    "    ### some coastal tracts do not contain any land grid cells ###\n",
    "    ### due to the WRF's underlying surface type for a given grid cell. ###\n",
    "    \n",
    "    # aggregate the gridded data to the tract level\n",
    "    clipped_gdf_diss = clipped_gdf.reset_index().dissolve(\n",
    "        by='GEOID', aggfunc='mean')\n",
    "    clipped_gdf_diss = clipped_gdf_diss.rename(\n",
    "        columns={f\"{ds_delta.name}_right\":\n",
    "                 ds_delta.name}\n",
    "    )\n",
    "    \n",
    "    # separate tracts with data from tracts without data\n",
    "    clipped_gdf_nan = clipped_gdf_diss[np.isnan(\n",
    "        clipped_gdf_diss[ds_delta.name]\n",
    "    )]\n",
    "    clipped_gdf_nan = clipped_gdf_nan[[\"geometry\",ds_delta.name]]\n",
    "    clipped_gdf_valid = clipped_gdf_diss[~np.isnan(\n",
    "        clipped_gdf_diss[ds_delta.name]\n",
    "    )]\n",
    "    clipped_gdf_valid = clipped_gdf_valid[[\"geometry\",ds_delta.name]]\n",
    "\n",
    "    # compute the centroid of each tract\n",
    "    clipped_gdf_nan[\"centroid\"] = clipped_gdf_nan.centroid\n",
    "    clipped_gdf_nan = clipped_gdf_nan.set_geometry(\"centroid\")\n",
    "    clipped_gdf_valid[\"centroid\"] = clipped_gdf_valid.centroid\n",
    "    clipped_gdf_valid = clipped_gdf_valid.set_geometry(\"centroid\")\n",
    "    \n",
    "    # fill in missing tracts with values from the closest tract\n",
    "    # in terms of distance between the tract centroids\n",
    "    clipped_gdf_filled = clipped_gdf_nan.sjoin_nearest(clipped_gdf_valid, how='left')\n",
    "    clipped_gdf_filled = clipped_gdf_filled[[\"geometry_left\",f\"{ds_delta.name}_right\"]]\n",
    "    clipped_gdf_filled = clipped_gdf_filled.rename(columns={\n",
    "        \"geometry_left\":\"geometry\", f\"{ds_delta.name}_right\":ds_delta.name\n",
    "    })\n",
    "    clipped_gdf_valid = clipped_gdf_valid.drop(columns=\"centroid\")\n",
    " \n",
    "    # concatenate filled-in tracts with the original tract which had data\n",
    "    gdf_all_tracts = pd.concat([clipped_gdf_valid,clipped_gdf_filled])\n",
    "\n",
    "    return gdf_all_tracts\n",
    "\n",
    "def min_max_standardize(df, col):\n",
    "    '''\n",
    "    Calculates min and max values for specified columns, then calculates\n",
    "    min-max standardized values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input dataframe   \n",
    "    cols_to_run_on: list\n",
    "        List of columns to calculate min, max, and standardize\n",
    "    '''\n",
    "    max_value = df[col].max()\n",
    "    min_value = df[col].min()\n",
    "\n",
    "    # Get min-max values, standardize, and add columns to df\n",
    "    prefix = col # Extracting the prefix from the column name\n",
    "    df[f'{prefix}_min'] = min_value\n",
    "    df[f'{prefix}_max'] = max_value\n",
    "    df[f'{prefix}_min_max_standardized'] = ((df[col] - min_value) / (max_value - min_value))\n",
    "\n",
    "    # note to add checker to make sure new min_max column values arent < 0 > 1\n",
    "\n",
    "    # Drop the original columns\n",
    "    df = df.drop(columns=[col])\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce1161-31a0-4dd6-878f-c338ecbd3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in CA census tiger file -- not working from s3 link, uploading manually to keep testing\n",
    "# census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "census_shp_dir = \"tl_2021_06_tract.shp\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "ca_counties = gpd.read_file('ca_counties/')\n",
    "ca_counties = ca_counties.to_crs(ca_boundaries.crs)\n",
    "ca_boundaries = ca_boundaries[[\"COUNTYFP\",\"GEOID\",\"geometry\"]]\n",
    "ca_boundaries = pd.merge(ca_boundaries,ca_counties[[\"COUNTYFP\",\"NAME\"]],on=\"COUNTYFP\")\n",
    "ca_boundaries = ca_boundaries.to_crs(crs=3310) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d9401-358a-489b-8968-a7f3a7b5bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "county = [\"Los Angeles\"]\n",
    "print(f\"Calculating metric for {county[0]} County\")\n",
    "\n",
    "# get bounding box for county + small tolerance to avoid missing edge data\n",
    "county_bounds = ca_counties[ca_counties.NAME == county[0]].bounds\n",
    "minx = county_bounds.minx.values[0] - 0.1\n",
    "maxx = county_bounds.maxx.values[0] + 0.1\n",
    "miny = county_bounds.miny.values[0] - 0.1\n",
    "maxy = county_bounds.maxy.values[0] + 0.1\n",
    "\n",
    "# retrieve 2 deg C temperature maximum\n",
    "wl = warming_levels()\n",
    "wl.wl_params.timescale = \"daily\"\n",
    "wl.wl_params.downscaling_method = \"Dynamical\"\n",
    "wl.wl_params.variable = \"Maximum air temperature at 2m\"\n",
    "wl.wl_params.area_subset = \"CA counties\"\n",
    "wl.wl_params.cached_area = [\"Los Angeles County\"]\n",
    "wl.wl_params.warming_levels = [\"2.0\"]\n",
    "wl.wl_params.units = \"degC\"\n",
    "wl.wl_params.resolution = \"3 km\"\n",
    "wl.wl_params.anom = \"No\"\n",
    "wl.calculate()\n",
    "ds = wl.sliced_data[\"2.0\"] # grab 2.0 degC data\n",
    "ds = ds.sel(all_sims = sims_wl)\n",
    "wl_max_ds = add_dummy_time_to_wl(ds) # add time dimension back in, as this is removed by WL and is required for xclim functionality\n",
    "\n",
    "# retrieve 2 deg C temperature minimum\n",
    "wl = warming_levels() # reset\n",
    "wl.wl_params.timescale = \"daily\"\n",
    "wl.wl_params.downscaling_method = \"Dynamical\"\n",
    "wl.wl_params.variable = \"Minimum air temperature at 2m\"\n",
    "wl.wl_params.area_subset = \"CA counties\"\n",
    "wl.wl_params.cached_area = [\"Los Angeles County\"]\n",
    "wl.wl_params.warming_levels = [\"2.0\"]\n",
    "wl.wl_params.units = \"degC\"\n",
    "wl.wl_params.resolution = \"3 km\"\n",
    "wl.wl_params.anom = \"No\"\n",
    "wl.calculate()\n",
    "ds = wl.sliced_data[\"2.0\"] # grab 2.0 degC data\n",
    "ds = ds.sel(all_sims = sims_wl)\n",
    "wl_min_ds = add_dummy_time_to_wl(ds) # add time dimension back in, as this is removed by WL and is required for xclim functionality\n",
    "\n",
    "# retrieve historical baseline max temperature\n",
    "selections = ck.Select()\n",
    "selections.area_average = 'No'\n",
    "selections.timescale = 'daily'\n",
    "selections.variable = 'Maximum air temperature at 2m'\n",
    "selections.area_subset = 'CA counties'\n",
    "selections.cached_area = [\"Los Angeles County\"]\n",
    "selections.scenario_historical = ['Historical Climate']\n",
    "selections.time_slice = (1981, 2010)\n",
    "selections.resolution = '3 km'\n",
    "selections.units = 'degC'\n",
    "hist_max_ds = selections.retrieve()\n",
    "hist_max_ds = hist_max_ds.sel(simulation=sims_hist)\n",
    "\n",
    "# retrieve historical baseline min temperature\n",
    "selections = ck.Select() # rest\n",
    "selections.area_average = 'No'\n",
    "selections.timescale = 'daily'\n",
    "selections.variable = 'Minimum air temperature at 2m'\n",
    "selections.area_subset = 'CA counties'\n",
    "selections.cached_area = [\"Los Angeles County\"]\n",
    "selections.scenario_historical = ['Historical Climate']\n",
    "selections.time_slice = (1981, 2010)\n",
    "selections.resolution = '3 km'\n",
    "selections.units = 'degC'\n",
    "hist_min_ds = selections.retrieve()\n",
    "hist_min_ds = hist_min_ds.sel(simulation=sims_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3472b-a6cd-44fd-a72a-96e1181fc430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get change in # of hot days\n",
    "hd_delta_ds = count_delta_extreme_heat_events(\n",
    "    hist_max_ds, wl_max_ds\n",
    ")\n",
    "hd_delta_ds = ck.load(hd_delta_ds)\n",
    "hd_delta_ds.name = \"Mean change in annual extreme heat days\"\n",
    "\n",
    "# get change in # of warm nights\n",
    "wn_delta_ds = count_delta_extreme_heat_events(\n",
    "    hist_min_ds, wl_min_ds\n",
    ")\n",
    "wn_delta_ds.name = \"Mean change in annual warm nights\"\n",
    "wn_delta_ds = ck.load(wn_delta_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb111139-8f18-43e1-8aac-57e9dd79f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject data to census tracts\n",
    "hd_df = reproject_to_tracts(hd_delta_ds, ca_boundaries, county)\n",
    "wn_df = reproject_to_tracts(wn_delta_ds, ca_boundaries, county)\n",
    "    \n",
    "wn_data_std = min_max_standardize(wn_df, col=wn_delta_ds.name)\n",
    "display(wn_data_std)\n",
    "hd_data_std = min_max_standardize(hd_df, col=hd_delta_ds.name)\n",
    "display(hd_data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b8bba-0b91-4012-b3ed-218a96b9e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_data_std.to_csv('climate_extreme_heat_warm_night_metric.csv')\n",
    "hd_data_std.to_csv('climate_extreme_heat_hot_day_metric.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd273b51-b7f3-4ca0-81b3-f7c173552927",
   "metadata": {},
   "source": [
    "### Step 6: Metadata\n",
    "This function below is designed to be run solely in Cal-CRAI environment, not on the Analytics Engine JupyterHub. \n",
    "\n",
    "Since this is slightly different than the other metrics, process to generate metadata:\n",
    "* Open in Cal-CRAI environment\n",
    "* Make sure that csv file to export is in same directory\n",
    "* **Only run the following 3 cells**\n",
    "* Upload to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6dc43-87a2-4a1d-8100-599bf4331ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import upload_csv_aws\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31becc-30e0-40fd-b5f9-a096a42a497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = pd.read_csv('climate_extreme_heat_likelihood_metric.csv') # make sure this is in the same folder!\n",
    "df_in # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cacbc2c-8ea4-4fce-b4b8-d1d98a66f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def extreme_heat_ae_data_process(df, export=False, export_filename=None, varname=''):\n",
    "    '''\n",
    "    Reduces the size of the initial daily raw temperature data in order to streamline compute time.\n",
    "    Transforms the raw data into the following baseline metrics:\n",
    "    * Warm night frequency\n",
    "    * Extreme heat day frequency\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    Metric is aggregated using xclim.indices functionality corresponding to the varname.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Input data.\n",
    "    export: True/False boolean\n",
    "        False = will not upload resulting df containing CAL CRAI extreme heat metric to AWS\n",
    "        True = will upload resulting df containing CAL CRAI extreme heat metric to AWS\n",
    "    export_filename: string\n",
    "        name of csv file to be uploaded to AWS\n",
    "    varname: string\n",
    "        Final metric name, for metadata generation\n",
    "        \n",
    "    Script\n",
    "    ------\n",
    "    climate_ae_heatday_warmnight.ipynb\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Because the climate projections data is on the order of 2.4 TB in size, intermediary\n",
    "    processed files are not produced for each stage of the metric calculation. All processing\n",
    "    occurs in a single complete run in the notebook listed above.\n",
    "    '''\n",
    "        \n",
    "    # historical baseline\n",
    "    print(\"Data transformation: historical baseline data retrieved for 1981-2010, averaging across models.\")\n",
    "    print(\"Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\")\n",
    "    print(\"Data transformation: drop all singleton dimensions (scenario).\")\n",
    "    print(\"Data transformation: daily minimum calculated from hourly data for input into xclim.indices.warm_night_frequency.\")\n",
    "    print(\"Data transformation: daily maximum calculated from hourly data for input into xclim.indices.hot_spell_frequency.\")\n",
    "    print(\"Data transformation: percent likelihood of event occurrence calculated for historical baseline (1981-2010) period, averaging across time.\")\n",
    "    \n",
    "    # calculate chronic with 2째C WL\n",
    "    print('Data transformation: raw projections data retrieved for warming level of 2.0째C, by manually subsetting based on GWL for parent GCM and calculating 30 year average.')\n",
    "    print(\"Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\")\n",
    "    print(\"Data transformation: drop all singleton dimensions (scenario).\")\n",
    "    print(\"Data transformation: daily minimum calculated from hourly data for input into xclim.indices.warm_night_frequency.\")\n",
    "    print(\"Data transformation: daily maximum calculated from hourly data for input into xclim.indices.hot_spell_frequency.\")\n",
    "    print(\"Data transformation: percent likelihood of event occurrence calculated for chronic period (2.0째C warming level), averaging across time.\")\n",
    "    \n",
    "    # calculate delta signal\n",
    "    print(\"Data transformation: hot spell frequency and warm night frequency likelihoods averaged together into single metric.\")\n",
    "    print(\"Data transformation: delta signal calculated by taking difference between chronic (2.0째C) and historical baseline.\")\n",
    "\n",
    "    # reprojection to census tracts\n",
    "    print(\"Data transformation: data transformed from xarray dataset into pandas dataframe.\")\n",
    "    print(\"Data transformation: data reprojected from Lambert Conformal Conic CRS to CRS 3857.\")\n",
    "        \n",
    "    # min-max standardization\n",
    "    print(\"Data transformation: data min-max standardized with min_max_standardize function.\")\n",
    "    \n",
    "    # export data as csv\n",
    "    if export == True:\n",
    "        bucket_name = 'ca-climate-index'\n",
    "        directory = '3_fair_data/index_data'\n",
    "        export_filename = [export_filename]\n",
    "        upload_csv_aws(export_filename, bucket_name, directory)\n",
    "        \n",
    "        # # Check if file exists before attempting to remove it\n",
    "        # if os.path.exists('climate_extreme_heat_likelihood_metric.csv'):\n",
    "        #     os.remove('climate_extreme_heat_likelihood_metric') # remove from local to clear up directory\n",
    "        \n",
    "        # if os.path.exists(export_filename[0]):\n",
    "        #     os.remove(export_filename[0])\n",
    "    \n",
    "    if export == False:\n",
    "        print(f'{export_filename} uploaded to AWS.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a4987-12aa-4a3c-abd3-f0b209440ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_heat_ae_data_process(df_in, export=False, export_filename ='climate_extreme_heat_likelihood_metric.csv', varname='test') # varname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
