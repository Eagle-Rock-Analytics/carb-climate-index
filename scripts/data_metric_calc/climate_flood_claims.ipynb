{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cal-CRAI Metric Calculation\n",
    "Domain: Climate Risks \\\n",
    "Indicator: Flooding Loss\n",
    "\n",
    "This notebook calculates one metric, sourced from the Federal Emergency Management Agency:\n",
    "* Metric 1: Average flood insurance payout per number of claims per census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")\n",
    "from scripts.utils.file_helpers import (\n",
    "    pull_csv_from_directory, upload_csv_aws\n",
    ") \n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in CA census tiger file\n",
    "ca_tract_county = \"s3://ca-climate-index/0_map_data/ca_tracts_county.csv\"\n",
    "ca_tract_county = gpd.read_file(ca_tract_county)\n",
    "ca_tract_county = ca_tract_county.drop(columns={'field_1', 'geometry'})\n",
    "ca_tract_county.columns = ca_tract_county.columns.str.lower()\n",
    "ca_tract_county = ca_tract_county.applymap(lambda s: s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'ca-climate-index'\n",
    "aws_dir = '2a_subset/climate_risk/flood/loss/fema/flood_claims_ca/'\n",
    "folder = 'csv_folder'\n",
    "\n",
    "pull_csv_from_directory(bucket_name, aws_dir, folder, search_zipped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_claim_data = pd.read_csv(r'csv_folder/fema_claims_CA_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_claim_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_claim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select relevant columns to our metrics\n",
    "columns_keep = [\n",
    "    'id',\n",
    "    'countyCode',\n",
    "    'censusTract',\n",
    "    'policyCount',\n",
    "    'yearOfLoss',\n",
    "    'netIccPaymentAmount',\n",
    "    'netContentsPaymentAmount',\n",
    "    'netBuildingPaymentAmount',\n",
    "    \"amountPaidOnBuildingClaim\", \n",
    "    \"amountPaidOnContentsClaim\", \n",
    "    \"amountPaidOnIncreasedCostOfComplianceClaim\"\n",
    "]\n",
    "flood_claim_columns = flood_claim_data[columns_keep]\n",
    "\n",
    "# adjust county and tract columns, changing name, data type, and data formatting\n",
    "flood_claim_columns['countyCode'] = flood_claim_columns['countyCode'].astype(str).str[1:].str.split('.').str[0]\n",
    "flood_claim_columns = flood_claim_columns.rename(columns={'countyCode':'countyfp'})\n",
    "\n",
    "flood_claim_columns['censusTract'] = flood_claim_columns['censusTract'].apply(lambda x: '0' + str(int(float(x))) if pd.notnull(x) else x)\n",
    "flood_claim_columns = flood_claim_columns.rename(columns={'censusTract':'tract'})\n",
    "\n",
    "# drop duplicates based on event id columns, there are no duplicates\n",
    "selected_columns = ['id']\n",
    "flood_claim_drop_duplicates = flood_claim_columns.duplicated(subset=selected_columns, keep='first')\n",
    "duplicate_count = flood_claim_drop_duplicates.sum()\n",
    "duplicate_count\n",
    "\n",
    "duplicate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that do not have location identifier\n",
    "flood_claim_cleaned = flood_claim_columns.dropna(subset=['countyfp', 'tract'], how='all')\n",
    "\n",
    "# drop rows that do not have a value when summing payout columns\n",
    "columns_to_sum = [\"amountPaidOnBuildingClaim\", \n",
    "                  \"amountPaidOnContentsClaim\", \n",
    "                  \"amountPaidOnIncreasedCostOfComplianceClaim\"]\n",
    "\n",
    "# Create a new dataframe with rows where the sum of the specified columns is non-zero\n",
    "flood_claim_cleaned = flood_claim_cleaned[flood_claim_cleaned[columns_to_sum].sum(axis=1) != 0]\n",
    "\n",
    "# Display the new dataframe\n",
    "print(len(flood_claim_cleaned))\n",
    "flood_claim_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 1: Average flood insurance payout per number of claims per census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column summing relevant columns representing how much insurance paid out on these claims\n",
    "flood_claim_total_cost = flood_claim_cleaned\n",
    "flood_claim_total_cost['total_insurance_payment'] = flood_claim_cleaned['netIccPaymentAmount'] + flood_claim_cleaned['netContentsPaymentAmount'] + flood_claim_cleaned['netBuildingPaymentAmount']\n",
    "flood_claim_total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data my tract and year of loss and sum the total cost\n",
    "flood_claim_cost_grouped = flood_claim_total_cost.groupby(['tract', 'yearOfLoss']).agg({\n",
    "    'total_insurance_payment': 'sum',            # Sum the policyCount\n",
    "    'id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "flood_claim_cost_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by countyCode and calculate the mean for total_insurance_payment\n",
    "county_cost_averages = flood_claim_cost_grouped.groupby('tract').agg({\n",
    "    'total_insurance_payment': 'mean',   # Calculate the average rowCount\n",
    "    'id' : 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "county_cost_averages = county_cost_averages.rename(columns={\n",
    "    'total_insurance_payment': 'average_insurance_payout',\n",
    "    'id':'total_claims'\n",
    "})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "county_cost_averages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cost_averages['avg_insurance_payout_per_claim'] = county_cost_averages['average_insurance_payout'] / county_cost_averages['total_claims']\n",
    "\n",
    "county_cost_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with CA census tracts\n",
    "merged_flood_claims_cost = pd.merge(ca_tract_county, county_cost_averages, on='tract', how='left')\n",
    "print(len(merged_flood_claims_cost))\n",
    "merged_flood_claims_cost.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_claims_metric = merged_flood_claims_cost.drop(columns={'countyfp', 'average_insurance_payout', 'total_claims'})\n",
    "flood_claims_metric = flood_claims_metric.rename(columns={'avg_insurance_payout_per_claim':'avg_flood_insurance_payout_per_claim'})\n",
    "flood_claims_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_count = flood_claims_metric['avg_flood_insurance_payout_per_claim'].count()\n",
    "non_nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_claims_metric.to_csv('climate_flood_cost_metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def flood_claims_cost_upload(input_csv, export=False, varname=''):\n",
    "    '''\n",
    "    Uploads the flood claim and cost metrics to S3 bucket. The metrics are:\n",
    "    \n",
    "    * Average flood insurance payout per number of claims per census tract\n",
    "    \n",
    "    Data for these metrics are sourced from FEMA's redacted NFIP claims at:\n",
    "    https://www.google.com/url?q=https://www.fema.gov/openfema-data-page/fima-nfip-redacted-claims-v2&sa=D&source=editors&ust=1723749642983941&usg=AOvVaw0-Ri52Pad7wzLYu2eNKABx\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    Relevant columns to our data metrics were isolated, renamed, and adjusted for consistency.\n",
    "    Data was isolated to include data in non-nan census tracts.\n",
    "    Duplicates were removed based on event ID.\n",
    "    Only rows that had non-nan values after summing payout columns were retained.\n",
    "    Data were grouped by tract and year then summed to identify number of events/cost per tract per year.\n",
    "    Data were grouped again by tract and averaged to identify average cost/number of policies per census tract.\n",
    "    The payout column was divided by number of claims.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_csv: string\n",
    "        csv flood claim/cost data \n",
    "    export: True/False boolean\n",
    "        False = will not upload resulting df containing CAL CRAI flood claim/cost  metrics to AWS\n",
    "        True = will upload resulting df containing CAL CRAI flood claim/cost metrics to AWS\n",
    "\n",
    "    Script\n",
    "    ------\n",
    "    climate_flood_claims.ipynb\n",
    "\n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "    '''\n",
    "    print('Data transformation: relevant columns were isolated and renamed')\n",
    "    print('Data transformation: duplicate entries by event ID were dropped.')\n",
    "    print('Data transformation: data was grouped by tract & year then summed, then grouped once again and averaged.')\n",
    "    print('Data transformation: data was merged with California census tracts.') \n",
    " \n",
    "    if export == True:\n",
    "        bucket_name = 'ca-climate-index'\n",
    "        directory = '3_fair_data/index_data'\n",
    "        export_filename = [input_csv]\n",
    "        upload_csv_aws(export_filename, bucket_name, directory)\n",
    "\n",
    "    if export == False:\n",
    "        print(f'{input_csv} uploaded to AWS.')\n",
    " \n",
    "    if os.path.exists(input_csv):\n",
    "        os.remove(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csvs = ['climate_flood_cost_metric.csv']\n",
    "\n",
    "varnames = ['climate_fema_nfip_claim_cost']\n",
    "\n",
    "# Process the data and export\n",
    "for input_csv, varname in zip(input_csvs, varnames):\n",
    "    print(f'Processing {input_csv} with varname {varname}')\n",
    "    flood_claims_cost_upload(input_csv, export=True, varname='test')\n",
    "    print(f'Completed uploading {input_csv} with varname {varname}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
