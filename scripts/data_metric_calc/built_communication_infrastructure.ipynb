{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cal-CRAI metric calculation: built environment communication infrastructure\n",
    "* num of cell towers\n",
    "* num of radio towers\n",
    "* num of microwave towers\n",
    "* num of paging towers\n",
    "* num of broadcast towers\n",
    "* num of broadcast providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import io\n",
    "import geopandas as gpd\n",
    "\n",
    "# suppress pandas purely educational warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import pull_csv_from_directory, upload_csv_aws#, pull_gpkg_from_directory\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_gpkg_from_directory(bucket_name, directory):\n",
    "    \"\"\"\n",
    "    Pulls GeoPackage files from a specified directory in an S3 bucket.\n",
    "    \n",
    "    Parameters:\n",
    "    - bucket_name (str): The name of the S3 bucket.\n",
    "    - directory (str): The directory within the bucket to search for GeoPackage files.\n",
    "    \"\"\"\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # List objects in the specified directory\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=directory)\n",
    "\n",
    "    # Check if objects were found\n",
    "    if 'Contents' in response:\n",
    "        # Iterate through each object found\n",
    "        for obj in response['Contents']:\n",
    "            # Get the key (filename) of the object\n",
    "            key = obj['Key']\n",
    "            \n",
    "            # Check if the object is a .gpkg file\n",
    "            if key.endswith('.gpkg'):\n",
    "                # Download the GeoPackage file into memory\n",
    "                gpkg_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                gpkg_data = io.BytesIO(gpkg_object['Body'].read())\n",
    "                \n",
    "                # Save the GeoPackage file locally\n",
    "                gpkg_filename = os.path.basename(key)\n",
    "                with open(gpkg_filename, 'wb') as gpkg_file:\n",
    "                    gpkg_file.write(gpkg_data.getvalue())\n",
    "                \n",
    "                print(f\"Saved GeoPackage as '{gpkg_filename}' locally\")\n",
    "                # You can now use the saved file for further processing\n",
    "    else:\n",
    "        print(\"No objects found in the specified directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull csv from aws\n",
    "bucket_name = 'ca-climate-index'\n",
    "aws_dir = '2b_reproject/built_environment/communication_infrastructure/homeland_infrastructure_foundation_level_data/'\n",
    "\n",
    "pull_gpkg_from_directory(bucket_name, aws_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellular_towers_data = gpd.read_file('built_hifld_cellular_towers.gpkg')\n",
    "print('complete')\n",
    "microwave_towers_data = gpd.read_file('built_hifld_microwave_towers.gpkg')\n",
    "print('complete')\n",
    "mobile_towers_data = gpd.read_file('built_hifld_mobile_towers.gpkg')\n",
    "print('complete')\n",
    "paging_towers_data = gpd.read_file('built_hifld_paging_towers.gpkg')\n",
    "print('complete')\n",
    "radio_towers_data = gpd.read_file('built_hifld_radio_towers.gpkg')\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microwave_towers_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def county_count(df, county_col, county, counter):\n",
    "    county_isolate = df[df[county_col]==county]\n",
    "    county_isolate_drop_duplicates= county_isolate.drop_duplicates(subset=[county_col, counter])\n",
    "    print(f'Length of df for {county} county without dropping duplicates:  {len(county_isolate)}')\n",
    "    print(f'Length of df for {county} county after dropping duplicates: {len(county_isolate_drop_duplicates)}')\n",
    "\n",
    "county_count(microwave_towers_data, 'LocCounty', 'SANTA CLARA', 'Callsign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fresno = paging_towers_data[paging_towers_data['LocCounty']=='FRESNO']\n",
    "#Fresno = Fresno[Fresno['cellular_towers_data_count'].notna()]\n",
    "#pd.set_option('display.max_rows', None)  # None means display all rows\n",
    "iso_fresno = Fresno.drop_duplicates(subset=['LocCounty', 'UniqSysID'])\n",
    "\n",
    "iso_fresno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigger file, running later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv_contour_data = gpd.read_file('built_hifld_tv_contour.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CA census tiger file\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "\n",
    "# Select columns and rename\n",
    "filtered_ca_boundaries = ca_boundaries[['GEOID', 'geometry']].copy()\n",
    "filtered_ca_boundaries.rename(columns={'GEOID': 'tract'}, inplace=True)\n",
    "\n",
    "# Modify 'tract' column\n",
    "filtered_ca_boundaries['tract'] = filtered_ca_boundaries['tract'].str[1:]\n",
    "filtered_ca_boundaries = filtered_ca_boundaries.to_crs(crs=4269) \n",
    "# Output the modified GeoDataFrame\n",
    "filtered_ca_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication_infrastructure_data = [cellular_towers_data, microwave_towers_data, mobile_towers_data, paging_towers_data, radio_towers_data]\n",
    "words_to_search = ['OBJECTID','UniqSysID', 'USCB_COUNTYFP', 'Licensee', 'CALLSIGN', 'LocCounty', 'CITY', 'AllStruc', 'StrucType', 'LicStatus', 'STATUS', 'FREQUENCY', 'geometry']\n",
    "\n",
    "def filter_and_spatial_join(data_list, filtered_ca_boundaries, words_to_search, ca_tract_county):\n",
    "    county_count_dfs = {}\n",
    "    \n",
    "    for df, df_name in zip(data_list, ['cellular_towers_data', 'microwave_towers_data', 'mobile_towers_data', 'paging_towers_data', 'radio_towers_data']):        \n",
    "        # Filter columns based on words_to_search\n",
    "        filtered_df = df[[col for col in df.columns if any(word in col for word in words_to_search)]].copy()\n",
    "        filtered_df.index = df.index\n",
    "        filtered_df = filtered_df.to_crs(crs=4269)\n",
    "\n",
    "        # Convert all string columns to lowercase\n",
    "        str_columns = filtered_df.select_dtypes(include=['object']).columns\n",
    "        for col in str_columns:\n",
    "            filtered_df[col] = filtered_df[col].str.lower()\n",
    "\n",
    "        # Perform the spatial join\n",
    "        joined_df = gpd.sjoin(filtered_df, filtered_ca_boundaries, how='right', predicate='within')\n",
    "        \n",
    "        # Ensure necessary columns are retained\n",
    "        necessary_columns = ['tract', 'UniqSysID', 'OBJECTID', 'LocCounty', 'USCB_COUNTYFP']\n",
    "        joined_df = joined_df[[col for col in necessary_columns if col in joined_df.columns]].copy()\n",
    "                \n",
    "        # Use 'UniqSysID' if it exists, otherwise use 'OBJECTID'\n",
    "        if 'UniqSysID' in joined_df.columns:\n",
    "            id_column = 'UniqSysID'\n",
    "        elif 'OBJECTID' in joined_df.columns:\n",
    "            id_column = 'OBJECTID'\n",
    "        else:\n",
    "            raise ValueError(f\"Neither 'UniqSysID' nor 'OBJECTID' found in the DataFrame for {df_name}\")\n",
    "        \n",
    "        # Determine county_id\n",
    "        if 'LocCounty' in joined_df.columns:\n",
    "            joined_df = joined_df.rename(columns={'LocCounty':'county'})\n",
    "            county_id = 'county'\n",
    "        elif 'USCB_COUNTYFP' in joined_df.columns:\n",
    "            joined_df = joined_df.rename(columns={'USCB_COUNTYFP':'countyfp'})\n",
    "            county_id = 'countyfp'\n",
    "        else:\n",
    "            raise ValueError(f\"Neither 'LocCounty' nor 'USCB_COUNTYFP' found in the DataFrame for {df_name}\")\n",
    "\n",
    "        # Remove duplicates based on county and the chosen ID column\n",
    "        unique_communication_structures_county = joined_df.drop_duplicates(subset=[county_id, id_column])\n",
    "\n",
    "        county_power_counts = unique_communication_structures_county.groupby(county_id)[id_column].apply(lambda x: x.notnull().sum()).reset_index(name=f\"{df_name}_count\")\n",
    "        \n",
    "        # Merge with ca_tract_county\n",
    "        merged_df = pd.merge(ca_tract_county, county_power_counts, on=county_id, how='left')\n",
    "        \n",
    "        county_df_name = f\"county_count_{df_name}\"\n",
    "        \n",
    "        county_count_dfs[county_df_name] = merged_df\n",
    "        \n",
    "        # Dynamically create global variables\n",
    "        globals()[county_df_name] = merged_df\n",
    "        \n",
    "        print(county_df_name)\n",
    "    \n",
    "    return county_count_dfs\n",
    "\n",
    "county_count_dfs = filter_and_spatial_join(communication_infrastructure_data, filtered_ca_boundaries, words_to_search, ca_tract_county)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_count_microwave_towers_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication_infrastructure_data = [county_count_cellular_towers_data\n",
    "                                    county_count_microwave_towers_data\n",
    "                                    county_count_mobile_towers_data\n",
    "                                    county_count_paging_towers_data\n",
    "                                    county_count_radio_towers_data]\n",
    "\n",
    "def merge_dfs(data_list, filtered_ca_boundaries, words_to_search):\n",
    "    county_count_merged_dfs = {}\n",
    "    \n",
    "    for df, df_name in zip(data_list, ['cellular_towers_data', 'microwave_towers_data', 'mobile_towers_data', 'paging_towers_data', 'radio_towers_data']):        \n",
    "        # Filter columns based on words_to_search\n",
    "        filtered_df = df[[col for col in df.columns if any(word in col for word in words_to_search)]].copy()\n",
    "        filtered_df.index = df.index\n",
    "        filtered_df = filtered_df.to_crs(crs=4269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.merge(ca_tract_county,county_count_paging_towers_data, on='county', how='left')\n",
    "print(len(merge))\n",
    "merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_count_paging_towers_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
