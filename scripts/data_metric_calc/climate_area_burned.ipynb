{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook calculates the following metrics sourced from Cal-Adapt Tools: Wildfire\n",
    "* Change in absolute tract area burned in m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:46:45.693834Z",
     "iopub.status.busy": "2024-08-23T19:46:45.692909Z",
     "iopub.status.idle": "2024-08-23T19:46:46.833134Z",
     "shell.execute_reply": "2024-08-23T19:46:46.832537Z",
     "shell.execute_reply.started": "2024-08-23T19:46:45.693770Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import cftime\n",
    "\n",
    "import pyproj\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# projection information\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# suppress pandas purely educational warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import upload_csv_aws, filter_counties\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:46:46.835268Z",
     "iopub.status.busy": "2024-08-23T19:46:46.834885Z",
     "iopub.status.idle": "2024-08-23T19:46:46.839702Z",
     "shell.execute_reply": "2024-08-23T19:46:46.839212Z",
     "shell.execute_reply.started": "2024-08-23T19:46:46.835255Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_fire_data(fn):\n",
    "    bucket_loc = \"s3://ca-climate-index/1_pull_data/climate_risk/wildfire/loss/projections/caladapt/\"\n",
    "    filepath = bucket_loc + fn + \"_AA.monthly_all.bau.mu.nc.zarr\"\n",
    "    print('Opening: {}'.format(filepath))\n",
    "    ds = xr.open_dataset(\n",
    "        filepath, \n",
    "        engine=\"zarr\",\n",
    "        consolidated=False,\n",
    "        decode_times=False\n",
    "    )    \n",
    "    units, refdate = ds.time.attrs['units'].split('since')\n",
    "    ds['time'] = pd.date_range(start=refdate, periods=ds.sizes['time'], freq='MS')\n",
    "\n",
    "    # make mask to re-nan out grid cells with no data,\n",
    "    # since summing per year ends up turning the nans to 0\n",
    "    mask_layer = ds.isel(time=0).squeeze()\n",
    "\n",
    "    # calculate annual area burned\n",
    "    print('Calculating annual sum of area burned...')\n",
    "    ds = ds.resample(time='1Y').sum()\n",
    "\n",
    "    # mask out grid cells which originally had nans\n",
    "    ds = xr.where(np.isnan(mask_layer), x=np.nan, y=ds)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def convert_30yr(start):\n",
    "    return (start-5, start+24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:46:46.840584Z",
     "iopub.status.busy": "2024-08-23T19:46:46.840399Z",
     "iopub.status.idle": "2024-08-23T20:09:45.312210Z",
     "shell.execute_reply": "2024-08-23T20:09:45.311565Z",
     "shell.execute_reply.started": "2024-08-23T19:46:46.840570Z"
    }
   },
   "outputs": [],
   "source": [
    "# this takes a few minutes because it is opening the file and then pre-processing by calculating the annual sum\n",
    "# takes approx 25 min total\n",
    "# RCP 4.5\n",
    "miroc45 = process_fire_data(fn='MIROC5_45')\n",
    "cnrm45 = process_fire_data(fn='CNRM-CM5_45')\n",
    "hadgem45 = process_fire_data(fn='HadGEM2-ES_45')\n",
    "canesm45 = process_fire_data(fn='CanESM2_45')\n",
    "\n",
    "# RCP 8.5\n",
    "miroc85 = process_fire_data(fn='MIROC5_85')\n",
    "cnrm85 = process_fire_data(fn='CNRM-CM5_85')\n",
    "hadgem85 = process_fire_data(fn='HadGEM2-ES_85')\n",
    "canesm85 = process_fire_data(fn='CanESM2_85')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1a) Calculate historical baseline (1981-2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:09:45.314136Z",
     "iopub.status.busy": "2024-08-23T20:09:45.313744Z",
     "iopub.status.idle": "2024-08-23T20:09:45.397989Z",
     "shell.execute_reply": "2024-08-23T20:09:45.397493Z",
     "shell.execute_reply.started": "2024-08-23T20:09:45.314122Z"
    }
   },
   "outputs": [],
   "source": [
    "# grab historical period in each, and take multimodel mean\n",
    "all_sims = [miroc45, miroc85, cnrm45, cnrm85, hadgem45, hadgem85, canesm45, canesm85]\n",
    "\n",
    "ds_hist = xr.concat(all_sims, 'simulation')\n",
    "ds_hist = ds_hist.sel(time=slice('1981', '2010')).mean(dim='time').mean(dim='simulation') # subset for historical baseline period, and take multi-model mean\n",
    "ds_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick validation of the data before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:09:45.398772Z",
     "iopub.status.busy": "2024-08-23T20:09:45.398672Z",
     "iopub.status.idle": "2024-08-23T20:09:45.401972Z",
     "shell.execute_reply": "2024-08-23T20:09:45.401624Z",
     "shell.execute_reply.started": "2024-08-23T20:09:45.398762Z"
    }
   },
   "outputs": [],
   "source": [
    "# sum total area to compare against historical observations\n",
    "# convert to acres for easy comparison \n",
    "acre_per_hectare = 2.47105\n",
    "ds_hist_sum = ds_hist.sum()*acre_per_hectare\n",
    "print(f\"Historical model-mean total burned area in CA is {ds_hist_sum.hectares.values} acres.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wikipedia article on CA wildfires says](https://en.wikipedia.org/wiki/List_of_California_wildfires): \"...since 2000, the area that burned annually has ranged between 90,000 acres... and 1,590,000 acres...\", so an average annual burn of ~452,000 acres is reasonable especially when considering the earlier time period considered in the historical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1b) Calculate warming level per model\n",
    "This will have to be manually done per model\n",
    "* https://github.com/mathause/cmip_warming_levels/blob/main/warming_levels/cmip5_all_ens/csv/cmip5_warming_levels_all_ens_1850_1900_no_bounds_check.csv\n",
    "* Assuming all models are r1i1ip1f1\n",
    "* The csv file at loc above has typos in it, making it unreadable programmatically -- working manually\n",
    "   * uses a 20 year average around start year\n",
    "   * MIROC5 4.5 -- 2063-2082, MIROC5 8.5 -- 2039-2058\n",
    "   * CNRM-CM5 4.5 -- 2049-2068, CNRM-CM5 8.5 -- 2036-2055\n",
    "   * HADGEM-ES 4.5 -- 2034-2053, HADGEM-ES 8.5 -- 2026-2045\n",
    "   * CANESM2 4.5 -- 2022-2041, CANESM2 8.5 -- 2017-2036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:09:45.402506Z",
     "iopub.status.busy": "2024-08-23T20:09:45.402414Z",
     "iopub.status.idle": "2024-08-23T20:09:45.405257Z",
     "shell.execute_reply": "2024-08-23T20:09:45.404943Z",
     "shell.execute_reply.started": "2024-08-23T20:09:45.402497Z"
    }
   },
   "outputs": [],
   "source": [
    "sim_yrs = {\n",
    "    'miroc45'  : 2063,\n",
    "    'miroc85'  : 2039,\n",
    "    'cnrm45'   : 2049,\n",
    "    'cnrm85'   : 2036,\n",
    "    'hadgem45' : 2034,\n",
    "    'hadgem85' : 2026,\n",
    "    'canesm45' : 2022,\n",
    "    'canesm85' : 2017\n",
    "}\n",
    "\n",
    "for i in sim_yrs.keys():\n",
    "    print(i, '20-yr-start: ', sim_yrs.get(i), '30-yr range:', convert_30yr(sim_yrs.get(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:09:45.405942Z",
     "iopub.status.busy": "2024-08-23T20:09:45.405842Z",
     "iopub.status.idle": "2024-08-23T20:09:45.433787Z",
     "shell.execute_reply": "2024-08-23T20:09:45.433373Z",
     "shell.execute_reply.started": "2024-08-23T20:09:45.405933Z"
    }
   },
   "outputs": [],
   "source": [
    "# RCP 4.5\n",
    "miroc45_wl = miroc45.sel(time=slice('2058', '2087')).mean(dim='time')\n",
    "cnrm45_wl = cnrm45.sel(time=slice('2044', '2073')).mean(dim='time')\n",
    "hadgem45_wl = hadgem45.sel(time=slice('2029', '2058')).mean(dim='time')\n",
    "canesm45_wl = canesm45.sel(time=slice('2017', '2046')).mean(dim='time')\n",
    "\n",
    "# RCP 8.5\n",
    "miroc85_wl = miroc85.sel(time=slice('2034', '2063')).mean(dim='time')\n",
    "cnrm85_wl = cnrm85.sel(time=slice('2031', '2060')).mean(dim='time')\n",
    "hadgem85_wl = hadgem85.sel(time=slice('2021', '2050')).mean(dim='time')\n",
    "canesm85_wl = canesm85.sel(time=slice('2012', '2041')).mean(dim='time')\n",
    "\n",
    "projs = [miroc45_wl, miroc85_wl, cnrm45_wl, cnrm85_wl, hadgem45_wl, hadgem85_wl, canesm45_wl, canesm85_wl]\n",
    "ds_proj = xr.concat(projs,'simulation')\n",
    "ds_proj = ds_proj.mean(dim='simulation')\n",
    "ds_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:09:45.435453Z",
     "iopub.status.busy": "2024-08-23T20:09:45.435313Z",
     "iopub.status.idle": "2024-08-23T20:09:45.437814Z",
     "shell.execute_reply": "2024-08-23T20:09:45.437507Z",
     "shell.execute_reply.started": "2024-08-23T20:09:45.435442Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert from hectares to m2\n",
    "ds_proj_m2 = ds_proj.hectares * 10000\n",
    "ds_hist_m2 = ds_hist.hectares * 10000\n",
    "ds_proj_m2.name = 'burn_area_m2'\n",
    "ds_hist_m2.name = 'burn_area_m2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:09:45.438801Z",
     "iopub.status.busy": "2024-08-23T20:09:45.438521Z",
     "iopub.status.idle": "2024-08-23T20:09:45.441254Z",
     "shell.execute_reply": "2024-08-23T20:09:45.440910Z",
     "shell.execute_reply.started": "2024-08-23T20:09:45.438787Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate delta signal\n",
    "ds_delta = ds_proj_m2 - ds_hist_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:09:45.441991Z",
     "iopub.status.busy": "2024-08-23T20:09:45.441829Z",
     "iopub.status.idle": "2024-08-23T20:09:45.445434Z",
     "shell.execute_reply": "2024-08-23T20:09:45.445106Z",
     "shell.execute_reply.started": "2024-08-23T20:09:45.441980Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_delta.min(), ds_delta.max(), ds_delta.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:11:36.061060Z",
     "iopub.status.busy": "2024-08-23T20:11:36.060222Z",
     "iopub.status.idle": "2024-08-23T20:11:48.137632Z",
     "shell.execute_reply": "2024-08-23T20:11:48.137211Z",
     "shell.execute_reply.started": "2024-08-23T20:11:36.061015Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in CA census tiger file -- not working from s3 link, uploading manually to keep testing\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "\n",
    "column_names = ca_boundaries.columns\n",
    "ca_boundaries = ca_boundaries.to_crs(crs=3857) \n",
    "ca_boundaries = ca_boundaries.set_index(['GEOID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:11:48.138750Z",
     "iopub.status.busy": "2024-08-23T20:11:48.138486Z",
     "iopub.status.idle": "2024-08-23T20:11:48.345784Z",
     "shell.execute_reply": "2024-08-23T20:11:48.345457Z",
     "shell.execute_reply.started": "2024-08-23T20:11:48.138740Z"
    }
   },
   "outputs": [],
   "source": [
    "df_delta = ds_delta.to_dataframe().reset_index()\n",
    "\n",
    "gdf_delta = gpd.GeoDataFrame(\n",
    "    df_delta, geometry=gpd.points_from_xy(df_delta.lon, df_delta.lat), crs=\"EPSG:4326\")\n",
    "gdf_delta = gdf_delta.to_crs(ca_boundaries.crs)\n",
    "\n",
    "# spatially join gridded data with the census tracts\n",
    "clipped_gdf = gpd.sjoin(ca_boundaries, gdf_delta, how='left', predicate='intersects')\n",
    "clipped_gdf = clipped_gdf[[\"geometry\",\"burn_area_m2\"]]\n",
    "clipped_gdf['tract_area'] = clipped_gdf.area\n",
    "clipped_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:12:10.478202Z",
     "iopub.status.busy": "2024-08-23T20:12:10.477427Z",
     "iopub.status.idle": "2024-08-23T20:12:10.524505Z",
     "shell.execute_reply": "2024-08-23T20:12:10.523920Z",
     "shell.execute_reply.started": "2024-08-23T20:12:10.478161Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_gdf = clipped_gdf[~np.isnan(clipped_gdf[\"burn_area_m2\"])]\n",
    "valid_gdf = valid_gdf.sort_values(by=['GEOID']).reset_index()\n",
    "valid_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:12:22.773263Z",
     "iopub.status.busy": "2024-08-23T20:12:22.772615Z",
     "iopub.status.idle": "2024-08-23T20:12:22.781233Z",
     "shell.execute_reply": "2024-08-23T20:12:22.780358Z",
     "shell.execute_reply.started": "2024-08-23T20:12:22.773228Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_gdf.burn_area_m2.min(), valid_gdf.burn_area_m2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:12:44.269538Z",
     "iopub.status.busy": "2024-08-23T20:12:44.268835Z",
     "iopub.status.idle": "2024-08-23T20:14:03.738710Z",
     "shell.execute_reply": "2024-08-23T20:14:03.738283Z",
     "shell.execute_reply.started": "2024-08-23T20:12:44.269504Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes about 1.5 min\n",
    "# now sum all valid grid cells contained within the tracts\n",
    "diss_gdf_valid = valid_gdf.reset_index().dissolve(by='GEOID', aggfunc='sum')\n",
    "display(diss_gdf_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:14:53.851696Z",
     "iopub.status.busy": "2024-08-23T20:14:53.850969Z",
     "iopub.status.idle": "2024-08-23T20:14:53.906501Z",
     "shell.execute_reply": "2024-08-23T20:14:53.906000Z",
     "shell.execute_reply.started": "2024-08-23T20:14:53.851660Z"
    }
   },
   "outputs": [],
   "source": [
    "# now make a new geodataframe with all GEOIDs and set hectares to nan\n",
    "gdf_to_fill = ca_boundaries[[\"geometry\"]]\n",
    "gdf_to_fill[\"burn_area_m2\"] = np.nan\n",
    "\n",
    "# last, fill nans with valid data where it exists\n",
    "filled_gdf = (diss_gdf_valid.combine_first(gdf_to_fill))\n",
    "filled_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:26:14.444700Z",
     "iopub.status.busy": "2024-08-23T20:26:14.443988Z",
     "iopub.status.idle": "2024-08-23T20:26:14.456622Z",
     "shell.execute_reply": "2024-08-23T20:26:14.455954Z",
     "shell.execute_reply.started": "2024-08-23T20:26:14.444666Z"
    }
   },
   "outputs": [],
   "source": [
    "filled_gdf.burn_area_m2.min(), filled_gdf.burn_area_m2.max(), filled_gdf.burn_area_m2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:28:08.669659Z",
     "iopub.status.busy": "2024-08-23T20:28:08.668430Z",
     "iopub.status.idle": "2024-08-23T20:28:09.068917Z",
     "shell.execute_reply": "2024-08-23T20:28:09.068487Z",
     "shell.execute_reply.started": "2024-08-23T20:28:08.669591Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap = 'bwr'\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "filled_gdf.plot(column='burn_area_m2', ax=ax, cmap=cmap, vmin=-100000, vmax=100000, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:28:20.562523Z",
     "iopub.status.busy": "2024-08-23T20:28:20.561399Z",
     "iopub.status.idle": "2024-08-23T20:28:20.603322Z",
     "shell.execute_reply": "2024-08-23T20:28:20.602795Z",
     "shell.execute_reply.started": "2024-08-23T20:28:20.562456Z"
    }
   },
   "outputs": [],
   "source": [
    "# double check on invalid census tracts remaining as nans\n",
    "check_valid_tracts = filled_gdf.loc[~filled_gdf.burn_area_m2.isnull()]\n",
    "check_valid_tracts\n",
    "\n",
    "# of tracts checks out -- we're good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Min-max standardization\n",
    "Using Cal-CRAI min-max standardization function, available in `utils.calculate_index.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:28:37.152944Z",
     "iopub.status.busy": "2024-08-23T20:28:37.152207Z",
     "iopub.status.idle": "2024-08-23T20:28:37.165512Z",
     "shell.execute_reply": "2024-08-23T20:28:37.164360Z",
     "shell.execute_reply.started": "2024-08-23T20:28:37.152901Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_max_standardize(df, cols_to_run_on):\n",
    "    '''\n",
    "    Calculates min and max values for specified columns, then calculates\n",
    "    min-max standardized values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input dataframe   \n",
    "    cols_to_run_on: list\n",
    "        List of columns to calculate min, max, and standardize\n",
    "    '''\n",
    "    for col in cols_to_run_on:\n",
    "        max_value = df[col].max()\n",
    "        min_value = df[col].min()\n",
    "\n",
    "        # Get min-max values, standardize, and add columns to df\n",
    "        prefix = col # Extracting the prefix from the column name\n",
    "        df[f'{prefix}_min'] = min_value\n",
    "        df[f'{prefix}_max'] = max_value\n",
    "        df[f'{prefix}_min_max_standardized'] = ((df[col] - min_value) / (max_value - min_value))\n",
    "        \n",
    "        # note to add checker to make sure new min_max column values arent < 0 >\n",
    "        df[f'{prefix}_min_max_standardized'].loc[df[f'{prefix}_min_max_standardized'] < 0] = 0\n",
    "        df[f'{prefix}_min_max_standardized'].loc[df[f'{prefix}_min_max_standardized'] > 1] = 1\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:28:39.551566Z",
     "iopub.status.busy": "2024-08-23T20:28:39.551034Z",
     "iopub.status.idle": "2024-08-23T20:28:39.956290Z",
     "shell.execute_reply": "2024-08-23T20:28:39.955732Z",
     "shell.execute_reply.started": "2024-08-23T20:28:39.551534Z"
    }
   },
   "outputs": [],
   "source": [
    "data_std = min_max_standardize(filled_gdf, cols_to_run_on=['burn_area_m2'])\n",
    "\n",
    "#one more quick visual\n",
    "cmap = 'bwr'\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "data_std.plot(column='burn_area_m2_min_max_standardized', cmap=\"Oranges\", legend=True, ax=ax)\n",
    "\n",
    "# drop geometry column\n",
    "data_std = data_std.drop(columns = ['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Export data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:28:53.519369Z",
     "iopub.status.busy": "2024-08-23T20:28:53.518149Z",
     "iopub.status.idle": "2024-08-23T20:28:53.566970Z",
     "shell.execute_reply": "2024-08-23T20:28:53.566407Z",
     "shell.execute_reply.started": "2024-08-23T20:28:53.519289Z"
    }
   },
   "outputs": [],
   "source": [
    "data_std.to_csv('climate_wildfire_burned_area_metric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix header output and move column order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the desired columns\n",
    "new_columns = ['GEOID', 'burn_area_m2_min_max_standardized', 'burn_area_m2']\n",
    "data_std_cleaned = pd.DataFrame(columns=new_columns)\n",
    "\n",
    "data_std_reset = data_std.reset_index()\n",
    "\n",
    "# Populate the new DataFrame with data from the existing one\n",
    "data_std_cleaned['GEOID'] = data_std.index  # Get GEOID from the index\n",
    "data_std_cleaned['burn_area_m2_min_max_standardized'] = data_std_reset['burn_area_m2_min_max_standardized']  # Get burn_area_m2_min_max_standardized column\n",
    "data_std_cleaned['burn_area_m2'] = data_std_reset['burn_area_m2']  # Get burn_area_m2 column\n",
    "\n",
    "# Check the new DataFrame\n",
    "data_std_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std_cleaned.to_csv('climate_wildfire_burned_area_metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std_cleaned = pd.read_csv('climate_wildfire_burned_area_metric.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:29:41.333924Z",
     "iopub.status.busy": "2024-08-23T20:29:41.333110Z",
     "iopub.status.idle": "2024-08-23T20:29:41.352766Z",
     "shell.execute_reply": "2024-08-23T20:29:41.351531Z",
     "shell.execute_reply.started": "2024-08-23T20:29:41.333886Z"
    }
   },
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def wildfire_burned_area_metadata(df, export=False, export_filename=None, varname=''):\n",
    "    '''\n",
    "    Transforms the raw data into the following baseline metrics:\n",
    "    * Change in % tract area burned\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    Data is natively modeled area burned\n",
    "    Uses CMIP5 warming level targets identified from: \n",
    "    https://github.com/mathause/cmip_warming_levels/blob/main/warming_levels/cmip5_all_ens/csv/cmip5_warming_levels_all_ens_1850_1900_no_bounds_check.csv\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Input data.\n",
    "    export: True/False boolean\n",
    "        False = will not upload resulting df containing CAL CRAI wildfire burn area metric to AWS\n",
    "        True = will upload resulting df containing CAL CRAI wildfire burn area metric to AWS\n",
    "    export_filename: string\n",
    "        name of csv file to be uploaded to AWS\n",
    "    varname: string\n",
    "        Final metric name, for metadata generation\n",
    "        \n",
    "    Script\n",
    "    ------\n",
    "    climate_area_burned.ipynb\n",
    "    '''\n",
    "    print('Data transformation: monthly # of hectares summed to annual counts per model.')\n",
    "          \n",
    "    # historical baseline\n",
    "    print(\"Data transformation: historical baseline data subsetted for 1981-2010, averaging across models.\")\n",
    "    \n",
    "    # calculate with 2°C WL\n",
    "    print('Data transformation: data subsetted for warming level of 2.0°C, by manually subsetting based on GWL for parent GCM, and calculating 30 year average, averaging across models.')\n",
    "\n",
    "    # calculate delta signal\n",
    "    print('Data transformation: data unit converted to m2 from hectares.')\n",
    "    print(\"Data transformation: delta signal calculated by taking difference between chronic (2.0°C) and historical baseline.\")\n",
    "\n",
    "    # reprojection to census tracts\n",
    "    print(\"Data transformation: data transformed from xarray dataset into pandas dataframe.\")\n",
    "    print(\"Data transformation: data reprojected from Lambert Conformal Conic CRS to CRS 3857.\")\n",
    "    print(\"Data transformation: data spatially joined with census tracts.\")\n",
    "    print(\"Data transformation: point based burn area summed within tracts with valid data (ie, all grid points contain measurements rather than nans)\")\n",
    "    print(\"Data transformation: tracts which originally contained any nan grid points are masked out with nan values.\")\n",
    "    print(\"Data transformation: valid data merged with masked data to create a complete geodataframe with all tracts.\")\n",
    "        \n",
    "    # min-max standardization\n",
    "    print(\"Data transformation: data min-max standardized with min_max_standardize function.\")\n",
    "    \n",
    "    if export == True:\n",
    "        bucket_name = 'ca-climate-index'\n",
    "        directory = '3_fair_data/index_data'\n",
    "        export_filename = [df]\n",
    "        upload_csv_aws(export_filename, bucket_name, directory)\n",
    "\n",
    "    if export == False:\n",
    "        print(f'{df} uplaoded to AWS.')\n",
    "\n",
    "    if os.path.exists(df):\n",
    "        os.remove(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T20:29:41.941023Z",
     "iopub.status.busy": "2024-08-23T20:29:41.940258Z",
     "iopub.status.idle": "2024-08-23T20:29:41.949291Z",
     "shell.execute_reply": "2024-08-23T20:29:41.948470Z",
     "shell.execute_reply.started": "2024-08-23T20:29:41.940985Z"
    }
   },
   "outputs": [],
   "source": [
    "wildfire_burned_area_metadata('climate_wildfire_burned_area_metric.csv', export=True, export_filename=None, varname='test') # varname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
