{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a184be96-f61f-4546-8ff2-1cb43042623b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T22:15:17.898163Z",
     "iopub.status.busy": "2024-01-04T22:15:17.897747Z",
     "iopub.status.idle": "2024-01-04T22:15:17.902224Z",
     "shell.execute_reply": "2024-01-04T22:15:17.901452Z",
     "shell.execute_reply.started": "2024-01-04T22:15:17.898091Z"
    }
   },
   "source": [
    "## Cal-CRAI metric: # of chill hours\n",
    "This notebook generates the text metadata files for the extreme heat exposure metric: `change in average number of seasonal chill hours` using Cal-Adapt: Analytics Engine data. Because the AE data represents 200+ GB of data, metrics were calculated with a cluster in a high performance computing environment (i.e. a pcluster). Please see the processing script `climate_ae_chill_hours.py` for full methodological process. \n",
    "\n",
    "### Step 1: Generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29011bef-5f0d-4ea5-a71a-a4f683978c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import upload_csv_aws\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b85836-b2c2-4205-b681-84a4bfa858fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = pd.read_csv('climate_heat_chill_hours_metric.csv') # make sure this is in the same folder!\n",
    "df_in # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb1d99-bef0-4a3e-aba3-e73817380801",
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def extreme_heat_chill_hours_process(df, export=False, export_filename=None, varname=''):\n",
    "    '''\n",
    "    Reduces the size of the initial daily raw temperature data in order to streamline compute time.\n",
    "    Transforms the raw data into the following baseline metrics:\n",
    "    * change in number of average annual seasonal chill hours\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    Metric is calculated with the \"45°F and under\" model for the cold season (NDJF).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Input data.\n",
    "    export: True/False boolean\n",
    "        False = will not upload resulting df containing CAL CRAI extreme heat metric to AWS\n",
    "        True = will upload resulting df containing CAL CRAI extreme heat metric to AWS\n",
    "    export_filename: string\n",
    "        name of csv file to be uploaded to AWS\n",
    "    varname: string\n",
    "        Final metric name, for metadata generation\n",
    "        \n",
    "    Script\n",
    "    ------\n",
    "    Metric calculation: climate_ae_chill_hours.py via pcluster run\n",
    "    Metadata generation: climate_ae_chill_hours_metadata.ipynb\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    Because the climate projections data is on the order of 2.4 TB in size, intermediary\n",
    "    processed files are not produced for each stage of the metric calculation. All processing\n",
    "    occurs in a single complete run in the notebook listed above.\n",
    "    '''\n",
    "        \n",
    "    # calculate chronic with 2°C WL\n",
    "    print('Data transformation: raw projections data retrieved for warming level of 2.0°C, by manually subsetting based on GWL for parent GCM and calculating 30 year average.')\n",
    "    print(\"Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\")\n",
    "    print(\"Data transformation: drop all singleton dimensions (scenario).\")\n",
    "    print(\"Data transformation: data subsetted for cold season (NDJF).\")\n",
    "    print(\"Data transformation: number of chill hours calculated by summing the number of hours per year below 45°F threshold.\")\n",
    "\n",
    "    # historical baseline\n",
    "    print(\"Data transformation: historical baseline data retrieved for 1981-2010, averaging across models.\")\n",
    "    print(\"Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\")\n",
    "    print(\"Data transformation: drop all singleton dimensions (scenario).\")\n",
    "    print(\"Data transformation: data subsetted for cold season (NDJF).\")\n",
    "    print(\"Data transformation: number of chill hours calculated by summing the number of hours per year below 45°F threshold.\")\n",
    "\n",
    "    # calculate delta signal\n",
    "    print(\"Data transformation: delta signal calculated by taking difference between chronic (2.0°C) and historical baseline.\")\n",
    "\n",
    "    # reprojection to census tracts\n",
    "    print(\"Data transformation: data transformed from xarray dataset into pandas dataframe.\")\n",
    "    print(\"Data transformation: data reprojected from Lambert Conformal Conic CRS to CRS 3857.\")\n",
    "        \n",
    "    # min-max standardization\n",
    "    print(\"Data transformation: data min-max standardized with min_max_standardize function.\")\n",
    "    \n",
    "    # export data as csv\n",
    "    if export == True:\n",
    "        bucket_name = 'ca-climate-index'\n",
    "        directory = '3_fair_data/index_data'\n",
    "        export_filename = [export_filename]\n",
    "        upload_csv_aws(export_filename, bucket_name, directory)\n",
    "        \n",
    "        # # Check if file exists before attempting to remove it\n",
    "        # if os.path.exists('climate_heat_chill_hours_metric.csv'):\n",
    "        #     os.remove('climate_heat_chill_hours_metric') # remove from local to clear up directory\n",
    "        \n",
    "        # if os.path.exists(export_filename[0]):\n",
    "        #     os.remove(export_filename[0])\n",
    "    \n",
    "    if export == False:\n",
    "        print(f'{export_filename} uploaded to AWS.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041159e-f4bb-43ef-bb02-c9536210122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_heat_chill_hours_process(df_in, export=False, export_filename ='climate_heat_chill_hours_metric.csv', varname='test') # varname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
