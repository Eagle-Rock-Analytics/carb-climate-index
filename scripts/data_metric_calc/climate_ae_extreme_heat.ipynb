{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da4e854-5d2f-48f7-8dec-a074b2019239",
   "metadata": {},
   "source": [
    "# Extreme Heat Day and Warm Night Likelihood\n",
    "\n",
    "This notebook briefly walks through how to calculate the extreme heat exposure metric `% of change in extreme heat day and warm night event likelihood` from Cal-Adapt: Analytics Engine data. This notebook may be expanded upon for inclusion in cae-notebooks in the future. \n",
    "\n",
    "### Step 0: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a261b8be-6c78-4ae2-b24f-0bfa4d770e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T17:26:41.591637Z",
     "iopub.status.busy": "2024-05-17T17:26:41.591244Z",
     "iopub.status.idle": "2024-05-17T17:26:41.596032Z",
     "shell.execute_reply": "2024-05-17T17:26:41.595182Z",
     "shell.execute_reply.started": "2024-05-17T17:26:41.591610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import climakitae as ck\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from xclim.indices import warm_night_frequency, hot_spell_frequency # extreme heat day and warm night function\n",
    "from climakitae.util.utils import convert_to_local_time\n",
    "\n",
    "## AWS CREDENTIALS ARE NECESSARY WHEN WORKING THROUGH AE HUB FOR CRI WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f024f41-9a44-41a3-ae53-6c276f94e1f2",
   "metadata": {},
   "source": [
    "### Step 1: Retrieve data\n",
    "\n",
    "For our purposes, we will need to retrieve the 3km spatial resolution data. In the panel that comes up with `selections.show()` make the following selections:\n",
    "- Model grid spacing: 3km\n",
    "- Timescale: hourly\n",
    "- Variable: Air temperature\n",
    "- Historical Data: Historical Climate\n",
    "- Projections Data: SSP3-7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a47e46b-8990-4467-afcc-4ae746ebe04f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T15:51:12.026917Z",
     "iopub.status.busy": "2024-05-17T15:51:12.026509Z",
     "iopub.status.idle": "2024-05-17T15:51:16.926590Z",
     "shell.execute_reply": "2024-05-17T15:51:16.925729Z",
     "shell.execute_reply.started": "2024-05-17T15:51:12.026892Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transformation: PULL STEP\n"
     ]
    }
   ],
   "source": [
    "# selections = ck.Select()\n",
    "\n",
    "# selections.area_average = 'No'\n",
    "# selections.timescale = 'hourly'\n",
    "# selections.variable = 'Air Temperature at 2m'\n",
    "# selections.area_subset = 'states'\n",
    "# selections.cached_area = ['CA']\n",
    "# selections.scenario_historical = ['Historical Climate']\n",
    "# selections.scenario_ssp = ['SSP3-7.0 -- Business as Usual']\n",
    "# selections.time_slice = (1980, 1982)\n",
    "# selections.resolution = '3 km'\n",
    "# selections.units = 'degC'\n",
    "\n",
    "ds = selections.retrieve()\n",
    "# ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cdf682-921e-4dfd-80ab-f232bfd69b50",
   "metadata": {},
   "source": [
    "### Step 2: Subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b45721da-da60-4282-8692-ee9f43b46987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T17:25:57.429758Z",
     "iopub.status.busy": "2024-05-17T17:25:57.429351Z",
     "iopub.status.idle": "2024-05-17T17:25:57.435930Z",
     "shell.execute_reply": "2024-05-17T17:25:57.435243Z",
     "shell.execute_reply.started": "2024-05-17T17:25:57.429729Z"
    }
   },
   "outputs": [],
   "source": [
    "def extreme_heat_ae_data_process(ds, varname):\n",
    "    '''\n",
    "    Reduces the size of the initial hourly raw temperature data in order to streamline compute time.\n",
    "    Transforms the raw data into the following baseline metrics:\n",
    "    * Warm night frequency\n",
    "    * Extreme heat day frequency\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    Raw data are subsetted for bias-corrected dynamically-downscaled models.\n",
    "    Model-subsetted data have num. of dimensions reduced (dropping any unnecessary dimensions).\n",
    "    Metric is aggregated using xclim.indices functionality corresponding to the varname.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xarray.Dataset\n",
    "        Input data.\n",
    "    varname: string\n",
    "        Final metric name.\n",
    "        \n",
    "    Script\n",
    "    ------\n",
    "    cri_extreme_heat.ipynb\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Because the climate projections data is on the order of 2.4 TB in size, intermediary\n",
    "    processed files are not produced for each stage of the metric calculation. All processing\n",
    "    occurs in a single complete run in the notebook listed above.\n",
    "    '''\n",
    "    \n",
    "    # subset for bias-corrected models\n",
    "    data_models = ['WRF_EC-Earth3_r1i1p1f1', 'WRF_MPI-ESM1-2-HR_r3i1p1f1','WRF_TaiESM1_r1i1p1f1', 'WRF_MIROC6_r1i1p1f1']\n",
    "    ds = ds.sel(simulation = data_models)\n",
    "    print(\"Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\") ## metadata transformation\n",
    "    ds = ds.squeeze()\n",
    "    print(\"Data transformation: drop all singleton dimensions (scenario).\") ## metadata transformation\n",
    "    \n",
    "    if varname == \"warm_night_freq\":\n",
    "        # warm_night_frequency requires daily minimum temperature\n",
    "        tas_min = ds.resample(time='1D').min()\n",
    "        # warm_night_frequency requires daily minimum temperature, threshold default is 22degC, freq annual\n",
    "        ds_processed = warm_night_frequency(tas_min) # using all other default options\n",
    "        print(\"Data transformation: daily minimum calculated from hourly data for input into xclim.indices.warm_night_frequency.\")\n",
    "        \n",
    "    elif varname == \"hot_spell_freq\":\n",
    "        # hot_spell_frequency requires daily maximum temperature\n",
    "        tas_max = ds.resample(time='1D').max()\n",
    "        # hot_spell_frequency requires daily max temperature, threshold default is 30degC, window 3 days, freq annual\n",
    "        ds_processed = hot_spell_frequency(tas_max) # using all other default options\n",
    "        print(\"Data transformation: daily maximum calculated from hourly data for input into xclim.indices.hot_spell_frequency.\") # metadata transformation\n",
    "\n",
    "    return ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc0dce88-7245-4e7e-a015-13f3573e76c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T17:26:11.811725Z",
     "iopub.status.busy": "2024-05-17T17:26:11.811308Z",
     "iopub.status.idle": "2024-05-17T17:26:11.910728Z",
     "shell.execute_reply": "2024-05-17T17:26:11.909793Z",
     "shell.execute_reply.started": "2024-05-17T17:26:11.811697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\n",
      "Data transformation: drop all singleton dimensions (scenario).\n",
      "Data transformation: daily minimum calculated from hourly data for input into xclim.indices.warm_night_frequency.\n"
     ]
    }
   ],
   "source": [
    "processed_ds = extreme_heat_ae_data_process(ds, \"warm_night_freq\") # varname options are warm_night_freq, hot_spell_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70444b09-1fee-463a-aa52-a83ba09f6e82",
   "metadata": {},
   "source": [
    "### Step 3: Reproject data to census tract projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52c22c77-268e-40a4-81fd-f0b841a8026e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T17:33:34.874167Z",
     "iopub.status.busy": "2024-05-17T17:33:34.873795Z",
     "iopub.status.idle": "2024-05-17T17:33:35.702705Z",
     "shell.execute_reply": "2024-05-17T17:33:35.702059Z",
     "shell.execute_reply.started": "2024-05-17T17:33:34.874140Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in CA census tiger file -- not working from s3 link, uploading manually to keep testing\n",
    "# census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\"\n",
    "census_shp_dir = \"tl_2021_06_tract.shp\"\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)\n",
    "\n",
    "# # need to rename columns so we don't have any duplicates in the final geodatabase\n",
    "column_names = ca_boundaries.columns\n",
    "new_column_names = [\"USCB_\"+column for column in column_names if column != \"geometry\"]\n",
    "ca_boundaries = ca_boundaries.rename(columns=dict(zip(column_names, new_column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf236d-9790-4cd6-ad1c-2d6be505a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_ae_data(ds, ca_boundaries, varname=VARNAME, additional_comments='Data not exported to s3 bucket.'):\n",
    "    '''\n",
    "    Given a shapefule with California Census Tracts:\n",
    "    (1) Reproject the input data to the CRS of the California Census Tracts,\n",
    "    (2) Clip to California Census Tracts,\n",
    "    (3) Return reprojected dataset for metric calculation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xarray.Dataset\n",
    "        Input data.\n",
    "    ca_boundaries: shapefile\n",
    "        CA census tract boundary shapefile.\n",
    "    varname: string\n",
    "        Final metric name.\n",
    "    additional_comments: string\n",
    "        Additional notes for processing.\n",
    "        \n",
    "    Script\n",
    "    ------\n",
    "    cri_extreme_heat.ipynb\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Because the climate projections data is on the order of 2.4 TB in size, intermediary\n",
    "    processed files are not produced for each stage of the metric calculation. All processing\n",
    "    occurs in a single complete run in the notebook listed above.\n",
    "    '''\n",
    "    \n",
    "    # identify CRS of data\n",
    "    orig_crs = ds.attrs.grid_mapping ## not right, just placeholder for now\n",
    "    print(f\"Original CRS of data for {varname}: {orig_crs}\")\n",
    "    # cehck current coordinate system of the census tract data\n",
    "    print(f\"CRS of Census Tracts Shapefile: {ca_boundaries.crs}\")\n",
    "    \n",
    "    if (orig_crs==ca_boundaries.crs):   \n",
    "        print(f\"Do not need to reproject {varname} since it is already in the same projection as the Census Tracts Shapefile.\")\n",
    "    else:       \n",
    "        gdf = gdf.to_crs(ca_boundaries.crs) ## This is the big step here\n",
    "        print(f\"{varname} reprojected from {orig_crs} to {gdf.crs} with geopandas to_crs() function.\")\n",
    "\n",
    "    print(f\"Additional comments: {additional_comments}.\") # eg, code rerun, bug fix, etc\n",
    "    \n",
    "    return reprojected_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f764f5-8349-46c8-82ab-9631242d92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprojected_ds = reproject_ae_data(processed_ds, ca_boundaries, varname=VARNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc0cf3-6c44-4987-b77d-4f8e302d3800",
   "metadata": {},
   "source": [
    "### Step 4: Calculate metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0c20d8b-c704-4bcc-a647-658e3d983aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T17:42:18.499374Z",
     "iopub.status.busy": "2024-05-17T17:42:18.498977Z",
     "iopub.status.idle": "2024-05-17T17:42:18.502949Z",
     "shell.execute_reply": "2024-05-17T17:42:18.502155Z",
     "shell.execute_reply.started": "2024-05-17T17:42:18.499347Z"
    }
   },
   "outputs": [],
   "source": [
    "def clip_ae_data_to_tracts(ds):\n",
    "    '''\n",
    "    Ultimately need to aggregate gridded data to a single value per census tract\n",
    "    Input: gridded data (3km)\n",
    "    Returns: csv file one value per census tract\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e0e74-0c77-4070-9559-490a3078379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of change in extreme heat day and warm night event likelihood\n",
    "def extreme_heat_ae_data_metric_calc(ds, varname):\n",
    "    '''\n",
    "    \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xarray.Dataset\n",
    "        Input data.\n",
    "    varname: string\n",
    "        Final metric name.\n",
    "        \n",
    "    Script\n",
    "    ------\n",
    "    cri_extreme_heat.ipynb\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Because the climate projections data is on the order of 2.4 TB in size, intermediary\n",
    "    processed files are not produced for each stage of the metric calculation. All processing\n",
    "    occurs in a single complete run in the notebook listed above.\n",
    "    '''\n",
    "        \n",
    "    # historical baseline period 1980-2010 or whatever\n",
    "    \n",
    "    print(\"Data transformation: \") # metadata transformation\n",
    "\n",
    "    \n",
    "    # future period (WL?)\n",
    "    print(\"Data transformation: \") # metadata transformation\n",
    "\n",
    "    \n",
    "    # calculate % change of likelihood\n",
    "    print(\"Data transformation: % change of likelihood calculated.\") # metadata transformation\n",
    "    \n",
    "    \n",
    "    ## EXPORT STAGE\n",
    "    # export two files here per metric\n",
    "    # some sort of helper file across AE notebooks would be good\n",
    "    ### Idea is to copy all functions over from this notebook into a single script to run just the metadata pieces?\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0924f33a-c2fd-4ffe-9457-30830f3098a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_heat_ae_data_metric_calc(reprojected_ds, VARNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71176418-3d4f-4f88-9cee-822a19d7fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLOSE DATA TO SAVE MEMORY\n",
    "ds.close\n",
    "processed_ds.close()\n",
    "reprojected_ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
