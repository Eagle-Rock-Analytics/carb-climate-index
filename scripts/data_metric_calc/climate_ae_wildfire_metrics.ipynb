{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da4e854-5d2f-48f7-8dec-a074b2019239",
   "metadata": {},
   "source": [
    "## Fosberg Fire Weather Index\n",
    "\n",
    "This notebook briefly walks through how to calculate the wildfire metric `change in annual median # of days with FFWI greater than 50` from Cal-Adapt: Analytics Engine data. This notebook may be expanded upon for inclusion in cae-notebooks in the future. \n",
    "\n",
    "**Order of operations**:\n",
    "1. Read data in\n",
    "2. Calculate delta signal\n",
    "3. Reprojection to census tracts\n",
    "4. Min-max standardization\n",
    "5. Export data\n",
    "6. Generate metadata (via Cal-CRAI environment, not AE)\n",
    "\n",
    "### Step 0: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5a1aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:18:57.342288Z",
     "iopub.status.busy": "2024-09-26T16:18:57.341452Z",
     "iopub.status.idle": "2024-09-26T16:18:57.357151Z",
     "shell.execute_reply": "2024-09-26T16:18:57.354548Z",
     "shell.execute_reply.started": "2024-09-26T16:18:57.342244Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import pyproj\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "# projection information\n",
    "import cartopy.crs as ccrs\n",
    "crs = ccrs.LambertConformal(\n",
    "    central_longitude=-70, \n",
    "    central_latitude=38, \n",
    "    false_easting=0.0, \n",
    "    false_northing=0.0,  \n",
    "    standard_parallels=[30, 60], \n",
    "    globe=None,\n",
    ")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import upload_csv_aws, pull_nc_from_directory\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b06fac9-053f-4821-8c4d-8c18277eaae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:19:00.460473Z",
     "iopub.status.busy": "2024-09-26T16:19:00.459739Z",
     "iopub.status.idle": "2024-09-26T16:19:00.481215Z",
     "shell.execute_reply": "2024-09-26T16:19:00.480381Z",
     "shell.execute_reply.started": "2024-09-26T16:19:00.460437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reproject_to_tracts(ds_delta, ca_boundaries):\n",
    "    df = ds_delta.to_dataframe().reset_index()\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.x,df.y))\n",
    "    gdf = gdf.set_crs(crs)\n",
    "    gdf = gdf.to_crs(ca_boundaries.crs)\n",
    "    \n",
    "    ca_boundaries = ca_boundaries.set_index(['GEOID'])\n",
    "    \n",
    "    clipped_gdf = gpd.sjoin_nearest(ca_boundaries, gdf, how='left')\n",
    "    clipped_gdf = clipped_gdf.drop(['index_right'], axis=1)\n",
    "    clipped_gdf = clipped_gdf.reset_index()[\n",
    "        [\"GEOID\",f\"{ds_delta.name}\",\"geometry\"]]\n",
    "    ### some coastal tracts do not contain any land grid cells ###\n",
    "    ### due to the WRF's underlying surface type for a given grid cell. ###\n",
    "    \n",
    "    # aggregate the gridded data to the tract level\n",
    "    clipped_gdf_diss = clipped_gdf.reset_index().dissolve(\n",
    "        by='GEOID', aggfunc='mean')\n",
    "    clipped_gdf_diss = clipped_gdf_diss.rename(\n",
    "        columns={f\"{ds_delta.name}_right\":\n",
    "                 ds_delta.name}\n",
    "    )\n",
    "    \n",
    "    # separate tracts with data from tracts without data\n",
    "    clipped_gdf_nan = clipped_gdf_diss[np.isnan(\n",
    "        clipped_gdf_diss[ds_delta.name]\n",
    "    )]\n",
    "    clipped_gdf_nan = clipped_gdf_nan[[\"geometry\",ds_delta.name]]\n",
    "    clipped_gdf_valid = clipped_gdf_diss[~np.isnan(\n",
    "        clipped_gdf_diss[ds_delta.name]\n",
    "    )]\n",
    "    clipped_gdf_valid = clipped_gdf_valid[[\"geometry\",ds_delta.name]]\n",
    "\n",
    "    # compute the centroid of each tract\n",
    "    clipped_gdf_nan[\"centroid\"] = clipped_gdf_nan.centroid\n",
    "    clipped_gdf_nan = clipped_gdf_nan.set_geometry(\"centroid\")\n",
    "    clipped_gdf_valid[\"centroid\"] = clipped_gdf_valid.centroid\n",
    "    clipped_gdf_valid = clipped_gdf_valid.set_geometry(\"centroid\")\n",
    "    \n",
    "    # fill in missing tracts with values from the closest tract\n",
    "    # in terms of distance between the tract centroids\n",
    "    clipped_gdf_filled = clipped_gdf_nan.sjoin_nearest(clipped_gdf_valid, how='left')\n",
    "    clipped_gdf_filled = clipped_gdf_filled[[\"geometry_left\",f\"{ds_delta.name}_right\"]]\n",
    "    clipped_gdf_filled = clipped_gdf_filled.rename(columns={\n",
    "        \"geometry_left\":\"geometry\", f\"{ds_delta.name}_right\":ds_delta.name\n",
    "    })\n",
    "    clipped_gdf_valid = clipped_gdf_valid.drop(columns=\"centroid\")\n",
    " \n",
    "    # concatenate filled-in tracts with the original tract which had data\n",
    "    gdf_all_tracts = pd.concat([clipped_gdf_valid,clipped_gdf_filled])\n",
    "\n",
    "    return gdf_all_tracts\n",
    "\n",
    "def min_max_standardize(df, col):\n",
    "    '''\n",
    "    Calculates min and max values for specified columns, then calculates\n",
    "    min-max standardized values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input dataframe   \n",
    "    cols_to_run_on: list\n",
    "        List of columns to calculate min, max, and standardize\n",
    "    '''\n",
    "    max_value = df[col].max()\n",
    "    min_value = df[col].min()\n",
    "\n",
    "    # Get min-max values, standardize, and add columns to df\n",
    "    prefix = col # Extracting the prefix from the column name\n",
    "    df[f'{prefix}_min'] = min_value\n",
    "    df[f'{prefix}_max'] = max_value\n",
    "    df[f'{prefix}_min_max_standardized'] = ((df[col] - min_value) / (max_value - min_value))\n",
    "\n",
    "    # checker to make sure new min_max column values arent < 0 > 1\n",
    "    df[f'{prefix}_min_max_standardized'].loc[df[f'{prefix}_min_max_standardized'] < 0] = 0\n",
    "    df[f'{prefix}_min_max_standardized'].loc[df[f'{prefix}_min_max_standardized'] > 1] = 1\n",
    "\n",
    "    # Drop the original columns -- we want to keep as a check\n",
    "    # df = df.drop(columns=[col])\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4859bfc-b496-41bf-9172-0477233061d6",
   "metadata": {},
   "source": [
    "### Step 1: Read in intermediary files\n",
    "These files were generated via the `climate_ae_wildfire_intermediary_data_generation.ipynb` notebook on the Analytics Engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2acc94-980d-419c-857a-807f11491476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:20:11.804990Z",
     "iopub.status.busy": "2024-09-26T16:20:11.804145Z",
     "iopub.status.idle": "2024-09-26T16:20:13.063750Z",
     "shell.execute_reply": "2024-09-26T16:20:13.063390Z",
     "shell.execute_reply.started": "2024-09-26T16:20:11.804948Z"
    }
   },
   "outputs": [],
   "source": [
    "# historical baseline (1981-2010) data\n",
    "# download intermediary files\n",
    "pull_nc_from_directory(\"2a_subset/climate_risk/wildfire/exposure/ffwi_hist_9km.nc\", \"ffwi_hist_9km.nc\")\n",
    "\n",
    "# open files\n",
    "ds_hist = xr.open_dataset('ffwi_hist_9km.nc')\n",
    "ds_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afae43f-dd95-4c51-ac72-e44de74c2259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:21:00.789160Z",
     "iopub.status.busy": "2024-09-26T16:21:00.788330Z",
     "iopub.status.idle": "2024-09-26T16:21:04.668970Z",
     "shell.execute_reply": "2024-09-26T16:21:04.668382Z",
     "shell.execute_reply.started": "2024-09-26T16:21:00.789120Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2degC WL data -- each model is stored in a separate file\n",
    "# download intermediary files\n",
    "pull_nc_from_directory(\"2a_subset/climate_risk/wildfire/exposure/ffwi_proj_9km_ECEarth3.nc\", \"ffwi_proj_9km_ECEarth3.nc\")\n",
    "pull_nc_from_directory(\"2a_subset/climate_risk/wildfire/exposure/ffwi_proj_9km_MIROC.nc\", \"ffwi_proj_9km_MIROC.nc\")\n",
    "pull_nc_from_directory(\"2a_subset/climate_risk/wildfire/exposure/ffwi_proj_9km_MPI.nc\", \"ffwi_proj_9km_MPI.nc\")\n",
    "pull_nc_from_directory(\"2a_subset/climate_risk/wildfire/exposure/ffwi_proj_9km_TaiESM.nc\", \"ffwi_proj_9km_TaiESM.nc\")\n",
    "\n",
    "# open files\n",
    "ds_proj_sim1 = xr.open_dataset('ffwi_proj_9km_ECEarth3.nc')\n",
    "ds_proj_sim2 = xr.open_dataset('ffwi_proj_9km_MIROC.nc')\n",
    "ds_proj_sim3 = xr.open_dataset('ffwi_proj_9km_MPI.nc')\n",
    "ds_proj_sim4 = xr.open_dataset('ffwi_proj_9km_TaiESM.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab5583-94d0-4989-8889-b07a8ed5d4a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:21:07.323386Z",
     "iopub.status.busy": "2024-09-26T16:21:07.322553Z",
     "iopub.status.idle": "2024-09-26T16:21:07.371735Z",
     "shell.execute_reply": "2024-09-26T16:21:07.370932Z",
     "shell.execute_reply.started": "2024-09-26T16:21:07.323343Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge all 4 simulations into a single dataset on a new dimension \"simulation\"\n",
    "ds_proj = xr.concat([ds_proj_sim1, ds_proj_sim2, ds_proj_sim3, ds_proj_sim4], dim='simulation')\n",
    "ds_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa6fe2-709a-43fc-bc4c-07147304f85c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:21:10.350858Z",
     "iopub.status.busy": "2024-09-26T16:21:10.349927Z",
     "iopub.status.idle": "2024-09-26T16:21:10.380427Z",
     "shell.execute_reply": "2024-09-26T16:21:10.379845Z",
     "shell.execute_reply.started": "2024-09-26T16:21:10.350819Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduce and calculate metric\n",
    "ds_proj = ds_proj.mean(['simulation'])\n",
    "ds_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70936f5e-d9c1-4db0-b15a-bf43afe3b1ad",
   "metadata": {},
   "source": [
    "### Step 2: Calculate delta signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c179c5-86dd-45bb-a170-2e0b00654f5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:21:11.656490Z",
     "iopub.status.busy": "2024-09-26T16:21:11.655611Z",
     "iopub.status.idle": "2024-09-26T16:21:11.681019Z",
     "shell.execute_reply": "2024-09-26T16:21:11.679933Z",
     "shell.execute_reply.started": "2024-09-26T16:21:11.656456Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_delta = ds_proj - ds_hist\n",
    "ds_delta = ds_delta['Fosberg fire weather index'] \n",
    "ds_delta.name = 'change_ffwi_days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56f8ca-66e8-4f3c-b5dd-9724ed0e3b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:21:12.400015Z",
     "iopub.status.busy": "2024-09-26T16:21:12.399288Z",
     "iopub.status.idle": "2024-09-26T16:21:12.410485Z",
     "shell.execute_reply": "2024-09-26T16:21:12.409466Z",
     "shell.execute_reply.started": "2024-09-26T16:21:12.399978Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_delta.min().values, ds_delta.max().values, ds_delta.mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39796eba-6c05-4d7e-9166-778a8ddefbec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:21:13.138315Z",
     "iopub.status.busy": "2024-09-26T16:21:13.137520Z",
     "iopub.status.idle": "2024-09-26T16:21:13.589820Z",
     "shell.execute_reply": "2024-09-26T16:21:13.589345Z",
     "shell.execute_reply.started": "2024-09-26T16:21:13.138275Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_delta.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0705b-6177-43d2-b60b-b311e602d199",
   "metadata": {},
   "source": [
    "### Step 3: Reproject to census tract boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11f3539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in census tract shapefile\n",
    "census_shp_dir = \"s3://ca-climate-index/0_map_data/2021_tiger_census_tract/2021_ca_tract/\" # pulling from AWS\n",
    "# census_shp_dir = \"2021_ca_tract/tl_2021_06_tract.shp\" # local run, requires having census tracts loaded in file tree\n",
    "\n",
    "ca_boundaries = gpd.read_file(census_shp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a6fa5-1489-4777-954d-df6f197a9448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:21:14.646082Z",
     "iopub.status.busy": "2024-09-26T16:21:14.644593Z",
     "iopub.status.idle": "2024-09-26T16:22:04.266453Z",
     "shell.execute_reply": "2024-09-26T16:22:04.266087Z",
     "shell.execute_reply.started": "2024-09-26T16:21:14.646030Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to area-preserving CRS\n",
    "ca_boundaries = ca_boundaries.to_crs(crs=3310)\n",
    "ffwi_df = reproject_to_tracts(ds_delta, ca_boundaries)\n",
    "ffwi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fed91",
   "metadata": {},
   "source": [
    "## Fixing header output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7653f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the desired columns\n",
    "new_columns = ['GEOID', 'geometry', 'change_ffwi_days']\n",
    "ffwi_df_cleaned = pd.DataFrame(columns=new_columns)\n",
    "\n",
    "ffwi_df_reset = ffwi_df.reset_index()\n",
    "\n",
    "# Populate the new DataFrame with data from the existing one\n",
    "ffwi_df_cleaned['GEOID'] = ffwi_df.index  # Get GEOID from the index\n",
    "ffwi_df_cleaned['geometry'] = ffwi_df_reset['geometry']  # Get geometry column\n",
    "ffwi_df_cleaned['change_ffwi_days'] = ffwi_df_reset['change_ffwi_days']  # Get change_ffwi_days column\n",
    "\n",
    "# Check the new DataFrame\n",
    "ffwi_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9cd7e-dde0-4791-a5bf-561bc2253fff",
   "metadata": {},
   "source": [
    "### Step 4: Min-max standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a561a5a-5234-4d86-b944-bdb4c2159765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:22:04.270534Z",
     "iopub.status.busy": "2024-09-26T16:22:04.270434Z",
     "iopub.status.idle": "2024-09-26T16:22:04.273717Z",
     "shell.execute_reply": "2024-09-26T16:22:04.273445Z",
     "shell.execute_reply.started": "2024-09-26T16:22:04.270525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## min-max standardization\n",
    "data_std = min_max_standardize(ffwi_df_cleaned, col='change_ffwi_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550b658-cc92-4070-9f6f-cf8957da8728",
   "metadata": {},
   "source": [
    "### Step 5: Export final data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f9ed7c-6e69-4d50-a8ec-cdc2f8e74f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:22:04.274964Z",
     "iopub.status.busy": "2024-09-26T16:22:04.274834Z",
     "iopub.status.idle": "2024-09-26T16:22:04.311723Z",
     "shell.execute_reply": "2024-09-26T16:22:04.311290Z",
     "shell.execute_reply.started": "2024-09-26T16:22:04.274953Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data will be exported via pcluster run\n",
    "# clean up dataframes prior to export\n",
    "data_std = data_std.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a521b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move a specific column to the end of the DataFrame\n",
    "column_to_move = 'change_ffwi_days'  # Replace with the actual column name\n",
    "columns = [col for col in data_std.columns if col != column_to_move]  # Keep all other columns\n",
    "columns.append(column_to_move)  # Add the column to move to the end\n",
    "\n",
    "# Reassign the DataFrame with the new column order\n",
    "df_in = data_std[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "050aa90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "metric_fname = 'climate_wildfire_ffwi_metric.csv'\n",
    "df_in.to_csv(metric_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28962bc3-d6f3-4aa0-99b3-59ed997a63e1",
   "metadata": {},
   "source": [
    "### Step 6: Metadata generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a551e1e-bf26-4a49-a031-43bcfc7ccfae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T16:22:04.312500Z",
     "iopub.status.busy": "2024-09-26T16:22:04.312299Z",
     "iopub.status.idle": "2024-09-26T16:22:04.317079Z",
     "shell.execute_reply": "2024-09-26T16:22:04.316749Z",
     "shell.execute_reply.started": "2024-09-26T16:22:04.312478Z"
    }
   },
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def wildfire_ffwi_process(df, export=False, export_filename=None, varname=''):\n",
    "    '''\n",
    "    Reduces the size of the initial daily raw temperature data in order to streamline compute time.\n",
    "    Transforms the raw data into the following baseline metrics:\n",
    "    * change in median annual # of days with Fosberg Fire Weather index value >50\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    Metric is calculated with the FFWI threshold of 50 (indicating a moderate risk day).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Input data.\n",
    "    export: True/False boolean\n",
    "        False = will not upload resulting df containing CAL CRAI wildfire metric to AWS\n",
    "        True = will upload resulting df containing CAL CRAI wildfire metric to AWS\n",
    "    export_filename: string\n",
    "        name of csv file to be uploaded to AWS\n",
    "    varname: string\n",
    "        Final metric name, for metadata generation\n",
    "        \n",
    "    Script\n",
    "    ------\n",
    "    Intermediary file generation: climate_ae_wildfire_intermediary_data_generation.ipynb\n",
    "    Metric calculation: climate_ae_wildfire_metrics.ipynb\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Because the climate projections data is on the order of 2.4 TB in size, intermediary\n",
    "    processed files are not produced for each stage of the metric calculation. All processing\n",
    "    occurs in a single complete run in the notebook listed above.\n",
    "    '''\n",
    "         \n",
    "    # calculate chronic with 2°C WL\n",
    "    print('Data transformation: raw projections data retrieved for warming level of 2.0°C, by manually subsetting based on GWL for parent GCM and calculating 30 year average.')\n",
    "    print(\"Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\")\n",
    "    print(\"Data transformation: drop all singleton dimensions (scenario).\")\n",
    "\n",
    "    # calculate historical baseline\n",
    "    print(\"Data transformation: historical baseline data retrieved for 1981-2010, averaging across models.\")\n",
    "    print(\"Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\")\n",
    "    print(\"Data transformation: drop all singleton dimensions (scenario).\")\n",
    "    \n",
    "    # calculate delta signal       \n",
    "    print(\"Data transformation: number of median annual days calculated by summing the number of days per year above 50 threshold.\")\n",
    "    print(\"Data transformation: intermediary files generated of calculated metric data, due to size of input data.\")\n",
    "    print(\"Data transformation: delta signal calculated by taking difference between chronic (2.0°C) and historical baseline.\")\n",
    "\n",
    "    # reprojection to census tracts\n",
    "    print(\"Data transformation: data transformed from xarray dataset into pandas dataframe.\")\n",
    "    print(\"Data transformation: data reprojected from Lambert Conformal Conic CRS to CRS 3857.\")\n",
    "    print(\"Data transformation: data infilling for coastal census tracts by the average of nearest valid census tract via sjoin.nearest\") ## confirm\n",
    "        \n",
    "    # min-max standardization\n",
    "    print(\"Data transformation: data min-max standardized with min_max_standardize function.\")\n",
    "    \n",
    "    if export == True:\n",
    "        bucket_name = 'ca-climate-index'\n",
    "        directory = '3_fair_data/index_data'\n",
    "        export_filename = [df]\n",
    "        upload_csv_aws(export_filename, bucket_name, directory)\n",
    "\n",
    "    if export == False:\n",
    "        print(f'{df} uplaoded to AWS.')\n",
    "\n",
    "    if os.path.exists(df):\n",
    "        os.remove(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'climate_caladapt_wildfire_ffwi'\n",
    "filename = 'climate_wildfire_ffwi_metric.csv'\n",
    "wildfire_ffwi_process(filename, export=True, export_filename=None, varname='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
