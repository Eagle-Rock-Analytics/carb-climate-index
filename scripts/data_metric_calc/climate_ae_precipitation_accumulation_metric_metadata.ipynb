{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2aa917a-2085-43d9-8c6b-aabfbc167d4f",
   "metadata": {},
   "source": [
    "## Absolute change in 99th percentile 1-day accumulated precipitation\n",
    "This notebook generates the text metadata files for the in-land flooding exposure metric `absolute change in 99th percentile 1-day accumulated precipitation`, using data from Cal-Adapt: Analytics Engine data. \n",
    "Please see the processing notebook `climate_ae_precipitation_accumulation_metrics.ipynb` for full methodological process. Note this notebook can only be on the AE Jupyter Hub, or a computing environment with a large enough memory capacity (e.g., at least 30 GB).\n",
    "\n",
    "### Step 1: Generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166cf3f4-de4a-4edb-9c0b-6577997f679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import upload_csv_aws, pull_csv_from_directory\n",
    "from scripts.utils.write_metadata import append_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'ca-climate-index'\n",
    "aws_dir = '3_fair_data/index_data/climate_flood_exposure_precipitation_metric.csv'\n",
    "folder = 'csv_folder'\n",
    "\n",
    "pull_csv_from_directory(bucket_name, aws_dir, folder, search_zipped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149e67e-d33b-4a6e-9ad3-a522158fe310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options here: climate_extreme_heat_hot_day_metric, climate_extreme_heat_warm_night_metric\n",
    "df_in = pd.read_csv(r'csv_folder/climate_flood_exposure_precipitation_metric.csv') # make sure this is in the same folder!\n",
    "df_in # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad66c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move a specific column to the end of the DataFrame\n",
    "column_to_move = 'precip_99percentile'  # Replace with the actual column name\n",
    "columns = [col for col in df_in.columns if col != column_to_move]  # Keep all other columns\n",
    "columns.append(column_to_move)  # Add the column to move to the end\n",
    "\n",
    "# Reassign the DataFrame with the new column order\n",
    "df_in = df_in[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf9ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in.to_csv('climate_flood_exposure_precipitation_metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03699578-d4e7-48f4-be44-ac5799f63738",
   "metadata": {},
   "outputs": [],
   "source": [
    "@append_metadata\n",
    "def precip_ae_data_process(df, export=False, export_filename=None, varname=''):\n",
    "    '''\n",
    "    Reduces the size of the initial daily raw precipitation data in order to streamline compute time.\n",
    "    Transforms the raw data into the following baseline metrics:\n",
    "    * Absolute change in 99th percentile 1-day accumulated precipitation\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    Metric is calculated by pooling data across models and calculating the 99th percentile. \n",
    "    See https://github.com/cal-adapt/cae-notebooks/blob/main/exploratory/internal_variability.ipynb\n",
    "    for reasoning behind data pooling for precipitation model data. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Input data.\n",
    "    export: True/False boolean\n",
    "        False = will not upload resulting df containing CAL CRAI flooding metric to AWS\n",
    "        True = will upload resulting df containing CAL CRAI flooding metric to AWS\n",
    "    export_filename: string\n",
    "        name of csv file to be uploaded to AWS\n",
    "    varname: string\n",
    "        Final metric name, for metadata generation\n",
    "        \n",
    "    Script\n",
    "    ------\n",
    "    Metric calculation: climate_ae_precipitation_accumulation_metric.py via pcluster run\n",
    "    Example metric calculation for Alameda county: climate_ae_precipitation_accumulation_metric_example.ipynb\n",
    "    Metadata generation: climate_ae_precipitation_accumulation_metadata.ipynb\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Because the climate projections data is on the order of 2.4 TB in size, intermediary\n",
    "    processed files are not produced for each stage of the metric calculation. All processing\n",
    "    occurs in a single complete run in the notebook listed above.\n",
    "    '''\n",
    "        \n",
    "    # calculate with 2°C WL\n",
    "    print('Data transformation: raw projections data retrieved for warming level of 2.0°C, by manually subsetting based on GWL for parent GCM and calculating 30 year average.')\n",
    "    print(\"Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\")\n",
    "\n",
    "    # historical baseline\n",
    "    print(\"Data transformation: historical baseline data retrieved for 1981-2010, averaging across models.\")\n",
    "    print(\"Data transformation: dynamically-downscaled climate data subsetted for a-priori bias-corrected models.\")\n",
    "\n",
    "    # calculate delta signal\n",
    "    print(\"Data transformation: snowfall sigal removed from precipitation data to isolate liquid precipitation from total precipitation.\")\n",
    "    print(\"Data transformation: data clipped to remove 0.1mm to remove trace precipitation.\")\n",
    "    print(\"Data transformation: leap days removed from historical data to match time periods.\")\n",
    "    print(\"Data transformation: data pooled across models to increase sample size and drop all singleton dimensions (scenario).\")\n",
    "    print(\"Data transformation: calculate 99th percentile from pooled data.\")\n",
    "    print(\"Data transformation: delta signal calculated by taking difference between chronic (2.0°C) and historical baseline.\")\n",
    "    print(\"Data transformation: non-CA grid points removed from data.\")\n",
    "\n",
    "    # reprojection to census tracts\n",
    "    print(\"Data transformation: data transformed from xarray dataset into pandas dataframe.\")\n",
    "    print(\"Data transformation: data reprojected from Lambert Conformal Conic CRS to CRS 3310.\")\n",
    "        \n",
    "    # min-max standardization\n",
    "    print(\"Data transformation: data min-max standardized with min_max_standardize function.\")\n",
    "    \n",
    "    if export == True:\n",
    "        bucket_name = 'ca-climate-index'\n",
    "        directory = '3_fair_data/index_data'\n",
    "        export_filename = [df]\n",
    "        upload_csv_aws(export_filename, bucket_name, directory)\n",
    "\n",
    "    if export == False:\n",
    "        print(f'{df} uplaoded to AWS.')\n",
    "\n",
    "    if os.path.exists(df):\n",
    "        os.remove(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecb61bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'climate_caladapt_flood_exposure_precipitation'\n",
    "filename = 'climate_flood_exposure_precipitation_metric.csv'\n",
    "precip_ae_data_process(filename, export=True, export_filename=None, varname='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
