{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individually plot all 110 Cal-CRAI metric files\n",
    "Step 1) Pull, clean, and merge data\n",
    "* each metric is similarly processed as they are for each domain index\n",
    "    * grouped together by shared GEOID column\n",
    "    * cleaned for infinite values, obsolete island tract, NaN GEOID values\n",
    "\n",
    "Step 2) Process data\n",
    "* min-max standardize each metric column\n",
    "* perform vulnerability adjustment\n",
    "    * isolate 'vulnerable high' metrics\n",
    "    * divide 1 by their values so they are 'resilience high' (high values are more resilient)\n",
    "\n",
    "Step 3) Plot each metric\n",
    "* each plot has its full metric name as its title\n",
    "* 0-1 resilience scale, higher values are more resilient\n",
    "* consistent color scheme\n",
    "* all resulting .png files are stored in a single folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box\n",
    "import re  # For regex operations\n",
    "import textwrap\n",
    "\n",
    "# suppress pandas purely educational warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import pull_csv_from_directory\n",
    "from scripts.utils.calculate_index import (min_max_standardize, \n",
    "                                        add_census_tracts)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Pull, clean, and merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull all metric files\n",
    "bucket_name = 'ca-climate-index'\n",
    "aws_dir = '3_fair_data/index_data/'\n",
    "\n",
    "pull_csv_from_directory(bucket_name, aws_dir, output_folder='aws_csvs', search_zipped=False, print_name=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge & Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_csv_files(input_folder, output_folder, meta_csv, merged_output_file):\n",
    "    '''\n",
    "    Pulls all metric CSV files from the input folder, merges all together based on shared GEOID column. \n",
    "    NaN values within the GEOID column are removed, and infinite values (if any) in other columns are adjusted to NaN values.\n",
    "    Lastly, an uninhabited island tract is also given NaN metric values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_folder: str\n",
    "        Name of the folder that is storing all metric csv files\n",
    "    output_folder: str\n",
    "        Name of the folder to store pulled domain specific csv files.\n",
    "    meta_csv: str\n",
    "        Local path to the metadata pipeline.\n",
    "    merged_output_file: str\n",
    "        Desired name of merged output csv file.\n",
    "    '''\n",
    "\n",
    "    # Function to detect incremental columns\n",
    "    def is_incremental(series):\n",
    "        \"\"\"Check if a column is incremental (starts from 0 or 1 and increases by 1).\"\"\"\n",
    "        if series.dtype in [np.int64, np.float64]:  # Ensure numeric columns\n",
    "            diff = series.diff().dropna()  # Calculate the difference between consecutive values\n",
    "            return (diff == 1).all() and series.iloc[0] in [0, 1]\n",
    "        return False\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load the metadata CSV\n",
    "    df = pd.read_csv(meta_csv)\n",
    "\n",
    "    # Get the list of metric file names and corresponding 'Metric' and 'High value result (vulnerable or resilient)' entries\n",
    "    metric_files = df[['Metric file name', 'Metric', 'High value result (vulnerable or resilient)']]\n",
    "\n",
    "    # Dictionary to hold the metric and corresponding last column from the CSV files\n",
    "    metric_last_column_dict = {}\n",
    "\n",
    "    # Second dictionary for resilience and vulnerability grouping\n",
    "    resilience_vulnerability_dict = {'resilient': [], 'vulnerable': []}\n",
    "\n",
    "    # Find all CSV files and match to their corresponding metric file names\n",
    "    source_files = [file for file in glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "                    if os.path.basename(file) in metric_files['Metric file name'].values]\n",
    "\n",
    "    # Iterate through the source files and process them\n",
    "    for file in source_files:\n",
    "        # Get the 'Metric' entry for the current file\n",
    "        metric_info = metric_files.loc[metric_files['Metric file name'] == os.path.basename(file)]\n",
    "        metric_name = metric_info['Metric'].values[0]\n",
    "        high_value_result = metric_info['High value result (vulnerable or resilient)'].values[0]\n",
    "\n",
    "        # Load the CSV file\n",
    "        csv_df = pd.read_csv(file)\n",
    "\n",
    "        # Detect and remove incremental columns\n",
    "        incremental_cols = [col for col in csv_df.columns if is_incremental(csv_df[col])]\n",
    "        csv_df.drop(columns=incremental_cols, inplace=True)\n",
    "\n",
    "        # Get the last column name\n",
    "        last_column = csv_df.columns[-1]\n",
    "\n",
    "        # Add the metric name and last column name to the dictionary\n",
    "        metric_last_column_dict[metric_name] = last_column\n",
    "\n",
    "        # Add the metric name to the appropriate resilience or vulnerability group\n",
    "        if high_value_result.lower() == 'resilient':\n",
    "            resilience_vulnerability_dict['resilient'].append(metric_name)\n",
    "        elif high_value_result.lower() == 'vulnerable':\n",
    "            resilience_vulnerability_dict['vulnerable'].append(metric_name)\n",
    "\n",
    "        # Construct the destination file path\n",
    "        destination_path = os.path.join(output_folder, os.path.basename(file))\n",
    "\n",
    "        # Save the modified CSV to the output folder\n",
    "        csv_df.to_csv(destination_path, index=False)\n",
    "\n",
    "        # Remove the original file\n",
    "        os.remove(file)\n",
    "\n",
    "    print(f\"Processed and saved {len(source_files)} CSV files within all domains.\")\n",
    "    print('\\nMetric dictionary created and called: metric_last_column_dict')\n",
    "\n",
    "    # --- Additional Processing: Merging CSV Files ---\n",
    "\n",
    "    # Get a list of all CSV files in the output folder\n",
    "    csv_files = glob.glob(os.path.join(output_folder, '*.csv'))\n",
    "\n",
    "    # Initialize an empty DataFrame for merging\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through each CSV file and merge them on the 'GEOID' column\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Rename 'GEO_ID', 'tract', 'TRACT', 'Census_Tract', 'GEOID', 'USCB_GEOID' to 'GEOID' if they exist\n",
    "        rename_cols = ['GEO_ID', 'GEOID', 'tract', 'TRACT', 'Census_Tract', 'census_tract', 'USCB_GEOID', 'Unnamed: 0']\n",
    "        for col in rename_cols:\n",
    "            if col in df.columns:\n",
    "                df.rename(columns={col: 'GEOID'}, inplace=True)\n",
    "                break\n",
    "         \n",
    "        # Keep only the 'GEOID' and the last column from each file\n",
    "        last_column = df.columns[-1]\n",
    "        df = df[['GEOID', last_column]]\n",
    "        \n",
    "        # Merge the DataFrame with the existing merged DataFrame\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on='GEOID', how='outer')\n",
    "\n",
    "    # Drop rows where 'GEOID' is NaN\n",
    "    merged_df = merged_df.dropna(subset=['GEOID'])\n",
    "\n",
    "    # Convert census tract to string and eliminate scientific notation default\n",
    "    merged_df['GEOID'] = merged_df['GEOID'].dropna().apply(lambda x: '{:.0f}'.format(x))\n",
    "\n",
    "    # Convert all values within the island tract (near San Francisco) to NaN, as it is uninhabited \n",
    "    island_tract = '6075980401'\n",
    "    merged_df.loc[merged_df['GEOID'] == island_tract, merged_df.columns != 'GEOID'] = np.nan\n",
    "\n",
    "    # Check if all entries within the island tract are NaN\n",
    "    island_row = merged_df.loc[merged_df['GEOID'] == island_tract]\n",
    "    if island_row.iloc[:, 1:].isnull().all().all():\n",
    "        print(f\"\\nAll entries within the island tract ({island_tract}) are NaN.\")\n",
    "    else:\n",
    "        print(f\"\\nSome entries within the island tract ({island_tract}) are not NaN.\")\n",
    "\n",
    "    merged_df['GEOID'] = merged_df['GEOID'].apply(lambda x: '0' + str(x))\n",
    "    merged_df['GEOID'] = merged_df['GEOID'].astype(str).apply(lambda x: x.rstrip('0').rstrip('.') if '.' in x else x)\n",
    "\n",
    "    # Selecting only numeric columns\n",
    "    numeric_df = merged_df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Counting infinite values\n",
    "    num_infinite = np.isinf(numeric_df).sum().sum()\n",
    "\n",
    "    print(f\"\\nNumber of infinite entries in the DataFrame: {num_infinite}\")\n",
    "    print('Replacing infinite entries (if any) with NaN')\n",
    "\n",
    "    # Replace infinite values with NaN\n",
    "    merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Counting infinite values after replacement\n",
    "    num_infinite = np.isinf(numeric_df).sum().sum()\n",
    "    print(f\"Number of infinite entries in the DataFrame after replacement: {num_infinite}\")\n",
    "\n",
    "    print(f\"\\nFile processing complete, dataframe will now be saved as a .csv\")\n",
    "    \n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    merged_df.to_csv(merged_output_file, index=False)\n",
    "\n",
    "    print(f\"Processed CSV saved as {merged_output_file}\")\n",
    "    \n",
    "    return metric_last_column_dict, resilience_vulnerability_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull all domain csv metric files and process them\n",
    "input_folder = r'aws_csvs'\n",
    "output_folder = \"output_folder\"\n",
    "meta_csv = r'../utils/calcrai_metrics.csv'\n",
    "\n",
    "merged_output_file = 'all_domain_files.csv'\n",
    "metric_last_column_dict, resilience_vulnerability_dict = process_all_csv_files(input_folder, output_folder, meta_csv, merged_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = pd.read_csv('all_domain_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max standardize each metric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max standardize all non-GEOID columns in our df\n",
    "columns_to_process = [col for col in all_metrics.columns if col != 'GEOID']\n",
    "min_max_metrics = min_max_standardize(all_metrics, columns_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words to filter columns\n",
    "words = ['GEOID', 'standardized']\n",
    "\n",
    "# Initialize an empty list to hold the selected columns\n",
    "selected_columns = []\n",
    "\n",
    "# Iterate over the words\n",
    "for word in words:\n",
    "    # If the word is 'standardized', use a regular expression to match only when it appears at the end of a string\n",
    "    if word == 'standardized':\n",
    "        selected_columns.extend(min_max_metrics.columns[min_max_metrics.columns.str.contains(r'standardized$', regex=True)].tolist())\n",
    "    else:\n",
    "        # For other words, use a normal contains check\n",
    "        selected_columns.extend(min_max_metrics.columns[min_max_metrics.columns.str.contains(word)].tolist())\n",
    "\n",
    "# Create the filtered DataFrame with the selected columns\n",
    "min_max_standardized_all_metrics_df = min_max_metrics[selected_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_standardized_all_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'GEOID' is treated as a string before modifying\n",
    "min_max_standardized_all_metrics_df.loc[:, 'GEOID'] = min_max_standardized_all_metrics_df['GEOID'].astype(str)\n",
    "\n",
    "# Add leading zero to 'GEOID' and strip trailing zeros and decimal points\n",
    "min_max_standardized_all_metrics_df.loc[:, 'GEOID'] = min_max_standardized_all_metrics_df['GEOID'].apply(lambda x: '0' + str(x))\n",
    "min_max_standardized_all_metrics_df.loc[:, 'GEOID'] = min_max_standardized_all_metrics_df['GEOID'].astype(str).apply(lambda x: x.rstrip('0').rstrip('.') if '.' in x else x)\n",
    "min_max_standardized_all_metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resilience_vulnerability_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerability adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Vulnerable Columns\n",
    "vulnerable_metric_names = resilience_vulnerability_dict['vulnerable']\n",
    "\n",
    "# Get the actual column names corresponding to resilient metrics\n",
    "vulnerable_columns_in_df = [\n",
    "    f\"{metric_last_column_dict[metric_name]}_min_max_standardized\"\n",
    "    for metric_name in vulnerable_metric_names if metric_name in metric_last_column_dict\n",
    "]\n",
    "\n",
    "# Adjust the resilient columns in the original DataFrame\n",
    "adjusted_vulnerable_df = min_max_standardized_all_metrics_df.copy()\n",
    "\n",
    "for column_name in vulnerable_columns_in_df:\n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column_name in adjusted_vulnerable_df.columns:\n",
    "        # Subtract from 1 to adjust the values\n",
    "        adjusted_vulnerable_df[column_name] = 1 - adjusted_vulnerable_df[column_name]\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' not found in DataFrame for adjustment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_vulnerable_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename all columns to their metric name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename all metric columns\n",
    "rename_mapping_all = {\n",
    "    f\"{v}_min_max_standardized\": k \n",
    "    for k, v in metric_last_column_dict.items() \n",
    "    if f\"{v}_min_max_standardized\" in adjusted_vulnerable_df.columns\n",
    "}\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "adjusted_vulnerablility_renamed_df = adjusted_vulnerable_df.rename(columns=rename_mapping_all)\n",
    "adjusted_vulnerablility_renamed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the df and census tracts and convert the geometry to our uniformly used coordinate reference system (4269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = add_census_tracts(adjusted_vulnerablility_renamed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to plot metrics individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_region_individual_metrics(gdf, counties_to_plot=None, region=None, plot_all=False, savefig=False, font_color='black', domain='society_economy_', domain_label_map=None, output_folder='output_plots'):\n",
    "    \"\"\"\n",
    "    Plots a domain score vulnerability for selected counties or regions, with the option to exclude features within a bounding box.\n",
    "    \n",
    "    This version also iterates over columns in the GeoDataFrame that contain the word 'standardized' and saves each plot.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        A GeoDataFrame containing the data you want to plot, which must include the column 'GEOID' to match with the census tract data.\n",
    "    \n",
    "    counties_to_plot : list of str, optional\n",
    "        A list of county FIPS codes (as strings) to plot. If None, no counties will be plotted.\n",
    "        Example: ['037', '071', '065', '029', '111'].\n",
    "    \n",
    "    region : str, optional\n",
    "        A predefined region to plot. Options: 'bay_area', 'central_region', 'inland_deserts', 'north_central', 'northern', or 'south_coast'.\n",
    "        If specified, this will override `counties_to_plot`.\n",
    "    \n",
    "    plot_all : bool, optional\n",
    "        If True, plots all counties in California. Overrides `counties_to_plot` and `region`.\n",
    "    \n",
    "    savefig : bool, optional\n",
    "        If True, the plot will be saved as a PNG file. Default is False.\n",
    "\n",
    "    font_color : str, optional\n",
    "        Color of the font for county labels. Default is 'black'.\n",
    "\n",
    "    domain : str, optional\n",
    "        The domain name used for labeling and column names. Default is 'society_economy_'.\n",
    "\n",
    "    domain_label_map : dict, optional\n",
    "        A dictionary to map the domain variable to a more readable label. Example: {'society_economy_': 'Society and Economy Domain'}\n",
    "\n",
    "    output_folder : str, optional\n",
    "        The folder where the plot files should be saved. Default is 'output_plots'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Iterates through standardized columns and saves the plots in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a domain label map is provided, use it to get a readable title. Otherwise, create it from the domain string.\n",
    "    if domain_label_map:\n",
    "        domain_name = domain_label_map.get(domain, domain.replace('_', ' ').title())\n",
    "    else:\n",
    "        domain_name = domain.replace('_', ' ').title()\n",
    "\n",
    "    # Dictionary of county labels\n",
    "    county_labels = {\n",
    "        '001': 'Alameda', '003': 'Alpine', '005': 'Amador', '007': 'Butte', '009': 'Calaveras',\n",
    "        '011': 'Colusa', '013': 'Contra Costa', '015': 'Del Norte', '017': 'El Dorado', '019': 'Fresno',\n",
    "        '021': 'Glenn', '023': 'Humboldt', '025': 'Imperial', '027': 'Inyo', '029': 'Kern',\n",
    "        '031': 'Kings', '033': 'Lake', '035': 'Lassen', '037': 'Los Angeles', '039': 'Madera',\n",
    "        '041': 'Marin', '043': 'Mariposa', '045': 'Mendocino', '047': 'Merced', '049': 'Modoc',\n",
    "        '051': 'Mono', '053': 'Monterey', '055': 'Napa', '057': 'Nevada', '059': 'Orange',\n",
    "        '061': 'Placer', '063': 'Plumas', '065': 'Riverside', '067': 'Sacramento', '069': 'San Benito',\n",
    "        '071': 'San Bernardino', '073': 'San Diego', '075': 'San Francisco', '077': 'San Joaquin',\n",
    "        '079': 'San Luis Obispo', '081': 'San Mateo', '083': 'Santa Barbara', '085': 'Santa Clara',\n",
    "        '087': 'Santa Cruz', '089': 'Shasta', '091': 'Sierra', '093': 'Siskiyou', '095': 'Solano',\n",
    "        '097': 'Sonoma', '099': 'Stanislaus', '101': 'Sutter', '103': 'Tehama', '105': 'Trinity',\n",
    "        '107': 'Tulare', '109': 'Tuolumne', '111': 'Ventura', '113': 'Yolo', '115': 'Yuba'\n",
    "    }\n",
    "\n",
    "    # Define the new regional groups of counties\n",
    "    regions = {\n",
    "        'bay_area': ['001', '013', '041', '055', '081', '085', '087', '075', '095', '097'],\n",
    "        'central_region': ['019', '029', '031', '039', '043', '047', '053', '069', '079', '099', '107', '109'],\n",
    "        'inland_deserts': ['025', '027', '051', '065', '071'],\n",
    "        'north_central': ['067', '077', '017', '033', '057', '061', '091', '101', '063', '113', '115'],\n",
    "        'northern': ['015', '023', '035', '045', '049', '093', '089', '103', '105'],\n",
    "        'south_coast': ['037', '059', '073', '083', '111']\n",
    "    }\n",
    "\n",
    "    # Set counties_to_plot based on the specified region or plot_all flag\n",
    "    if plot_all:\n",
    "        counties_to_plot = list(county_labels.keys())\n",
    "        title_prefix = f'Resiliency Index of All Counties in California \\n'\n",
    "    elif region:\n",
    "        counties_to_plot = regions.get(region, [])\n",
    "        region_name = region.replace('_', ' ').title()  # Capitalize the region name for display\n",
    "        title_prefix = f'Resiliency Index of California\\'s {region_name} \\n'\n",
    "    else:\n",
    "        title_prefix = f'Resiliency Index of Selected Counties \\n'\n",
    "\n",
    "    # Convert to GeoDataFrame with the correct CRS if necessary\n",
    "    df2_filtered = gpd.GeoDataFrame(gdf, geometry='geometry', crs=4269)\n",
    "\n",
    "    # Define the bounding box to exclude (xmin, ymin, xmax, ymax)\n",
    "    exclusion_box = box(-122.8, 37.6, -123.2, 37.85) \n",
    "    \n",
    "    # Exclude features within the bounding box\n",
    "    df2_filtered = df2_filtered[~df2_filtered.intersects(exclusion_box)]\n",
    "\n",
    "    # Check for invalid geometries\n",
    "    invalid_geometries = df2_filtered[~df2_filtered['geometry'].is_valid]\n",
    "    print(\"Number of invalid geometries:\", len(invalid_geometries))\n",
    "\n",
    "    # Group by COUNTYFP and take the geometry of the first row in each group\n",
    "    county_boundaries = df2_filtered.dissolve(by='COUNTYFP')['geometry']\n",
    "\n",
    "    # Check if there are any valid geometries left after filtering\n",
    "    if len(county_boundaries) == 0:\n",
    "        print('No valid geometries. Cannot plot.')\n",
    "        return\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Define columns to exclude\n",
    "    excluded_columns = [\n",
    "        'GEOID', 'STATEFP', 'COUNTYFP', 'TRACTCE', \n",
    "        'NAME', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', \n",
    "        'ALAND', 'AWATER', 'INTPTLAT', 'INTPTLON', 'geometry'\n",
    "    ]\n",
    "\n",
    "    # Function to sanitize column names\n",
    "    def sanitize_column_name(column_name):\n",
    "        # Replace specific characters with desired words\n",
    "        column_name = column_name.replace('%', 'percent')\n",
    "        column_name = column_name.replace('#', 'number')\n",
    "        column_name = column_name.replace('<', 'less than')\n",
    "        column_name = column_name.replace('>', 'greater than')\n",
    "        # Replace other invalid characters with '_'\n",
    "        return re.sub(r'[<>:\"/\\\\|?*]', '_', column_name)\n",
    "\n",
    "    # Iterate over each column in the gdf that is not in the excluded list\n",
    "    for column in gdf.columns:\n",
    "        if column not in excluded_columns:\n",
    "            print(f\"Plotting for column: {column}\")\n",
    "\n",
    "            # Create the plot\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "            df2_filtered.plot(column=column, \n",
    "                              ax=ax, \n",
    "                              vmin=0, vmax=1, \n",
    "                              legend=True, \n",
    "                              cmap='Purples', \n",
    "                              missing_kwds={\n",
    "                                  \"color\": \"grey\",\n",
    "                                  \"label\" : \"Missing values\"\n",
    "                              },\n",
    "                              legend_kwds={'label': 'Resilience (larger values are more resilient)', \n",
    "                                           'orientation': 'horizontal', 'shrink': 0.5, 'pad': 0.05}\n",
    "                             )\n",
    "\n",
    "            # Plot county boundaries\n",
    "            county_boundaries.boundary.plot(ax=ax, linewidth=0.55, edgecolor='black')\n",
    "\n",
    "            # Set the plot title using the current column name\n",
    "            title = f\"{title_prefix} {column.replace('_', ' ').title()}\"\n",
    "\n",
    "            max_title_length = 46\n",
    "            if len(title) > max_title_length:\n",
    "                # Use textwrap to avoid splitting words\n",
    "                wrapped_title = \"\\n\".join(textwrap.wrap(title, width=max_title_length, break_long_words=False))\n",
    "                title = wrapped_title\n",
    "\n",
    "            ax.set_title(title, fontsize=13)\n",
    "\n",
    "            # Sanitize the column name for the filename\n",
    "            sanitized_column_name = sanitize_column_name(column)\n",
    "\n",
    "            # Optionally save the figure\n",
    "            if savefig:\n",
    "                file_name = f\"{sanitized_column_name}_plot.png\"\n",
    "                output_path = os.path.join(output_folder, file_name)\n",
    "                plt.savefig(output_path, dpi=300)\n",
    "                print(f\"Plot saved: {output_path}\")\n",
    "\n",
    "            # Display the plot\n",
    "            plt.show()\n",
    "\n",
    "    print(f\"All plots have been saved to {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region_individual_metrics(gdf, domain='Individual Metric', counties_to_plot=None, region=None, plot_all=True, savefig=True, output_folder='plots_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your output folder\n",
    "output_folder = ''\n",
    "\n",
    "# Count the number of files in the directory\n",
    "file_count = len([f for f in os.listdir(output_folder) if os.path.isfile(os.path.join(output_folder, f))])\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of files in the output folder: {file_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
