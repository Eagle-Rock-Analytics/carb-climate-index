{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import (\n",
    "    min_max_standardize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gov_built_society_natural_domain_calc(output_folder_name, domain_name):\n",
    "    '''\n",
    "    Pulls dummy data from AWS, groups them into respective domains, and sums values from indicators per census tract/geoid. Total values are then \n",
    "    min/maxed standardized and placed into a .csv file within a new folder. Resulting files (per domain) are uploaded to AWS.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_folder_name: string\n",
    "        Name of output folder    \n",
    "    domain_name: string\n",
    "        Starting string of the dummy data e.g. DUMMY_built\n",
    "    '''\n",
    "    \n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Bucket name and file paths\n",
    "    bucket_name = 'ca-climate-index'\n",
    "    directory = '3_fair_data/dummy_data/dummy_dataset.zip'\n",
    "    \n",
    "    # Local directory to store the downloaded zip file and extracted contents\n",
    "    local_directory = 'dummy_dataset'\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "    \n",
    "    # Download the zip file\n",
    "    local_zip_file_path = os.path.join(local_directory, os.path.basename(directory))\n",
    "    s3_client.download_file(bucket_name, directory, local_zip_file_path)\n",
    "    \n",
    "    # Extract the contents of the zip file\n",
    "    with ZipFile(local_zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(local_directory)\n",
    "    \n",
    "    # List all files in the input folder\n",
    "    files = os.listdir(local_directory)\n",
    "    \n",
    "    # Filter files that start with 'DUMMY__' and end with '.csv'\n",
    "    csv_files = [file for file in files if file.startswith(domain_name) and file.endswith('.csv')]\n",
    "    \n",
    "    # Initialize an empty DataFrame to accumulate data from all files\n",
    "    aggregated_df = None\n",
    "    \n",
    "    # Loop through each CSV file\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(local_directory, file))\n",
    "        \n",
    "        # Group by 'GEOID' and sum the values in the second column\n",
    "        grouped_df = df.groupby('GEOID').agg({df.columns[1]: 'sum'})\n",
    "        \n",
    "        # Merge the grouped data with the aggregated DataFrame\n",
    "        if aggregated_df is None:\n",
    "            aggregated_df = grouped_df\n",
    "        else:\n",
    "            aggregated_df = aggregated_df.add(grouped_df, fill_value=0)\n",
    "    \n",
    "    # Reset index before saving\n",
    "    aggregated_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Add a final column containing the sum of values across all columns except the first\n",
    "    aggregated_df['sum_of_values'] = aggregated_df.iloc[:, 1:].sum(axis=1)\n",
    "    \n",
    "    # Use min max standardize function\n",
    "    min_max_standardize(aggregated_df, 'sum_of_values')\n",
    "\n",
    "    # check for values <0 or >1:\n",
    "    bad_values = aggregated_df[~aggregated_df['min_max_standardized'].between(0., 1., inclusive='both')]\n",
    "    if not bad_values.empty:\n",
    "        print(f\"ERROR: Out of bounds data found in {domain_name} data at the following tracts:\")\n",
    "        print(bad_values.GEOID)\n",
    "        raise Exception\n",
    "        \n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder_name):\n",
    "        os.makedirs(output_folder_name)\n",
    "    \n",
    "    # Save the final grouped DataFrame to a new CSV file\n",
    "    final_output_file_path = os.path.join(output_folder_name, f\"{domain_name}_summed_indicators.csv\")\n",
    "    aggregated_df.to_csv(final_output_file_path, index=False)\n",
    "    \n",
    "    \n",
    "    # Upload the sum file to AWS S3\n",
    "    s3_client.upload_file(final_output_file_path, bucket_name, f\"3_fair_data/dummy_data/{domain_name}_summed_indicators.csv\")\n",
    "    print('Summed indicators uploaded to aws')\n",
    "    \n",
    "# os.remove(metric_avgs_folder)\n",
    "# os.remove(final_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed indicators uploaded to aws\n",
      "Summed indicators uploaded to aws\n",
      "Summed indicators uploaded to aws\n",
      "Summed indicators uploaded to aws\n"
     ]
    }
   ],
   "source": [
    "list = ['DUMMY_built',\n",
    "        'DUMMY_society',\n",
    "        'DUMMY_governance',\n",
    "        'DUMMY_natural'\n",
    "        ]\n",
    "\n",
    "for file in list:\n",
    "    df = gov_built_society_natural_domain_calc('sum_of_indicators', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_domain_calc(output_folder_name, file_identifier):\n",
    "    '''\n",
    "    Pulls climate risk dummy data from AWS, groups them into exposure/loss indicators, and sums values from indicators per census tract. \n",
    "    \n",
    "    Note:\n",
    "    This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in ~/.aws/credentials.\n",
    "    See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_folder_name: string\n",
    "        Name of output folder    \n",
    "    file_identifier: string\n",
    "        Keyword to separate file names e.g. exposure\n",
    "    '''\n",
    "    \n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Bucket name and file paths\n",
    "    bucket_name = 'ca-climate-index'\n",
    "    directory = '3_fair_data/dummy_data/dummy_dataset.zip'\n",
    "    \n",
    "    # Local directory to store the downloaded zip file and extracted contents\n",
    "    local_directory = 'dummy_dataset'\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "    \n",
    "    # Download the zip file\n",
    "    local_zip_file_path = os.path.join(local_directory, os.path.basename(directory))\n",
    "    s3_client.download_file(bucket_name, directory, local_zip_file_path)\n",
    "    \n",
    "    # Extract the contents of the zip file\n",
    "    with ZipFile(local_zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(local_directory)\n",
    "    \n",
    "    # List all files in the input folder\n",
    "    files = os.listdir(local_directory)\n",
    "    \n",
    "    # Filter files that contain 'file_identifier' and end with '.csv'\n",
    "    csv_files = [file for file in files if (file_identifier) in file and file.endswith('.csv')]\n",
    "    \n",
    "    # Initialize an empty DataFrame to accumulate data from all files\n",
    "    aggregated_df = None\n",
    "    \n",
    "    # Loop through each CSV file\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(local_directory, file))\n",
    "        \n",
    "        # Group by 'GEOID' and sum the values in the second column\n",
    "        grouped_df = df.groupby('GEOID').agg({df.columns[1]: 'sum'})\n",
    "        \n",
    "        # Merge the grouped data with the aggregated DataFrame\n",
    "        if aggregated_df is None:\n",
    "            aggregated_df = grouped_df\n",
    "        else:\n",
    "            aggregated_df = aggregated_df.add(grouped_df, fill_value=0)\n",
    "    \n",
    "    # Reset index before saving\n",
    "    aggregated_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Add a final column containing the sum of values across all columns except the first\n",
    "    aggregated_df['sum_of_values'] = aggregated_df.iloc[:, 1:].sum(axis=1)\n",
    "    \n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder_name):\n",
    "        os.makedirs(output_folder_name)\n",
    "    \n",
    "    # Save the final grouped DataFrame to a new CSV file\n",
    "    final_output_file_path = os.path.join(output_folder_name, f\"{file_identifier}_summed_indicators.csv\")\n",
    "    aggregated_df.to_csv(final_output_file_path, index=False) \n",
    "    \n",
    "#os.remove(final_output_file_path)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate risk domain calculation uploaded to aws\n"
     ]
    }
   ],
   "source": [
    "# For climate risk domain\n",
    "list = ['exposure',\n",
    "        'loss'\n",
    "        ]\n",
    "\n",
    "for file in list:\n",
    "    climate_domain_calc('climate_domain', file)\n",
    "\n",
    "# Create output folder to hold output from multiplication & standardization steps\n",
    "output_folder = 'climate_domain_product'\n",
    "\n",
    "# Load both files created from climate_domain_calc\n",
    "df1 = pd.read_csv(os.path.join('climate_domain', 'exposure_summed_indicators.csv'))\n",
    "df2 = pd.read_csv(os.path.join('climate_domain', 'loss_summed_indicators.csv'))\n",
    "\n",
    "# Merge based on GEOID\n",
    "merged_df = pd.merge(df1, df2, on='GEOID')\n",
    "\n",
    "# Multiply 'sum_of_values' columns together\n",
    "merged_df['product_of_sum_of_values_exposure_loss'] = merged_df['sum_of_values_x'] * merged_df['sum_of_values_y']\n",
    "\n",
    "# Use min max standardize function\n",
    "min_max_standardize(merged_df, 'product_of_sum_of_values_exposure_loss')\n",
    "\n",
    "# Change the column names to accurately match the multiplication portion of the climate risk domain\n",
    "merged_df['min_product_value'] = merged_df['min_sum_value']\n",
    "merged_df['max_product_value'] = merged_df['max_sum_value']\n",
    "merged_df['min_max_standardized_from_product'] = merged_df['min_max_standardized']\n",
    "\n",
    "# Define the directory for the multiplied output file\n",
    "multiplied_output_folder = 'climate_domain_product'\n",
    "if not os.path.exists(multiplied_output_folder):\n",
    "    os.makedirs(multiplied_output_folder)\n",
    "\n",
    "# Save the multiplied DataFrame to a new CSV file\n",
    "multiplied_output_file_path = os.path.join(multiplied_output_folder, \"climate_multiplied.csv\")\n",
    "merged_df[['GEOID', \n",
    "           'sum_of_values_x', \n",
    "           'sum_of_values_y', \n",
    "           'product_of_sum_of_values_exposure_loss', \n",
    "           'min_product_value', \n",
    "           'max_product_value', \n",
    "           'min_max_standardized_from_product']].to_csv(multiplied_output_file_path, index=False)\n",
    "\n",
    "# Upload the sum file to AWS S3\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'ca-climate-index'\n",
    "s3_client.upload_file(multiplied_output_file_path, bucket_name, f\"3_fair_data/dummy_data/DUMMY_climate_indicator_product.csv\")\n",
    "print('Climate risk domain calculation uploaded to aws')\n",
    "\n",
    "#os.remove(multiplied_output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
