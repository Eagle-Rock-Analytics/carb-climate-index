{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(output_folder_name, domain_name):\n",
    "\n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Bucket name and file paths\n",
    "    bucket_name = 'ca-climate-index'\n",
    "    directory = '3_fair_data/dummy_data/dummy_dataset.zip'\n",
    "\n",
    "    # Local directory to store the downloaded zip file and extracted contents\n",
    "    local_directory = 'dummy_dataset'\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "\n",
    "    # Download the zip file\n",
    "    local_zip_file_path = os.path.join(local_directory, os.path.basename(directory))\n",
    "    s3_client.download_file(bucket_name, directory, local_zip_file_path)\n",
    "\n",
    "    # Extract the contents of the zip file\n",
    "    with ZipFile(local_zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(local_directory)\n",
    "        # List all files in the input folder\n",
    "\n",
    "    files = os.listdir('dummy_dataset')\n",
    "    \n",
    "    # Filter files that start with 'DUMMY_built' and end with '.csv'\n",
    "    csv_files = [file for file in files if file.startswith(domain_name) and file.endswith('.csv')]\n",
    "    \n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder_name):\n",
    "        os.makedirs(output_folder_name)\n",
    "    \n",
    "    # Loop through each CSV file\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(local_directory, file))\n",
    "        \n",
    "        # Get the second column\n",
    "        data = df.iloc[:, 1]\n",
    "\n",
    "        # Calculate the average of the 'data_standardized' column\n",
    "        standardized_average = data.mean()\n",
    "\n",
    "        # Create a new CSV file for the standardized average\n",
    "        avg_output_file_path = os.path.join(output_folder_name, f\"{os.path.splitext(file)[0]}_average.csv\")\n",
    "        with open(avg_output_file_path, 'w') as avg_file:\n",
    "            avg_file.write(f\"average_metric_score\\n{standardized_average}\\n\")\n",
    "\n",
    "    #os.remove(local_directory)\n",
    "\n",
    "def sum_domain_averages(metric_avgs_folder, output_domain_folder_name, domain_name):\n",
    "    \n",
    "    # List all files in the output folder\n",
    "    files = os.listdir(metric_avgs_folder)\n",
    "    \n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_domain_folder_name):\n",
    "        os.makedirs(output_domain_folder_name)\n",
    "\n",
    "    # Filter files that start with the domain_name and end with '_average.csv'\n",
    "    domain_files = [file for file in files if file.startswith(domain_name) and file.endswith('_average.csv')]\n",
    "    \n",
    "    domain_sum = 0\n",
    "    # Loop through each domain file\n",
    "    for file in domain_files:\n",
    "        # Read the average file\n",
    "        avg_df = pd.read_csv(os.path.join(metric_avgs_folder, file))\n",
    "        # Get the average value\n",
    "        avg_value = avg_df.iloc[0, 0]\n",
    "        # Sum the average values\n",
    "        domain_sum += avg_value\n",
    "    \n",
    "        # Write the sum to a new file\n",
    "    sum_output_file_path = os.path.join(output_domain_folder_name, f\"{domain_name}_sum.csv\")\n",
    "    with open(sum_output_file_path, 'w') as sum_file:\n",
    "        sum_file.write(f\"summed_average_metric_score\\n{domain_sum}\\n\")\n",
    "    \n",
    "    # Upload the sum file to AWS S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket_name = 'ca-climate-index'\n",
    "    s3_client.upload_file(sum_output_file_path, bucket_name, f\"3_fair_data/dummy_data/{domain_name}_sum.csv\")\n",
    "    print('Summed indicators uploaded to aws')\n",
    "    #os.remove(metric_avgs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed indicators uploaded to aws\n",
      "Summed indicators uploaded to aws\n",
      "Summed indicators uploaded to aws\n",
      "Summed indicators uploaded to aws\n"
     ]
    }
   ],
   "source": [
    "list = ['DUMMY_built',\n",
    "        'DUMMY_society',\n",
    "        'DUMMY_governance',\n",
    "        'DUMMY_natural'\n",
    "        ]\n",
    "\n",
    "for file in list:\n",
    "    process_csv_files('metric_averages', file)\n",
    "    \n",
    "for file in list:\n",
    "    sum_domain_averages('metric_averages', 'domain_sum', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(output_folder_name, domain_name):\n",
    "\n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Bucket name and file paths\n",
    "    bucket_name = 'ca-climate-index'\n",
    "    directory = '3_fair_data/dummy_data/dummy_dataset.zip'\n",
    "\n",
    "    # Local directory to store the downloaded zip file and extracted contents\n",
    "    local_directory = 'dummy_dataset'\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "\n",
    "    # Download the zip file\n",
    "    local_zip_file_path = os.path.join(local_directory, os.path.basename(directory))\n",
    "    s3_client.download_file(bucket_name, directory, local_zip_file_path)\n",
    "\n",
    "    # Extract the contents of the zip file\n",
    "    with ZipFile(local_zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(local_directory)\n",
    "        # List all files in the input folder\n",
    "\n",
    "    files = os.listdir('dummy_dataset')\n",
    "    \n",
    "    # Filter files that start with 'DUMMY_built' and end with '.csv'\n",
    "    csv_files = [file for file in files if file.startswith(domain_name) and file.endswith('.csv')]\n",
    "    \n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder_name):\n",
    "        os.makedirs(output_folder_name)\n",
    "    \n",
    "    # Loop through each CSV file\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(local_directory, file))\n",
    "        \n",
    "        # Get the second column\n",
    "        data = df.iloc[:, 1]\n",
    "\n",
    "        # Calculate the average of the 'data_standardized' column\n",
    "        standardized_average = data.mean()\n",
    "\n",
    "        # Create a new CSV file for the standardized average\n",
    "        avg_output_file_path = os.path.join(output_folder_name, f\"{os.path.splitext(file)[0]}_average.csv\")\n",
    "        with open(avg_output_file_path, 'w') as avg_file:\n",
    "            avg_file.write(f\"average_metric_score\\n{standardized_average}\\n\")\n",
    "\n",
    "    #os.remove(local_directory)\n",
    "\n",
    "def sum_domain_averages(metric_avgs_folder, output_domain_folder_name, domain_name):\n",
    "    \n",
    "    # List all files in the output folder\n",
    "    files = os.listdir(metric_avgs_folder)\n",
    "    \n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_domain_folder_name):\n",
    "        os.makedirs(output_domain_folder_name)\n",
    "\n",
    "    # Filter files that start with the domain_name and end with '_average.csv'\n",
    "    domain_files = [file for file in files if file.startswith(domain_name) and file.endswith('_average.csv')]\n",
    "    \n",
    "    # Collect individual average values\n",
    "    individual_averages = []\n",
    "    \n",
    "    # Loop through each domain file\n",
    "    for file in domain_files:\n",
    "        # Read the average file\n",
    "        avg_df = pd.read_csv(os.path.join(metric_avgs_folder, file))\n",
    "        # Get the average value\n",
    "        avg_value = avg_df.iloc[0, 0]\n",
    "        # Add the average value to the list\n",
    "        individual_averages.append(avg_value)\n",
    "    \n",
    "    # Sum the individual average values\n",
    "    domain_sum = sum(individual_averages)\n",
    "    \n",
    "    # Perform min-max standardization on the summed value\n",
    "    x_min = min(individual_averages)\n",
    "    x_max = max(individual_averages)\n",
    "    domain_sum_standardized = (domain_sum - x_min) / (x_max - x_min)\n",
    "    \n",
    "    # Write the sum and standardized sum to a new file\n",
    "    sum_output_file_path = os.path.join(output_domain_folder_name, f\"{domain_name}_sum.csv\")\n",
    "    with open(sum_output_file_path, 'w') as sum_file:\n",
    "        sum_file.write(f\"summed_average_metric_score,summed_average_metric_score_standardized\\n\")\n",
    "        sum_file.write(f\"{domain_sum},{domain_sum_standardized}\\n\")\n",
    "    '''\n",
    "    # Upload the sum file to AWS S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket_name = 'your_bucket_name'  # Replace with your S3 bucket name\n",
    "    s3_client.upload_file(sum_output_file_path, bucket_name, f\"{output_domain_folder_name}/{domain_name}_sum.csv\")\n",
    "    '''\n",
    "    #os.remove(metric_avgs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ['DUMMY_built',\n",
    "        'DUMMY_society',\n",
    "        'DUMMY_governance',\n",
    "        'DUMMY_natural'\n",
    "        ]\n",
    "\n",
    "for file in list:\n",
    "    process_csv_files('metric_averages', file)\n",
    "    \n",
    "for file in list:\n",
    "    sum_domain_averages('metric_averages', 'domain_sum', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
