{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cal-CRAI Index: Climate Hazard Calculations\n",
    "\n",
    "**Order of operations**:\n",
    "1) Metric handling \\\n",
    "   1a - Retrieve data \\\n",
    "   1b - Min-max standardization \\\n",
    "   1c - Set hazard risk orientation (positive for when a larger value represents greater vulnerability, negative for when a larger value corresponds to decreased vulnerability)\n",
    "\n",
    "2) Calculate indicators \\\n",
    "   2a - Isolate exposure and loss columns for all climate risk scenarios \\\n",
    "   2b - Isolate exposure and loss for each individual climate risk scenarios \\\n",
    "   2c - Merge the all climate risk indicator columns with the individual climate risk indicators columns\n",
    "   \n",
    "3) Calculate hazard score \\\n",
    "   3a - Exposure * Loss columns \\\n",
    "   3b - Outlier Handling\n",
    "   \n",
    "4) Mask out inland counties for Sea Level Rise (SLR) Hazard Column \\\n",
    "   4a - Merge with SLR masking data \\\n",
    "   4b - Any tract not 'SLR impacted' is changed to NaN\n",
    "   \n",
    "5) Finalize Hazard Score\n",
    "\n",
    "6) Visualize, save, and export Climate Hazard Score dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# suppress pandas purely educational warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import pull_csv_from_directory, upload_csv_aws, delete_items\n",
    "from scripts.utils.cal_crai_plotting import plot_hazard_score, plot_region_domain # type: ignore\n",
    "from scripts.utils.cal_crai_calculations import (handle_outliers, min_max_standardize, process_domain_csv_files,  # type: ignore\n",
    "                                        indicator_dicts, add_census_tracts, domain_summary_stats, compute_summed_climate_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Handling\n",
    "### 1a) Retrieve metric files and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "bucket_name = 'ca-climate-index'\n",
    "aws_dir = '3_fair_data/index_data/'\n",
    "\n",
    "pull_csv_from_directory(bucket_name, aws_dir, output_folder='aws_csvs', search_zipped=False, print_name=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and merge climate hazard metric files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain-specific\n",
    "domain_prefix = 'climate_'\n",
    "input_folder = r'aws_csvs'\n",
    "output_folder = domain_prefix + \"folder\"\n",
    "meta_csv = r'../utils/calcrai_metrics.csv'\n",
    "merged_output_file = f'concatenate_{domain_prefix}metrics.csv'\n",
    "\n",
    "metric_vulnerable_resilient_dict = process_domain_csv_files(domain_prefix, input_folder, output_folder, meta_csv, merged_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take a look at the merged singluar csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in and view processed data\n",
    "pd.set_option('display.max_columns', None)\n",
    "cleaned_climate_df = pd.read_csv(merged_output_file)\n",
    "cleaned_climate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the resulting dictionary: We will use this later to refactor certain metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_vulnerable_resilient_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Min-max standardization\n",
    "Metrics are min-max standardized on 0.01 to 0.99 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing our df\n",
    "columns_to_process = [col for col in cleaned_climate_df.columns if col != 'GEOID']\n",
    "min_max_metrics = min_max_standardize(cleaned_climate_df, columns_to_process)\n",
    "min_max_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate for GEOID and standardized columns exclusively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['GEOID','_standardized']\n",
    "selected_columns = []\n",
    "for word in words:\n",
    "    selected_columns.extend(min_max_metrics.columns[min_max_metrics.columns.str.endswith(word)].tolist())\n",
    "min_max_standardized_climate_metrics_df = min_max_metrics[selected_columns]\n",
    "min_max_standardized_climate_metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Set hazard risk orientation\n",
    "* High values indicate resiliency to a climate hazard\n",
    "* Low values indicate vulnerablility to a climate hazard\n",
    "\n",
    "For the climate domain, all metrics represent a communities vulnerablity to climate hazards rather than resilience. For example, 'median_heat_warning_days' represents a communities vulnerability to extreme heat. For this metric, the higher the number, the more vulnerable. So we identify these 'vulnerable' metrics (in this case all climate metrics) with our `metric_vulnerable_resilient_dict` dictionary and subtract their values from 1 so all high values indicate resiliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the vulnerable column names from the dictionary\n",
    "vulnerable_columns = metric_vulnerable_resilient_dict['vulnerable']\n",
    "\n",
    "# Identify columns in the DataFrame that contain any of the vulnerable column names as substrings\n",
    "vulnerable_columns_in_df = [col for col in min_max_standardized_climate_metrics_df.columns \n",
    "                           if any(resilient_col in col for resilient_col in vulnerable_columns)]\n",
    "\n",
    "# Create a new DataFrame with the adjusted vulnerable columns\n",
    "adjusted_vulnerable_df = min_max_standardized_climate_metrics_df.copy()\n",
    "\n",
    "# Subtract the standardized vulnerable columns from one and store the result in the new DataFrame\n",
    "adjusted_vulnerable_df.loc[:, vulnerable_columns_in_df] = (\n",
    "    1 - adjusted_vulnerable_df.loc[:, vulnerable_columns_in_df]\n",
    ")\n",
    "adjusted_vulnerable_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Indicators\n",
    "Loop to go through df columns and sum metrics that belong within an indicator based off of the metric to indicator dictionary\n",
    "\n",
    "For the climate domain, metrics are split between 'exposure' and 'loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_prefix[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Isolate exposure and loss columns for all climate risk scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_indicators_climate_systems = compute_summed_climate_indicators(\n",
    "    adjusted_vulnerable_df, \n",
    "    indicator_dicts(domain_prefix[:-1]), print_summary=True\n",
    ")\n",
    "\n",
    "# show resulting dataframe to highlight the indicator values\n",
    "summed_indicators_climate_systems = summed_indicators_climate_systems.rename(columns={'exposure':'all_domain_exposure', 'loss':'all_domain_loss'})\n",
    "summed_indicators_climate_systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Isolate exposure and loss for each individual climate risk scenarios\n",
    "* create dictionary that separates metric columns by the five climate risks\n",
    "* create another dictionary that further separates metric columns by exposure or loss\n",
    "* data are then summed by climate risk and indicator type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_climate_metrics = adjusted_vulnerable_df.copy()\n",
    "\n",
    "# Remove '_min_max_standardized' suffix from column names\n",
    "standardized_climate_metrics.columns = adjusted_vulnerable_df.columns.str.replace('_min_max_standardized', '', regex=False)\n",
    "\n",
    "# Climate risk dictionary to group columns\n",
    "climate_risk_mapping = {\n",
    "    'drought': [\n",
    "        'drought_coverage_percentage',\n",
    "        'drought_crop_loss_acres',\n",
    "        'drought_crop_loss_indemnity_amount',\n",
    "        'change_in_drought_years',\n",
    "        'percent_weeks_drought'\n",
    "    ],\n",
    "    'extreme_heat': [\n",
    "        'mean_change_annual_heat_days',\n",
    "        'mean_change_annual_warm_nights',\n",
    "        'mean_change_cold_days',\n",
    "        'heat_crop_loss_acres',\n",
    "        'heat_crop_loss_indemnity_amount',\n",
    "        'avg_age_adjust_heat_hospitalizations_per_10000',\n",
    "        'median_heat_warning_days'\n",
    "    ],\n",
    "    'inland_flooding': [\n",
    "        'floodplain_percentage',\n",
    "        'avg_flood_insurance_payout_per_claim',\n",
    "        'estimated_flood_crop_loss_cost',\n",
    "        'precip_99percentile',\n",
    "        'surface_runoff',\n",
    "        'total_flood_fatalities',\n",
    "        'median_flood_warning_days'\n",
    "    ],\n",
    "    'sea_level_rise': [\n",
    "        'slr_vulnerable_building_content_cost',\n",
    "        'building_exposed_slr_count',\n",
    "        'slr_vulnerability_delta_percentage_change',\n",
    "        'slr_vulnerable_wastewater_treatment_count',\n",
    "        'rcp_4.5__50th_percent_change',\n",
    "        'fire_stations_count_diff',\n",
    "        'hospitals_count_diff',\n",
    "        'police_stations_count_diff',\n",
    "        'schools_count_diff'\n",
    "    ],\n",
    "    'wildfire': [\n",
    "        'burn_area_m2',\n",
    "        'change_ffwi_days',\n",
    "        'average_damaged_destroyed_structures_wildfire',\n",
    "        'average_annual_fatalities_wildfire',\n",
    "        'median_red_flag_warning_days'\n",
    "    ]\n",
    "}\n",
    "\n",
    "metric_to_indicator_climate_dict = {\n",
    "                \"exposure\" :   ['drought_coverage_percentage',\n",
    "                                'change_in_drought_years',\n",
    "                                'percent_weeks_drought',\n",
    "                                'precip_99percentile',\n",
    "                                'surface_runoff',\n",
    "                                'floodplain_percentage',\n",
    "                                'median_flood_warning_days',\n",
    "                                'mean_change_annual_heat_days',\n",
    "                                'mean_change_annual_warm_nights',\n",
    "                                'median_heat_warning_days',\n",
    "                                'slr_vulnerability_delta_percentage_change',\n",
    "                                'fire_stations_count_diff',\n",
    "                                'police_stations_count_diff',\n",
    "                                'schools_count_diff',\n",
    "                                'hospitals_count_diff',\n",
    "                                'slr_vulnerable_wastewater_treatment_count',\n",
    "                                'building_exposed_slr_count',\n",
    "                                'slr_vulnerable_building_content_cost',\n",
    "                                'change_ffwi_days',\n",
    "                                'median_red_flag_warning_days'\n",
    "                ],\n",
    "                \"loss\"  :  ['drought_crop_loss_acres',\n",
    "                            'drought_crop_loss_indemnity_amount',\n",
    "                            'avg_flood_insurance_payout_per_claim',\n",
    "                            'estimated_flood_crop_loss_cost',\n",
    "                            'total_flood_fatalities',\n",
    "                            'mean_change_cold_days',\n",
    "                            'heat_crop_loss_acres',\n",
    "                            'heat_crop_loss_indemnity_amount',\n",
    "                            'avg_age_adjust_heat_hospitalizations_per_10000',\n",
    "                            'rcp_4.5__50th_percent_change',\n",
    "                            'burn_area_m2',\n",
    "                            'average_damaged_destroyed_structures_wildfire',\n",
    "                            'average_annual_fatalities_wildfire'\n",
    "]}\n",
    "\n",
    "# Step 2: Group and sum the columns by climate risk and metric type\n",
    "# Initialize an empty DataFrame to hold the summed data\n",
    "climate_sums_df = pd.DataFrame()\n",
    "climate_sums_df['GEOID'] = standardized_climate_metrics['GEOID']\n",
    "\n",
    "# Loop over each climate risk and categorize by exposure/loss\n",
    "for risk, columns in climate_risk_mapping.items():\n",
    "    # Separate columns by 'exposure' and 'loss'\n",
    "    exposure_columns = [col for col in columns if col in metric_to_indicator_climate_dict[\"exposure\"]]\n",
    "    loss_columns = [col for col in columns if col in metric_to_indicator_climate_dict[\"loss\"]]\n",
    "    \n",
    "    # Sum the values for each category and add to the dataframe\n",
    "    climate_sums_df[f'{risk}_exposure'] = standardized_climate_metrics[exposure_columns].sum(axis=1)\n",
    "    climate_sums_df[f'{risk}_loss'] = standardized_climate_metrics[loss_columns].sum(axis=1)\n",
    "    \n",
    "for risk in climate_risk_mapping.keys():\n",
    "    # Calculate product of exposure and loss for each climate risk\n",
    "    # If loss indicator is zero, keep the exposure value instead of multiplying\n",
    "    climate_sums_df[f'{risk}_hazard_score'] = np.where(\n",
    "        climate_sums_df[f'{risk}_loss'] == 0,\n",
    "        climate_sums_df[f'{risk}_exposure'],\n",
    "        climate_sums_df[f'{risk}_exposure'] * climate_sums_df[f'{risk}_loss']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_domain_exposure_loss = climate_sums_df.copy()\n",
    "\n",
    "# Define the list of columns to exclude\n",
    "exclude_columns = ['drought_hazard_score', 'extreme_heat_hazard_score', \n",
    "                   'inland_flooding_hazard_score', 'sea_level_rise_hazard_score', \n",
    "                   'wildfire_hazard_score']\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "climate_domain_exposure_loss = climate_domain_exposure_loss.drop(columns=exclude_columns, errors='ignore')\n",
    "climate_domain_exposure_loss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Merge the all climate risk indicator columns with the individual climate risk indicators columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Merge the aggregated data back with the original `summed_indicators_climate_systems`\n",
    "climate_exposure_loss_values = pd.merge(summed_indicators_climate_systems, climate_domain_exposure_loss, on='GEOID', how='left')\n",
    "climate_exposure_loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Indicator dataframe as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up file for export\n",
    "indicator_filename = '{}domain_indicators.csv'.format(domain_prefix)\n",
    "climate_exposure_loss_values.to_csv(indicator_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Hazard Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Calculate the hazard score\n",
    "Hazard score is: exposure * loss columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_hazard_scores_scenarios = climate_sums_df.copy()\n",
    "# Define the list of columns to exclude\n",
    "keep_columns = ['GEOID', 'drought_hazard_score', 'extreme_heat_hazard_score', \n",
    "                   'inland_flooding_hazard_score', 'sea_level_rise_hazard_score', \n",
    "                   'wildfire_hazard_score']\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "climate_hazard_scores_scenarios = climate_sums_df[keep_columns].copy()\n",
    "climate_hazard_scores_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_indicators_climate_systems['hazard_score'] = summed_indicators_climate_systems['all_domain_exposure'] * summed_indicators_climate_systems['all_domain_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_hazard_scores_cleaned = pd.merge(summed_indicators_climate_systems, climate_hazard_scores_scenarios, on='GEOID', how='left')\n",
    "climate_hazard_scores_cleaned = climate_hazard_scores_cleaned.drop(columns={'all_domain_exposure', 'all_domain_loss'})\n",
    "climate_hazard_scores_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Outlier Handling\n",
    "* set fencing for each hazard score at 25th and 75th percentiles\n",
    "* reset values that exceed the fence to nearest fence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_hazard_scores_outlier_handle = handle_outliers(climate_hazard_scores_cleaned, domain_prefix='climate', summary_stats=True)\n",
    "climate_hazard_scores_outlier_handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Min-max standardize the product columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_process = ['hazard_score'\n",
    "                      ,'drought_hazard_score'\n",
    "                      ,'extreme_heat_hazard_score'\n",
    "                      ,'inland_flooding_hazard_score'\n",
    "                      ,'sea_level_rise_hazard_score'\n",
    "                      ,'wildfire_hazard_score']\n",
    "\n",
    "min_max_domain = min_max_standardize(climate_hazard_scores_outlier_handle, columns_to_process)\n",
    "min_max_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate to census tract and product standardized columns\n",
    "* add a zero at the beginning of the GEOID to match census tract that will be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['GEOID', \n",
    "                'hazard_score_min_max_standardized'\n",
    "                ,'drought_hazard_score_min_max_standardized'\n",
    "                ,'extreme_heat_hazard_score_min_max_standardized'\n",
    "                ,'inland_flooding_hazard_score_min_max_standardized'\n",
    "                ,'sea_level_rise_hazard_score_min_max_standardized'\n",
    "                ,'wildfire_hazard_score_min_max_standardized'\n",
    "]\n",
    "\n",
    "climate_hazard_scores = min_max_domain[keep_columns].copy()\n",
    "\n",
    "# Rename columns by removing '_min_max_standardized' suffix\n",
    "climate_hazard_scores.columns = climate_hazard_scores.columns.str.replace('_min_max_standardized', '', regex=False)\n",
    "climate_hazard_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Mask out inland counties for Sea Level Rise (SLR) Hazard Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr_mask_data = '../utils/slr_mask_layer.csv'\n",
    "slr_mask = pd.read_csv(slr_mask_data)\n",
    "slr_mask = slr_mask.drop(columns={'county', 'geometry', 'COUNTYFP'})\n",
    "slr_mask.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) Merge with SLR masking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_hazard_scores['GEOID'] = climate_hazard_scores['GEOID'].astype(str)\n",
    "slr_mask['GEOID'] = slr_mask['GEOID'].astype(str)\n",
    "\n",
    "climate_hazard_scores_slr_masked = pd.merge(climate_hazard_scores, slr_mask, on='GEOID', how='left')\n",
    "climate_hazard_scores_slr_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) Any tract not 'SLR impacted' is changed to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_hazard_scores_slr_masked.loc[climate_hazard_scores_slr_masked['slr_impacted'] == 0, 'sea_level_rise_hazard_score'] = np.nan\n",
    "climate_hazard_scores_slr_masked = climate_hazard_scores_slr_masked.drop(columns='slr_impacted')\n",
    "climate_hazard_scores_slr_masked.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Finalize Hazard Score\n",
    "* Add beginning 0's to GEOID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_hazard_scores_final = climate_hazard_scores_slr_masked.copy()\n",
    "\n",
    "# GEOID handling\n",
    "climate_hazard_scores_final['GEOID'] = climate_hazard_scores_final['GEOID'].apply(lambda x: '0' + str(x))\n",
    "climate_hazard_scores_final['GEOID'] = climate_hazard_scores_final['GEOID'].astype(str).apply(lambda x: x.rstrip('0').rstrip('.') if '.' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_hazard_scores_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6) Visualize, save, and export Climate Hazard Score dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some summary statistics for this domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_summary_stats(climate_hazard_scores_final, 'hazard_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map all of the climate risk scenarios hazard scores\n",
    "* these are the denominators that go into each weighted scenario\n",
    "* values will be subtracted from 1 to indicate high values are high hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataset\n",
    "flipped_climate_scenarios = climate_hazard_scores_final.copy()\n",
    "\n",
    "# List of climate domain columns to process\n",
    "climate_domain_columns = [\n",
    "    'hazard_score',\n",
    "    'drought_hazard_score',\n",
    "    'extreme_heat_hazard_score',\n",
    "    'wildfire_hazard_score',\n",
    "    'sea_level_rise_hazard_score',\n",
    "    'inland_flooding_hazard_score'\n",
    "]\n",
    "\n",
    "# Process each column in the list\n",
    "for column in climate_domain_columns:\n",
    "    # Subtract 1 from the column values\n",
    "    flipped_climate_scenarios[column] = 1 - flipped_climate_scenarios[column]\n",
    "    \n",
    "     # Get domain name for plotting\n",
    "    if column == 'hazard_score':\n",
    "        domain_name = 'All Climate Scenarios'\n",
    "    else:\n",
    "        domain_name = column.split('_hazard_score')[0]  # Extract everything before '_hazard_score'\n",
    "        domain_name = domain_name.replace('_', ' ').title()\n",
    "    \n",
    "    # Call the plotting function\n",
    "    plot_hazard_score(flipped_climate_scenarios, column_to_plot=column, domain=domain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7) Export the final domain csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up file for export\n",
    "climate_hazard_scores_filename = 'climate_hazard_scores.csv'\n",
    "climate_hazard_scores_final.to_csv(climate_hazard_scores_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the indicator and hazard score csv files to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# upload to aws bucket\n",
    "bucket_name = 'ca-climate-index'\n",
    "directory = '3_fair_data/index_data'\n",
    "\n",
    "files_upload = indicator_filename, climate_hazard_scores_filename\n",
    "\n",
    "for file in files_upload:\n",
    "    upload_csv_aws([file], bucket_name, directory)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete desired csv files\n",
    "* all that were generated from this notebook by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_to_delete = [\"aws_csvs\", \"climate_folder\"]\n",
    "csv_files_to_delete = [\"concatenate_climate_metrics.csv\", \"climate_hazard_scores.csv\",\n",
    "                       \"climate_domain_indicators.csv\"]\n",
    "\n",
    "delete_items(folders_to_delete, csv_files_to_delete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
