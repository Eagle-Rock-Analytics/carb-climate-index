{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsys.path.append(os.path.expanduser('../../'))\\nfrom scripts.utils.file_helpers import (\\n    pull_zipped_csv, upload_csv_aws\\n)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import boto3\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.write_metadata import (\n",
    "    append_metadata\n",
    ")\n",
    "\n",
    "# IMPORT WHEN PR #42 IS MERGED\n",
    "'''\n",
    "sys.path.append(os.path.expanduser('../../'))\n",
    "from scripts.utils.file_helpers import (\n",
    "    pull_zipped_csv, upload_csv_aws\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame as 'DUMMY_built_summed_indicators.csv'\n",
      "Saved DataFrame as 'DUMMY_climate_indicator_product.csv'\n",
      "Saved DataFrame as 'DUMMY_governance_summed_indicators.csv'\n",
      "Saved DataFrame as 'DUMMY_natural_summed_indicators.csv'\n",
      "Saved DataFrame as 'DUMMY_society_summed_indicators.csv'\n"
     ]
    }
   ],
   "source": [
    "def pull_csv_from_directory(bucket_name, directory, search_zipped=True):\n",
    "    \"\"\"\n",
    "    Pulls CSV files from a specified directory in an S3 bucket.\n",
    "    \n",
    "    Parameters:\n",
    "    - bucket_name (str): The name of the S3 bucket.\n",
    "    - directory (str): The directory within the bucket to search for CSV files.\n",
    "    - search_zipped (bool): If True, search for CSV files within zip files. If False, search for CSV files directly.\n",
    "    \"\"\"\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # List objects in the specified directory\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=directory)\n",
    "\n",
    "    # Check if objects were found\n",
    "    if 'Contents' in response:\n",
    "        # Iterate through each object found\n",
    "        for obj in response['Contents']:\n",
    "            # Get the key (filename) of the object\n",
    "            key = obj['Key']\n",
    "            \n",
    "            # Check if the object is a .zip file\n",
    "            if search_zipped and key.endswith('.zip'):\n",
    "                # Download the zip file into memory\n",
    "                zip_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                zip_data = io.BytesIO(zip_object['Body'].read())\n",
    "                \n",
    "                # Open the zip file\n",
    "                with zipfile.ZipFile(zip_data, 'r') as zip_ref:\n",
    "                    # Iterate through each file in the zip\n",
    "                    for file_name in zip_ref.namelist():\n",
    "                        # Check if the file is a .csv file\n",
    "                        if file_name.endswith('.csv'):\n",
    "                            # Read the .csv file\n",
    "                            with zip_ref.open(file_name) as csv_file:\n",
    "                                # Convert the csv content to pandas DataFrame\n",
    "                                df = pd.read_csv(csv_file)\n",
    "                                # Save the DataFrame with a similar name as the .csv file\n",
    "                                df_name = file_name[:-4]  # Remove .csv extension\n",
    "                                df.to_csv(f\"{df_name}.csv\", index=False)\n",
    "                                print(f\"Saved DataFrame as '{df_name}.csv'\")\n",
    "                                # You can now manipulate df as needed\n",
    "            elif not search_zipped and key.endswith('.csv'):\n",
    "                # Directly download the CSV file\n",
    "                csv_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                csv_data = io.BytesIO(csv_object['Body'].read())\n",
    "                # Convert the csv content to pandas DataFrame\n",
    "                df = pd.read_csv(csv_data)\n",
    "                # Save the DataFrame with a similar name as the .csv file\n",
    "                df_name = key.split('/')[-1][:-4]  # Extract filename from key\n",
    "                df.to_csv(f\"{df_name}.csv\", index=False)\n",
    "                print(f\"Saved DataFrame as '{df_name}.csv'\")\n",
    "                # You can now manipulate df as needed\n",
    "\n",
    "    else:\n",
    "        print(\"No objects found in the specified directory.\")\n",
    "\n",
    "bucket_name = 'ca-climate-index'\n",
    "aws_dir = '3_fair_data/dummy_data/'\n",
    "\n",
    "# Search for non-zipped files\n",
    "pull_csv_from_directory(bucket_name, aws_dir, search_zipped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = [\n",
    "    'DUMMY_built_summed_indicators.csv',\n",
    "    'DUMMY_governance_summed_indicators.csv',\n",
    "    'DUMMY_natural_summed_indicators.csv',\n",
    "    'DUMMY_society_summed_indicators.csv'\n",
    "]\n",
    "\n",
    "# Define the output folder path\n",
    "output_folder = 'output_folder'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Iterate through source files and copy them to the output folder\n",
    "for file in source_files:\n",
    "    # Construct the source file path\n",
    "    source_path = file\n",
    "    \n",
    "    # Construct the destination file path\n",
    "    destination_path = os.path.join(output_folder, os.path.basename(file))\n",
    "    \n",
    "    # Copy the file to the output folder\n",
    "    shutil.copyfile(source_path, destination_path)\n",
    "\n",
    "    os.remove(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to output_folder\\merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_csv_files(input_folder):\n",
    "    # Initialize an empty DataFrame to store the merged data\n",
    "    master_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each CSV file in the folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(filepath)\n",
    "            # Extract 'GEOID' and 'min_max_standardized' columns\n",
    "            subset_df = df[['GEOID', 'min_max_standardized']].copy()\n",
    "            # Rename 'min_max_standardized' column with filename prefix\n",
    "            subset_df.rename(columns={'min_max_standardized': f\"{os.path.splitext(filename)[0]}_min_max_standardized\"}, inplace=True)\n",
    "            # Merge with master DataFrame based on 'GEOID'\n",
    "            if master_df.empty:\n",
    "                master_df = subset_df\n",
    "            else:\n",
    "                master_df = pd.merge(master_df, subset_df, on='GEOID', how='outer')\n",
    "\n",
    "    # Save the master DataFrame to a new CSV file\n",
    "    output_filename = 'merged_data.csv'\n",
    "    output_filepath = os.path.join(input_folder, output_filename)\n",
    "    master_df.to_csv(output_filepath, index=False)\n",
    "    print(f\"Merged data saved to {output_filepath}\")\n",
    "\n",
    "# Specify the input folder containing CSV files\n",
    "input_folder = 'your_input_folder_path'\n",
    "\n",
    "# Call the function\n",
    "merge_csv_files('output_folder')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
